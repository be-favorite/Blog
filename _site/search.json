[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "방태모",
    "section": "",
    "text": "학부, 석사를 통계학으로 전공하고 현재는 지마켓 AI Product 팀에서 Data Science를 하고있습니다.\n데이터로부터 인사이트를 추출하는 것을 좋아합니다.\n인과추론, 시계열 자료분석, 통계적학습과 기계학습에 관심이 많고, 가장 사랑하는 언어는 R입니다.\n과거에는 티스토리 블로그를 운영했었습니다.\n제가 배운 것들을 나누는 것이 누군가에게 도움이 되었으면 합니다."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "포스트",
    "section": "",
    "text": "논문 요약 - Simões et al (2022)\n\n\n1 min\n\n\n\nTime Series\n\n\nPaper\n\n\n\n프랑스 남부 지역의 심호흡곤란 입원 발생에 관한 대기오염원 영향 평가 논문 요약\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n관심 논문 읽고 요약하기\n\n\n4 min\n\n\n\nPaper\n\n\n\n논문을 읽고 요약하는 나만의 방식\n\n\n\nMay 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyverse로 데이터베이스랑 대화하기 - 1편\n\n\n5 min\n\n\n\nSQL\n\n\nR\n\n\n\nR을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyvese principle로 머신러닝 하기\n\n\n18 min\n\n\n\nStatistical/Machine Learning\n\n\nR\n\n\n\ntidymodels ecosystem 소개\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyverse principle로 시계열 자료 분석하기\n\n\n22 min\n\n\n\nTime Series\n\n\nR\n\n\n\ntidyverts ecosystem 소개\n\n\n\nMar 11, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "아카이브",
    "section": "",
    "text": "프랑스 남부 지역의 심호흡곤란 입원 발생에 관한 대기오염원 영향 평가 논문 요약\n\n\n\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n논문을 읽고 요약하는 나만의 방식\n\n\n\n\n\n\nMay 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nR을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels ecosystem 소개\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntidyverts ecosystem 소개\n\n\n\n\n\n\nMar 11, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html#슬기로운통계생활-객원칼럼",
    "href": "archive.html#슬기로운통계생활-객원칼럼",
    "title": "아카이브",
    "section": "슬기로운통계생활 객원칼럼",
    "text": "슬기로운통계생활 객원칼럼\n유튜브 채널 <슬기로운통계생활>에서 운영하고 있는 블로그에서 객원 작가로 칼럼을 작성하고 있습니다.\n Blog"
  },
  {
    "objectID": "archive.html#연구-아카이브",
    "href": "archive.html#연구-아카이브",
    "title": "아카이브",
    "section": "연구 아카이브",
    "text": "연구 아카이브\n논문과 책, 웹사이트 등을 통해 공부하고 연구한 것들을 아카이브하고 있습니다.\n Archive"
  },
  {
    "objectID": "archive.html#발표-아카이브",
    "href": "archive.html#발표-아카이브",
    "title": "아카이브",
    "section": "발표 아카이브",
    "text": "발표 아카이브\n발표 자료를 아카이브하고 있습니다.\n Archive"
  },
  {
    "objectID": "archive.html#스터디-노트",
    "href": "archive.html#스터디-노트",
    "title": "아카이브",
    "section": "스터디 노트",
    "text": "스터디 노트\n스터디 노트를 아카이브하고 있습니다.\n Tensorflow  Coding-Test  SQL"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "구독",
    "section": "",
    "text": "이메일\n    \n  \n  \n    이름 (옵션)\n    \n  \n  \n    성 (옵션)\n    \n  \n  \n    \n  \n  구독하시는 경우, Revue의 약관 및 개인 정보 보호 정책에 동의하게됩니다. \n  \n\n\n그리고, 여기서 저를 서포트해주실 수도 있어요.😀"
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "",
    "text": "Photo by Caspar Camille Rubin on Unsplash\n실무에서는 Data analyst, Data scientist를 가리지 않고 SQL에 관한 능력을 요구합니다. 우리나라의 채용공고를 둘러보면 Data analyst의 경우 특히 SQL 스킬을 중요하게 요구하는 듯 합니다. 방대한 양의 데이터를 저장하고 관리하기 위해 실무에서는 데이터베이스를 사용합니다. 데이터베이스는 종종 관계형 데이터베이스 시스템1(이하 RDBMS)이라 불리기도 하죠. 그리고, 우리는 SQL2 언어 또는 SQL을 조금 변형한(variant) 언어를 통해 이 데이터베이스에 질의(query)를 합니다. 여기서 변형이라는 말을 사용한 이유는, RDBMS를 제공하는 업체에서 표준화된 SQL을 제공하는 경우도 있지만, 표준화된 SQL을 조금 변형시켜 사용하는 경우도 있기 때문입니다.\n만약 이렇게 특정 업체로부터 제공되는 변형된 RDBMS를 사용해야한다면, 해당 업체에서 사용하는 특정 SQL dialect3를 사용해 쿼리를 작성하는 방법을 이해해야 하실겁니다. 변형된 RDBMS를 예로 들어보자면, PostgreSQL, PrestoDB(AWS의 Athena를 위한) 등이 있습니다. PostgreSQL DB의 JSON 필드는 AWS에서 구조화된 중첩 배열로(array) 수집되므로, 동일한 필드를 쿼리하고자 할 때 다른 쿼리문을 사용합니다.\nR을 사용하는 여러분 모두 잘 아시다시피, R에서는 {dplyr}4 패키지를 통해 이러한 작업을 데이터에 수행할 수 있습니다. {dplyr}이 select(), group_by(), left_join() 등 SQL 문법을 잘 모방하긴 했지만, SQL 문법과 R 문법 사이를 완벽하게 왔다갔다 하기는 어렵습니다. 예를 들자면, {dplyr}의 filter()를 이용해 특정 행을 뽑아올 때, 우리는 R 문법을 따라야하므로 조건문에 =이 아닌 ==을 사용하죠. 이는 SQL 문법과는 완벽히 다른 부분입니다.\n자, 여기서 이러한 상황을 타개할 방법은 무엇일까요. 엄청난 용량의 데이터베이스를 R로 가져올 수는 없습니다. 메모리 베이스인 R에 이 짓을 햇다가는요? 생각도 하기 싫습니다.😰 그럼, RDBMS 환경에서 이러한 무거운 작업을(e.g. computation) 수행하고 필요로 될 때에만 R에다가 가져오면 되지 않을까요? 예를 들면, 집계된 데이터를 가져와서 보고서용 그림을 그린다든지. 이를 가능하게끔 해주는 패키지에 대해 배워보려고 합니다.\n본 튜토리얼에서는 {dplyr}의 데이터베이스 백엔드 버전이라 할 수 있는 {dbplyr} 패키지에 대해 배울거에요. {dbplyr}은 당신의 RDBMS에 R의 tidyverse 문법을 사용한 쿼리문을 직접적으로 사용할 수 있게끔 해줄겁니다.😀"
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결하기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결하기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "1 DB 연결하기",
    "text": "1 DB 연결하기\n먼저 필요한 패키지를 불러오죠. install.packages(\"패키지명\")을 통해 설치할 수 있습니다.\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RSQLite)\nlibrary(odbc)\n\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인 패키지입니다.\n{dbplyr}: {dplyr} 문법을 사용하여 데이터베이스에 질의를 할 수 있게끔 해줍니다.\n{dplyr}: 데이터베이스에 질의할 때 사용할 패키지입니다.\n{RSQLite}: 가벼운 단일 유저용 데이터베이스 SQLite DB에 연결할 수 있게끔 해주는 DBI5 호환 패키지입니다. R-SQLite로 이해하시면 편합니다.\n다른 DBI 용 호환 패키지가 필요할 수도 있습니다. 예를 들어, {RPostgres}는 PostgreSQL RDBMS와 연결을 해주는 패키지입니다.6\n{odbc}: odbc 드라이버를 사용해 RDBMS 인터페이스에 인터페이스할 수 있도록 해주는 DBI 호환 인터페이스입니다.7\n\n\n예제용 토이 DB\nAlison Hill이 The Great British Bake off에서 만든 데이터를 사용하려고 합니다. 본 예제에서 다룰 데이터베이스는 여기서 내려받으세요. {bakeoff} 패키지의 데이터를 이용해 연습에 사용할 SQLite DB를 만들었습니다. 이 튜토리얼의 원 저자 Vebash Naidoo님께 감사의 말을 전합니다.\n\n\nSQLite DB 연결하기\n이제 DB를 SQLite DB에 연결해봅시다. DB와 대화를 나누기 위해서, 우선 연결(connection)을 해줘야합니다. 다음의 작업을 해줄겁니다.\n\nDBI 패키지 로딩: library(DBI)\n연결하기: con <- dbConnect(RSQLite::SQLite(), \"내려받은 db 경로\")\n\n\nlibrary(DBI) # main DB interface\nlibrary(dplyr) \nlibrary(dbplyr) # dplyr back-end for DBs\n\ncon <- dbConnect(drv = RSQLite::SQLite(), # give me a SQLite connection\n        dbname = \"data/great_brit_bakeoff.db\")\nsummary(con) # What do we have?\n\n          Length            Class             Mode \n               1 SQLiteConnection               S4 \n\n\n위와 같은 명령어가 출력되면 DB에 성공적으로 연결된 것입니다."
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-둘러보고-다뤄보기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-둘러보고-다뤄보기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "2 DB 둘러보고 다뤄보기",
    "text": "2 DB 둘러보고 다뤄보기\n자, DB 연결도 했으니 이제 몇 가지 DBI 함수를 이용해 연결한 DB를 둘러보고 다뤄봅시다.\n\nDBI 함수\nDBI 함수들의 이름은 꽤 직관적입니다.\n\ndbListTables(con) # 연결된 테이블 리스트를 보여줘!\n\n [1] \"baker_results\"     \"bakers\"            \"bakes\"            \n [4] \"challenge_results\" \"challenges\"        \"episode_results\"  \n [7] \"episodes\"          \"ratings\"           \"ratings_seasons\"  \n[10] \"results\"           \"seasons\"           \"series\"           \n\n\n\ndbListFields(con, # 연결한 DB로 가서\n      \"bakers\")   # bakes 테이블에 어떤 필드가 있는지 알려줘!\n\n[1] \"series\"     \"baker_full\" \"age\"        \"occupation\" \"hometown\"  \n\n\nDB에 질의는 다음과 같이 수행할 수 있어요.\n\nres <- dbSendQuery(con, \"SELECT * FROM bakers LIMIT 3\") # 쿼리문 실행\n# bakers 테이블에 모든 필드를 가져오는데, 관측치 3개까지만 가져와봐!\ndbFetch(res) # 결과 출력해줘\n\n  series          baker_full age                        occupation\n1      1       Annetha Mills  30                           Midwife\n2      1      David Chambers  31                      Entrepreneur\n3      1 Edward \"Edd\" Kimber  24 Debt collector for Yorkshire Bank\n       hometown\n1         Essex\n2 Milton Keynes\n3      Bradford\n\n\n\ndbClearResult(res) # 결과 지우기\n\n\n\ndplyr 함수\n이제, 우리가 잘하는 {dplyr}의 함수들을 이용해 마음껏 DB와 이야기해보죠.\n\ndplyr::tbl(con, \"테이블명\"): 연결한 DB(con)으로 가서 SELECT * FROM 테이블명 실행해줘.\n\n\ntbl(con, \"bakers\")\n\n# Source:   table<bakers> [?? x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker_full                age occupation                       homet…¹\n    <dbl> <chr>                   <dbl> <chr>                            <chr>  \n 1      1 \"Annetha Mills\"            30 Midwife                          Essex  \n 2      1 \"David Chambers\"           31 Entrepreneur                     Milton…\n 3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yorkshire Ba… Bradfo…\n 4      1 \"Jasminder Randhawa\"       45 Assistant Credit Control Manager Birmin…\n 5      1 \"Jonathan Shepherd\"        25 Research Analyst                 St Alb…\n 6      1 \"Lea Harris\"               51 Retired                          Midlot…\n 7      1 \"Louise Brimelow\"          44 Police Officer                   Manche…\n 8      1 \"Mark Whithers\"            48 Bus Driver                       South …\n 9      1 \"Miranda Gore Browne\"      37 Food buyer for Marks & Spencer   Midhur…\n10      1 \"Ruth Clemens\"             31 Retail manager/Housewife         Poynto…\n# … with more rows, and abbreviated variable name ¹​hometown\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\ntbl(con, \"bakers\") %>% \n    head(3) # \"SELECT * FROM bakers LIMIT 3\"와 동일\n\n# Source:   SQL [3 x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n  series baker_full                age occupation                        homet…¹\n   <dbl> <chr>                   <dbl> <chr>                             <chr>  \n1      1 \"Annetha Mills\"            30 Midwife                           Essex  \n2      1 \"David Chambers\"           31 Entrepreneur                      Milton…\n3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yorkshire Bank Bradfo…\n# … with abbreviated variable name ¹​hometown\n\n\n데이터베이스와 대화를 나눌 때 마다 초기에 연결해둔 con을 사용한다는 점을 유념해주세요. 초기에 불러왔던 con은 아까처럼 일반적인 SQL 쿼리문을 이용해 질의를 할 때 뿐만이 아닌 {dplyr}을 통해 타이디한 파이프라인으로 원하는 테이블을 가져올 때도 사용됩니다.\n자 이제 예시 상황을 하나 들어서 {dplyr}로 원하는 테이블을 가져와보겠습니다. baker_results 테이블에는 각 제빵 대회에 참가한 제빵사(baker)의 세부 정보 필드가 담겨있습니다:\n\ndbListFields(con, \"baker_results\")\n\n [1] \"series\"                    \"baker_full\"               \n [3] \"baker\"                     \"age\"                      \n [5] \"occupation\"                \"hometown\"                 \n [7] \"baker_last\"                \"baker_first\"              \n [9] \"star_baker\"                \"technical_winner\"         \n[11] \"technical_top3\"            \"technical_bottom\"         \n[13] \"technical_highest\"         \"technical_lowest\"         \n[15] \"technical_median\"          \"series_winner\"            \n[17] \"series_runner_up\"          \"total_episodes_appeared\"  \n[19] \"first_date_appeared\"       \"last_date_appeared\"       \n[21] \"first_date_us\"             \"last_date_us\"             \n[23] \"percent_episodes_appeared\" \"percent_technical_top3\"   \n\n\n각 제빵대회 우승자의 출신이 영국의 일부 지역에서 나왔는지, 아니면 다양한 지역으로부터 우상자가 배출되었는지 알고싶은 상황이라고 해봅시다. 우선 다음과 같이 관심있는 필드만 불러와주겠습니다.\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner)\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker     hometown              series_winner\n    <dbl> <chr>     <chr>                         <int>\n 1      1 Annetha   Essex                             0\n 2      1 David     Milton Keynes                     0\n 3      1 Edd       Bradford                          1\n 4      1 Jasminder Birmingham                        0\n 5      1 Jonathan  St Albans                         0\n 6      1 Lea       Midlothian, Scotland              0\n 7      1 Louise    Manchester                        0\n 8      1 Mark      South Wales                       0\n 9      1 Miranda   Midhurst, West Sussex             0\n10      1 Ruth      Poynton, Cheshire                 0\n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n24개 열 중 관심있는 4개 열만 불러왔습니다. 이제 제빵대회에 우승한 사람만 골라낸 뒤(filter()) 우승자들이 영국의 어떤 지역으로 부터 왔는지 지역별로 인원을 구하고(count()) 내림차순 정렬(sort())을 해보죠.\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>%\n  count(hometown, sort = TRUE)\n\n# Source:     SQL [8 x 2]\n# Database:   sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n# Ordered by: desc(n)\n  hometown                              n\n  <chr>                             <int>\n1 Wigan                                 1\n2 West Molesey, Surrey                  1\n3 Ongar, Essex                          1\n4 Market Harborough, Leicestershire     1\n5 Leeds / Luton                         1\n6 Bradford                              1\n7 Barton-Upon-Humber, Lincolnshire      1\n8 Barton-Le-Clay, Bedfordshire          1\n\n\n이 결과에 따르면, 제빵대회 우승자들의 출신 지역은 각기 다르다고 결론을 내릴 수 있겠네요.\n\n\ndplyr 문법을 SQL 쿼리문으로\n앞서 {dplyr}을 이용해 수행한 질의를 SQL 쿼리문으로는 어떻게 작성할까요? 코드 한 줄이면 손쉽게 알 수 있습니다.😀\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>% \n  count(hometown, sort = TRUE) %>% \n  show_query()\n\n<SQL>\nSELECT `hometown`, COUNT(*) AS `n`\nFROM (\n  SELECT `series`, `baker`, `hometown`, `series_winner`\n  FROM `baker_results`\n)\nWHERE (`series_winner` = 1.0)\nGROUP BY `hometown`\nORDER BY `n` DESC\n\n\n멋지지 않습니까? 이제 제가 왜 이 글의 맨 위 요약을 “R을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!”라고 적은 지 아시겠나요? {dplyr}로 작업을 수행하고, SQL 쿼리문으로 변환을 수행해보는 작업은 SQL을 배우는 과정에 꽤 큰 도움이 될겁니다. 직장 또는 기관에서 DB를 관리할 때 모두 같은 업체의 SQL DB를 사용하는 건 아니므로, 이렇게 광범위한 업체들로부터 공급되는 SQL을 알고, 읽는 것은 언제나 중요하기 때문입니다.\n\n\n출력문의 lazy query / ??의 의미\n앞서 테이블, 쿼리를 작성하며 출력물에서 Source: table [?? x 5] 또는 Source: lazy query [?? x 4]와 같은 문장을 확인하실 수 있었을 겁니다.\n\n이런 문장이 출력물에 포함되는 이유\n\n먼저, 우리가 직접적인 RDBMS 상에서가 아닌 R이라는 공간을 빌려 뒤에서(behind the scenes) 작성한 dplyr코드는 우리가 연결하려는 DB의 SQL에 해당하는 dialect로 변환됩니다.\n즉, SQL은 DB에 직접적으로 실행됩니다. 즉, 데이터를 먼저 R로 가져와서 조작하는 것이 아닌 쿼리 자체를 DB에 보내고 DB에서 계산(computation)이 수행됩니다.\n정리하면, dplyr 파이프라인을 사용해 DB에서 쿼리를 실행하면, DB에서 계산을 수행하고 실행된 최종 결과의 전체가 아닌 일부를 R에서 보여주는 식입니다.\n이러한 이유들을 들여다보면 우리는 ??를 이해할 수 있습니다.\n??는 “연결 DB con에서 이 쿼리(파이프라인을 SQL로 변환시킨 것)를 실행했고, 여기 R에서 출력물을 스니펫(snippet)으로만 가져왔는데, 얼마나 많은 수의 행이 있는지에 관한 메타 정보까진 캐치하진 못했어. 그저 출력물에 몇 개의 열이 있다는 것 정도만 캐치했어”라고 이해할 수 있습니다.\n이 튜토리얼은 파트 1 입니다. 다음 파트에서 가져온 테이블에 얼마나 많은 행들이 존재하는 지와 같은 메타 정보들을 R로 어떻게 가져오는지에 대해 알아볼 예정입니다."
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결-해제하기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결-해제하기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "3 DB 연결 해제하기",
    "text": "3 DB 연결 해제하기\n작업이 끝나면 연결을 해제하는 것을 잊지마세요!\n\ndbDisconnect(con) # db 연결 닫기\n\n연결 해제가 체크는 dbListTable(con)을 실행해보시면 됩니다. 연결해제가 잘 되었다면 에러문이 출력될겁니다.\n\n다음 파트에서 배울 내용\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인 패키지입니다.\n데이터 R로 가져오기\n\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n dbplyr      * 2.2.1   2022-06-27 [1] CRAN (R 4.2.0)\n dplyr       * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n odbc        * 1.3.3   2021-11-30 [1] CRAN (R 4.2.0)\n RSQLite     * 2.2.15  2022-07-17 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "blog.html#블로그를-지원해주시겠어요",
    "href": "blog.html#블로그를-지원해주시겠어요",
    "title": "포스트",
    "section": "블로그를 지원해주시겠어요?",
    "text": "블로그를 지원해주시겠어요?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "연구 아카이브",
    "section": "",
    "text": "논문과 책, 웹사이트 등을 통해 공부하고 연구한 것들을 아카이브합니다.\n참고 문헌과 스터디 노트, 그리고 재현가능한 소스코드를 함께 제공하고자 합니다."
  },
  {
    "objectID": "research.html#time-series",
    "href": "research.html#time-series",
    "title": "연구 아카이브",
    "section": "Time Series",
    "text": "Time Series\n\n1 추론 모델링 · Regression\n\nSpurious regression\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n여러 시계열로 회귀를 수행할 때, 꼭 주의해야 할 알아두어야할 사항\n🔗 스터디 노트\n🔗 R 튜토리얼: CCF 분석의 허구적 상관 확인 과정 참고\n\n\n\nRegression with ARIMA errors\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\nDistributed lag model\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n\n\n\nDistributed lag non-linear model\n\nGasparrini, Antonio, Benedict Armstrong, and M.G. Kenward. “Distributed Lag Non-Linear Models.” Statistics in Medicine 29 (September 20, 2010): 2224–34. https://doi.org/10.1002/sim.3940.\nGasparrini, Antonio. “Distributed Lag Linear and Non-Linear Models in R: The Package Dlnm.” Journal of Statistical Software 43 (July 1, 2011): 1–20. https://doi.org/10.18637/jss.v043.i08.\n🔗 스터디 노트\n🔗 PPT\n🔗 R 튜토리얼\n\n\n\n\n2 예측모델링 · Forecasting\n\nExponential Smoothing\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n🔗 R 튜토리얼: tidyverse principle로 시계열 자료분석하기\n\n\n\nARIMA model\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n\n\n\nProphet\n\nTaylor, Sean, and Benjamin Letham. Forecasting at Scale, 2017. https://doi.org/10.7287/peerj.preprints.3190v2.\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\nHierarchical Time Series Forecasting\n\nAthanasopoulos, George, Roman A. Ahmed, and Rob J. Hyndman. “Hierarchical Forecasts for Australian Domestic Tourism.” International Journal of Forecasting 25, no. 1 (January 1, 2009): 146–66. https://doi.org/10.1016/j.ijforecast.2008.07.004.\nAthanasopoulos, George, Rob Hyndman, Roman Ahmed, and Han Lin Shang. “Optimal Combination Forecasts for Hierarchical.” Computational Statistics & Data Analysis 55 (September 1, 2011): 2579–89. https://doi.org/10.1016/j.csda.2011.03.006.\nHyndman, Rob J, George Athanasopoulos, and Han Lin Shang. “Hts: An R Package for Forecasting Hierarchical or Grouped Time Series,” n.d., 12.\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\n\n3 Other techniques\n\nIntervention analysis (Interrupted Time Series)\n\nSlides. “Intervention Analysis.” Accessed April 17, 2022. https://slides.com/tonyg/intervention-analysis.\n🔗 참고 자료\n🔗 스터디 노트\n🔗 R 코드\n🔗 R 코드: arimax() 튜토리얼\n\n\n\nDynamic Time Warping (DTW)\n\nBerndt, Donald J., and James Clifford. “Using Dynamic Time Warping to Find Patterns in Time Series.” In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, 359–70. AAAIWS’94. Seattle, WA: AAAI Press, 1994.\n선행 또는 후행하는 시계열, 시차가 존재하나 유사한 패턴이 존재하는 두 시계열을 잡아낼 수 있게끔 해주는 비유사성 측도(거리 측도) 알고리즘\nDTW distance를 이용해 계층적 군집 분석 수행 가능\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\nDiscrete Wavelet Transform (DWT)\n\nGraps, Amara. “An Introduction to Wavelets.” IEEE Comp. Sci. Engi. 2 (February 1, 1995): 50–61. https://doi.org/10.1109/99.388960.\nLi, Daoyuan, Tegawendé F. Bissyandé, Jacques Klein, and Y. L. Traon. “Time Series Classification with Discrete Wavelet Transformed Data: Insights from an Empirical Study.” In SEKE, 2016. https://doi.org/10.18293/SEKE2016-067.\n시계열들을 데이터의 열로 나열하여 classification을 수행할 때, 효과적인 차원 감소 방법\n일종의 시계열 Feature engineering 기법에 해당\n🔗 스터디 노트\n🔗 R 튜토리얼"
  },
  {
    "objectID": "research.html#machine-learning-and-statistical-learning",
    "href": "research.html#machine-learning-and-statistical-learning",
    "title": "연구 아카이브",
    "section": "Machine Learning and Statistical Learning",
    "text": "Machine Learning and Statistical Learning\n\nPrerequisite\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n🔗 스터디 노트: Prerequisite 1 머신러닝 용어 정리\n\n\n\nEnsemble methods\n\nChen, Tianqi, and Carlos Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August 13, 2016, 785–94. https://doi.org/10.1145/2939672.2939785.\nChen, Lilly. “Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained.” Medium, January 2, 2019. https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725.\nMorde, Vishal. “XGBoost Algorithm: Long May She Reign!” Medium, April 8, 2019. https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d.\n“Light GBM vs XGBOOST: Which Algorithm Takes the Crown.” Accessed April 17, 2022. https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/.\nRandom Forest, AdaBoost, Gradient Boosting, XGBoost, Light GBM\n🔗 스터디 노트\n🔗 R 튜토리얼: tidyverse principle로 머신러닝하기\n\n\n\nLogistic regression\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference and Prediction. 2nd ed. Springer, 2009. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\nStatQuest with Josh Starmer. Logistic Regression Details Pt 2: Maximum Likelihood, 2018. https://www.youtube.com/watch?v=BfKanl1aSG0.\nChatterjee, Samprit, and Ali S. Hadi. “Regression Analysis by Example, Fifth Edition.”\n🔗 스터디 노트\n\n\n\nGeneralized Linear Model (GLM) and Generalized Additive Model (GAM)\n\nHayes, Genevieve. “Beyond Linear Regression: An Introduction to GLMs.” Medium, December 24, 2019. https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nGLM\n\n🔗 스터디 노트\n\nGAM\n\n🔗 스터디 노트: Prerequisite 1 선형모형의 한계\n🔗 스터디 노트: Prerequisite 2 다항 회귀와 계단 함수\n🔗 스터디 노트: Prerequisite 3 Regression splines\n🔗 스터디 노트: Prerequisite 4 Smoothing splines\n🔗 스터디 노트: Prerequisite 5 Local regressions\n🔗 스터디 노트: GAMs"
  },
  {
    "objectID": "research.html#deep-learning",
    "href": "research.html#deep-learning",
    "title": "연구 아카이브",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nPrerequisites\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n🔗 스터디 노트: Prerequisite 1 딥러닝의 모티베이션과 역사\n🔗 스터디 노트: Prerequisite 2 선형대수의 여러 객체 소개\n🔗 스터디 노트: Prerequisite 3 행렬의 전치와 브로드캐스팅\n🔗 스터디 노트: Prerequisite 4 행렬과 벡터의 곱연산\n🔗 스터디 노트: Prerequisite 5 선형방정식과 선형종속,span\n🔗 스터디 노트: Prerequisite 6 norms\n🔗 스터디 노트: Prerequisite 7 특별한 종류의 행렬과 벡터\n🔗 스터디 노트: Prerequisite 8 고윳값 분해\n🔗 스터디 노트: Prerequisite 9 특잇값 분해와 일반화 역행렬\n🔗 스터디 노트: Prerequisite 10 Trace 연산자와 행렬식\n🔗 스터디 노트: Prerequisite 11 선형대수를 이용한 주성분 유도\n🔗 스터디 노트: Prerequisite 12 머신러닝 용어 정리"
  },
  {
    "objectID": "research.html#high-dimensional-data-analysis",
    "href": "research.html#high-dimensional-data-analysis",
    "title": "연구 아카이브",
    "section": "High-Dimensional Data Analysis",
    "text": "High-Dimensional Data Analysis\n\nBreheny, Patrick. High-Dimensional Data Analysis. The University of Iowa, 2016. https://myweb.uiowa.edu/pbreheny/7600/s16/index.html.\n\n🔗 R 소스코드 및 예제 Dataset 제공\n\n일반적인 기계학습 기반의 예측 모델링으로 접근하기 어려운 n -> p 또는 n < p 인 자료의 예측 모델링에 관한 방법론(여기서 n은 관측치의 수, p는 예측변수의 수)\n꼭 고차원 자료가 아닌, 회귀모형의 예측 성능을 높이기 위해서도 사용되는 방법론들에 해당\n통계적 가설검정 관점에서 가설 검정시 발생하는 고차원 문제에 관한 솔루션 또한 제공함\n\n\n1 고차원 자료에 관한 예측 모델링\n\nPrerequisites\n\n🔗 스터디 노트: Prerequisite 고차원 자료에 대한 고전적인 회귀분석의 문제점\n\n\n\nRidge regression\n\n🔗 스터디 노트\n\n\n\nLasso regression\n\n🔗 스터디 노트\n\n\n\nBias reduction of Lasso estimator\n\n🔗 스터디 노트\n\n\n\nVariance reduction of Lasso eistimator\n\n🔗 스터디 노트\n\n\n\nPenalized logistic regression\n\n🔗 스터디 노트\n\n\n\nPenalized robust regression\n\n🔗 스터디 노트\n\n\n\n\n2 통계적 가설검정 관점의 고차원 문제\n\nPrerequisites\n\n🔗 스터디 노트: Prerequisite 1 통계적 가설검정의 원리\n🔗 스터디 노트: Prerequisite 2 다중 검정\n\n\n\nFamily-Wise Error Rates (FWER)\n\n🔗 스터디 노트\n\n\n\nFalse Discovery Rates (FDR)\n\n🔗 스터디 노트"
  },
  {
    "objectID": "research.html#statistics",
    "href": "research.html#statistics",
    "title": "연구 아카이브",
    "section": "Statistics",
    "text": "Statistics\n\n통계학, 통계적 가설검정과 관련한 것들을 아카이브 합니다.\n\n\n구간추정의 해석에 대한 고전적 관점(Frequentist)과 베이지안 관점\n\n🔗 스터디 노트\n\n\n\n검정력(power)과 검정력 함수에 대해\n\n🔗 스터디 노트\n\n\n\n자유도(Degrees of Freedom)\n\n🔗 스터디 노트\n\n\n\n표준편차와 표준오차\n\n🔗 스터디 노트\n\n\n\n“대립가설이 옳다.”라는 식의 주장을 지양해야하는 이유\n\n🔗 스터디 노트\n\n\n\n중심극한정리의 의미\n\n🔗 스터디 노트\n🔗 스터디 노트: 중심극한정리에 관한 고찰\n\n\n\nFixed effect와 random effect\n\n🔗 스터디 노트"
  },
  {
    "objectID": "research.html#miscellaneous",
    "href": "research.html#miscellaneous",
    "title": "연구 아카이브",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n결정론적 SIR 모형을 이용한 감염병 유행 모델링\n\n🔗 스터디 노트와 R 튜토리얼"
  },
  {
    "objectID": "research.html#statisticalmachine-learning",
    "href": "research.html#statisticalmachine-learning",
    "title": "연구 아카이브",
    "section": "Statistical/Machine Learning",
    "text": "Statistical/Machine Learning\n\nPrerequisite\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n🔗 스터디 노트: Prerequisite 1 머신러닝 용어 정리\n\n\n\nEnsemble methods\n\nChen, Tianqi, and Carlos Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August 13, 2016, 785–94. https://doi.org/10.1145/2939672.2939785.\nChen, Lilly. “Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained.” Medium, January 2, 2019. https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725.\nMorde, Vishal. “XGBoost Algorithm: Long May She Reign!” Medium, April 8, 2019. https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d.\n“Light GBM vs XGBOOST: Which Algorithm Takes the Crown.” Accessed April 17, 2022. https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/.\nRandom Forest, AdaBoost, Gradient Boosting, XGBoost, Light GBM\n🔗 스터디 노트\n🔗 R 튜토리얼: tidyverse principle로 머신러닝하기\n\n\n\nLogistic regression\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference and Prediction. 2nd ed. Springer, 2009. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\nStatQuest with Josh Starmer. Logistic Regression Details Pt 2: Maximum Likelihood, 2018. https://www.youtube.com/watch?v=BfKanl1aSG0.\nChatterjee, Samprit, and Ali S. Hadi. “Regression Analysis by Example, Fifth Edition.”\n🔗 스터디 노트\n\n\n\nGeneralized Linear Model (GLM) and Generalized Additive Model (GAM)\n\nHayes, Genevieve. “Beyond Linear Regression: An Introduction to GLMs.” Medium, December 24, 2019. https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nGLM\n\n🔗 스터디 노트\n\nGAM\n\n🔗 스터디 노트: Prerequisite 1 선형모형의 한계\n🔗 스터디 노트: Prerequisite 2 다항 회귀와 계단 함수\n🔗 스터디 노트: Prerequisite 3 Regression splines\n🔗 스터디 노트: Prerequisite 4 Smoothing splines\n🔗 스터디 노트: Prerequisite 5 Local regressions\n🔗 스터디 노트: GAMs"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "발표 아카이브",
    "section": "",
    "text": "발표자료와 강의자료를 아카이브합니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "",
    "text": "Photo by Agê Barros on Unsplash\ntidyverts ecosystem은 시계열 자료에 관한 분석을 tidyverse principle로 수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidyverse priciple이 데이터 전처리에 있어서 얼마나 많은 업무 생산성을 가져다 주는지 우리는 이미 알고있습니다. 시계열 자료를 자주 다루는 사람이라면 꼭 배워둘 만한 패키지죠.😄 tidyverts ecosystem을 이루는 대부분의 패키지들은 {fpp3}으로 불러올 수 있습니다. {tsibbletalk}은 {shiny}와 함께 동작하는 반응형 그래픽을 제공하는 패키지로 본 튜토리얼에서는 생략하겠습니다:\n위 패키지들이 설치되어 있지 않은 분들은 튜토리얼의 본격적인 시작전에, install.packages(\"패키지명\")을 통해 설치해주시기 바랍니다. 개발 버전을 설치하고 싶으신 분이 있다면 다음의 코드를 이용하세요:"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibble",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibble",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "1 tsibble",
    "text": "1 tsibble\n\n1.1 Get Started\n{tsibble}은 일반적인 시계열 자료를 tibble 형태로 표현할 수 있게해줍니다. 우리는 tsibble()을 통해 tidy한 자료에 대해 수행해왔던 {tidyverse}를 이용한 wrangling을 수행할 수 있습니다. 즉, tidyverse ecosystem이 tibble 객체를 기반으로 동작하듯이, tidyverts ecosytem은 tsibble 객체를 기반으로 동작합니다. tsibble 객체가 갖는 기본적인 원칙은 다음과 같습니다:\n\nindex: 과거부터 현재까지 순서화된 자료값의 관측 시간\nkey: 시간에 따른 관측 단위를 정의하는 변수의 집합\n각 관측치는 index와 key를 통해 유일하게(uniquely) 식별되어야만 함\n각 관측치는 등간격으로 관측된 자료여야만 함\n\n즉, 티블(데이터프레임)을 tsibble로 변환하기(coerce) 위해서는 key와 index를 명시해주어야 합니다. 예를 들어, 다음과 같은 {nycflights13} 패키지의 weather 자료를 이용해보겠습니다:\n\nweather_simple <- nycflights13::weather %>% \n    select(origin, time_hour, temp, humid, precip)\nweather_simple\n\n\n\n\n\n  \n\n\n\norigin을 key로 index를 time_hour로 해주면 될 것 같습니다:\n\nweather_tsbl <- as_tsibble(weather_simple, key = origin, index = time_hour)\nweather_tsbl\n\n\n\n\n\n  \n\n\n\n여기서는 자료 자체가 출발지(origin) 별로 기록된 다중(multiple) 시계열에 해당하므로, key를 origin으로 잡아줬지만, 만약 자료가 단일(univariate) 시계열에 해당한다면 해당 key는 설정을 하지 않으면 됩니다(see package?tsibble and vignette(\"intro-tsibble\") for details). 그리고, 사실 tsibble()은 irregular time interval을 갖는 자료에 대해서도 적용이 가능합니다. as_tsibble은 regular = TRUE 옵션이 default로 설정되는데, 이를 FALSE로 바꿔주면 되며, 이러한 irregular time interval을 갖는 tsibble 객체의 경우는 [!] 표시를 통해 확인할 수 있습니다:\n\nnycflights13::flights %>%\n    mutate(\n      sched_dep_datetime = make_datetime(year, month, day, hour, minute, \n                                         tz = \"America/New_York\")) %>%\n    as_tsibble(\n        key = c(carrier, flight), \n        index = sched_dep_datetime, \n        regular = FALSE\n        )\n\n\n\n\n\n  \n\n\n\n\n\n1.2 Turn impicit missing values into explicit missing values\n간혹 시계열 자료에는 암묵적 결측치(implicit missing values)가 존재하는 경우가 있습니다. 암묵적 결측치가 존재하는 시계열 자료가 일정한 시간 간격으로 수집되었을 경우, 우리는 fill_gaps()를 이용해 암묵적 결측을 명시적으로(explicit) 바꿀 수 있어요. 4년간 수집된 연도별 키위, 체리의 수확량(단위: kg)에 관한 자료를 직접 만들어서 fill_gaps()의 쓰임에 대해 알아봅시다. 본 자료에는 암묵적 결측이 존재합니다:\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest\n\n\n\n\n\n  \n\n\n\n암묵적 결측이란, 예를 들어 위 자료처럼 체리 생산량이 2010년에는 기록되지 않았음에도 불구하고 행이 생략되어있는 것을 말합니다. NA로 명시는 다음과 같이 손쉽게 가능합니다:\n\nfill_gaps(harvest, .full = TRUE)\n\n\n\n\n\n  \n\n\n\n다음의 각각 시작점, 끝점에 대해서만 결측치를 명시할 수도 있습니다:\n\n# at the same starting point across units\nfill_gaps(harvest, .full = start())\n# at the same end point across units\nfill_gaps(harvest, .full = end())\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n.full = FALSE를 설정할 경우(fill_gaps()의 default 옵션에 해당), 각 key 내의 period에서 발생한 결측에 대해서만 명시가 이루어집니다.\n\nfill_gaps(harvest, .full = FALSE)\n\n\n\n\n\n  \n\n\n\n특정값으로의 명시도 손쉽게 수행이 가능해요.\n\nharvest %>% \n    fill_gaps(kilo = 0L)\n\n\n\n\n\n  \n\n\n\n변수에 대해 함수를 적용하여 명시도 가능합니다. sum()을 이용하여 합으로 명시해보았습니다:\n\nharvest %>%\n    fill_gaps(kilo = sum(kilo))\n\n\n\n\n\n  \n\n\n\nkey에 대해 group_by를 통해 각 그룹에 대해 함수를 적용할 수도 있죠. 이번에는 median()을 통해 중위수로 명시해보았습니다:\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo))\n\n\n\n\n\n  \n\n\n\n원 자료 자체에 NA가 존재하는 경우, 적용하고자 하는 함수에 na.rm = TRUE을 설정해주면 됩니다:\n\nharvest[2, 3] <- NA\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo, na.rm = TRUE))\n\n\n\n\n\n  \n\n\n\n마지막으로, fill_gaps()아 tidyr::fill()을 함께 이용하면 암묵적 결측치를 이전 시점의 결측치로 대치할 수 있습니다.\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"down\")\n\n\n\n\n\n  \n\n\n\n반대로, 한 시점 미래의 값으로 대치도 가능합니다.\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"up\")\n\n\n\n\n\n  \n\n\n\n\n\n1.3 Aggregate over calendar periods\nindex_by()와 summarise()를 이용하면 관심있는 변수에 대해 특정 시간 주기(e.g. monthly)에 대해 함수(e.g. 합계: sum(), 평균: mean())를 적용할 수 있어요. index_by는 as.Date(), tsibble::yearweek(), tsibble::yearmonth(), tsibble::yearquarter(), 뿐만 아니라 {lubridate} 계열의 함수와 함께 사용됩니다. 예를 들어, weather 자료의 월별 평균 기온, 총 강수량은 다음과 같이 yearmonth()에 index 변수를 .으로 나타내어 계산할 수 있습니다.\n\nweather_tsbl %>% \n    group_by_key() %>% \n    index_by(year_month = ~yearmonth(.)) %>%\n    summarise(\n        avg_temp = mean(temp, na.rm = TRUE),\n        total_precip = sum(precip, na.rm = TRUE)\n    )\n\n\n\n\n\n  \n\n\n\nindex_by()+summarise()는 irregular time interval을 갖는 tsibble에 대해서도 수행이 가능합니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibbledata",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibbledata",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "2 tsibbledata",
    "text": "2 tsibbledata\n{tsibbledata}는 tsibble 형태의 다양한 예제 자료를 제공해줍니다. 어떤 패키지에 대한 튜토리얼을 진행할 때, 적절한 자료들이 필요로 되는데, 이렇게 예제 자료를 직접적으로 제공해준다는 점에서 R 유저들에 대한 배려가 담겨있다는 생각이 드네요. 예를 들어, 다음의 olympic_running은 4년 주기로 수집된 올림픽 달리기 종목의 성별 최고기록에 관한 자료입니다(see ?olympic_running for details).\n\nolympic_running\n\n\n\n\n\n  \n\n\n\n이 자료를 이용하여 달리기 종목별 최고 기록에 대한 시도표를 성별로 나누어서 그려보았습니다. 참고로, 1916, 1940, 1944년의 경우 세계대전으로 인해 결측 처리되었습니다.\n\nggplot(olympic_running, aes(x = Year, y = Time, colour = Sex)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(~ Length, scales = \"free_y\", nrow = 2) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"Dark2\") + \n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  ylab(\"Running time (seconds)\")"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#feasts",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#feasts",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "3 feasts",
    "text": "3 feasts\n{feasts}는 Feature Extraction And Statistics for Time Series의 약자로, 시계열 자료분석에 쓰이는 여러가지 툴을 제공해줍니다. tsibble 객체와 함께 동작하며, 시계열의 분해, feature 추출(e.g. 추세, 계절성), 시각화 등을 수행할 때 쓰입니다. 아울러, {feasts}를 통한 시계열 자료분석은 다음 섹션에서 소개할 tidyverts ecosystem의 예측 모델링 부분을 담당하는 {fable} 패키지와 긴밀하게 결합하여 사용됩니다.\n\n3.1 Graphics\n시각화는 주로 시계열 자료의 패턴을 이해하기 위한 첫 단계에 많이 이루어집니다. {feasts}는 시계열의 패턴을 {ggplot2}를 사용해 자유롭게 커스텀할 수 있는 그래픽을 제공합니다. 첫 번째로는 gg_season을 이용한 계절성(seasonality) 시각화입니다. 시각화에 사용된 자료 tsibbledata::aus_production은 호주의 맥주, 담배 등의 품목에 관한 분기별 생산지표 추정치에 관한 자료입니다. 맥주의 분기별 생산지표에 관한 계절성 시각화를 수행해보았습니다:\n\naus_production %>% \n  gg_season(Beer)\n\n\n\n\n\n\n\n\n다음으로 gg_subseries()를 이용하면 시계열의 각 season별로 시각화가 가능합니다. 예를 들어, aus_production과 같은 분기별 자료의 경우 분기별 패턴에 대한 시각화를 쉽게 수행할 수 있습니다:\n\naus_production %>% \n  gg_subseries(Beer)\n\n\n\n\n\n\n\n\ngg_lag()를 이용하면 원자료와 시차(lag)의 산점도를 season별로 나누어 그릴 수 있습니다:\n\naus_production %>% \n  filter(year(Quarter) > 1991) %>% \n  gg_lag(Beer, geom = \"point\")\n\n\n\n\n\n\n\n\n분기별 자료의 특성상, lag 4와 8 그림을 보면 각 season별로 원자료와의 관계가 \\(y=x\\) 직선에 잘 놓여있는 것을 캐치할 수 있죠. 마지막으로 ACF 그림도 손쉽게 그릴 수 있습니다:\n\naus_production %>% \n  ACF(Beer) %>% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n3.2 Decompositions\n시계열 분해(decomposition)는 시계열 자료분석에서 흔히 수행되는 작업 중 하나이며, 이는 시계열에 대한 패턴을 이해하는데에 큰 도움을 줍니다. 그리고, 추후 예측 모델링을 정교하게 하는 것에도 상당한 도움을 준다. 즉, 시계열 분해는 본인이 분석하고자 하는 시계열의 패턴을 좀 더 정교하게 캐치하고 예측 성능을 향상시키기 위한 목적으로 꼭 필요로 되는 사전 작업이라고 할 수 있습니다. 본 튜토리얼에서는 {feasts}에서 제공하고 있는 2가지 시계열 분해 방법에 대해 소개하려고 합니다.\n\n3.2.1 Classical decompostion\nclassical decompostion은 1920년대에 고안된 방법입니다. 오래된 방법론인 만큼 요즘 쓰이는 시계열 분해 방법들의 초석이 되는 방법이라고 할 수 있으며, 다른 방법들에 비해 상대적으로 간단하다는 장점이 있습니다. classical decompostion은 가법 분해와 승법 분해가 있습니다. 두 방법은 계절성의 반영 방식에 따라 나뉩니다(e.g. 분기별 자료 \\(m = 4\\), 월별 자료 \\(m = 12\\), 일별 자료 \\(m = 7\\)). 보통 가법 classical decompostion의 경우 계절성이 추세에 따라 무관하게 일정한 크기를 유지할 때 사용하며, 반대로 계절성의 크기가 추세의 크기에 따라 변화하는 경우에는 승법 classical decompostion을 사용합니다. 승법 계절성 classical decompostion는 계절 성분이 연도에 따라 상수라고 가정한채로 진행되며, 승법 계절성에서 계절 성분을 형성하는 \\(m\\)은 계절 지수(seasonal indices)라 불리기도 합니다.\nclassical decompostion의 자세한 분해 과정은 여기를 참고해주시기 바랍니다. 여기서는 바로 R을 이용한 튜토리얼을 진행하겠습니다. 앞서 사용했던 자료의 맥주 생산지표를 가법 classical decomposition을 통해 분해해보겠습니다.\n\ndcmp <- aus_production %>%\n    model(classical_decomposition(Beer, type = \"additive\"))\ncomponents(dcmp)\n\n\n\n\n\n  \n\n\n\n먼저, 분해된 시계열의 요소들은 componenets()로 불러올 수 있습니다. 그리고, 이 components()에 대해 autoplot()을 수행해주면 다음과 같이 시각화를 수행할 수 있습니다:\n\ndcmp %>%\n    components() %>% \n    autoplot() +\n    labs(title = \"Classical additive decomposition of Quarterly production of beer in Australia\")\n\n\n\n\n\n\n\n\n\n\n3.2.2 STL decomposition\nSTL은 “Seasonal and Trend decomposition using Loess”의 준말로 다재다능(versatile)하고 로버스트한 시계열 분해 방법에 해당합니다. 그리고, 여기서 loess란 Local regression의 준말로 자료를 비선형으로 추정하는 방법 중 하나에 해당합니다. STL은 앞서 소개한 classical decomposition, 그리고 {feasts}에서 제공하는 또 다른 시계열 분해 방법 SEATS, X-11과 비교하여 몇몇 이점을 갖는다. 자세한 사항은 여기를 참고해주세요. 본 글은 tidyverts ecosystem에 대한 소개 이므로, deep한 이론 정리는 추후에 fpp3 책을 공부하면서 하나하나 정리해나가겠습니다. 일단 바로 실습으로 넘어가겠습니다.😊 다음은 STL decomposition을 이용하여 시계열의 추세 요소는 window = 7을 통해 좀 더 flexible하게 추정하고, 계절 패턴의 경우는 window = \"periodic\"으로 하여 고정(fixed)되도록 하였습니다(see ?STL for details). 여기서. window란, 창을 말하며 자료를 여러 창으로 잘게 쪼갤수록 더 flexible하고 복잡한 함수를 추정하게 됩니다. splines에 지식이 있는 분들은 이해하기 쉬울거라고 생각합니다.\n\naus_production %>%\n  model(\n    STL(Beer ~ trend(window = 7) + season(window = \"periodic\"),\n        robust = TRUE)) %>%\n  components() %>%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Feature extraction and statistics\n{feast}에서 소개할 마지막 기능은 시계열의 feature(e.g. ACF)와 통계량(e.g. 평균)을 뽑아내는 것입니다. {feast}에서는 feature() 함수를 통해 많은 종류의 features들에 대한 정보를 제공합니다만, 본 튜토리얼에서는 시계열의 평균, 분위수, ACF를 뽑아내는 방법에 대해서만 소개하겠습니다(see ?feature for details). 그 외 다른 features들에 관심이 있으시다면, 여기를 참고해주세요.\n\n3.3.1 Some simple statistics\n먼저, 시계열의 평균과 분위수를 뽑는 방법에 대해 소개하겠습니다. 평균, 분위수 등 시계열의 기본적인 통계량은 feature()와 R의 기본 함수(e.g. mean(), median())들을 이용해 간편하게 계산할 수 있습니다. 여기서 이용할 자료 tourism()은 지역, 주, 목적별로 나눠진 1998-2016년 분기별 호주 여행객수에 관한 자료로, 지역, 주, 여행 목적별 여행객 수의 전체 평균과 분위수를 계산해봤습니다:\n\ntourism %>%\n    features(Trips, \n             list(mean = mean, quantile))\n\n\n\n\n\n  \n\n\n\n\n\n3.3.2 ACF features\nACF에 관한 정보는 feat_acf()를 이용하면 됩니다. feat_acf()는 기본적으로 ACF와 관련한 6가지 또는 최대 7가지의 features를 제공해줍니다(see ?feat_acf() for details):\n\n원 계열의 1차 자기상관계수\n원 계열의 1차-10차 자기상관계수의 제곱합\n1차 차분 계열의 1차 자기상관계수\n1차 차분 계열의 1차-10차 자기상관계수의 제곱합\n2차 차분 계열의 1차 자기상관계수\n2차 차분 계열의 1차-10차 자기상관계수의 제곱합\n(계절 시계열에 대해) 첫번째 계절 시차(seasonal lag)에서의 자기상관계수\n\n\ntourism %>% \n  features(Trips, feat_acf)\n\n\n\n\n\n  \n\n\n\n맨 마지막 열이 첫번째 계절 시차에서의 자기상관계수를 나타내는데, 본 자료의 경우 분기별 자료에 해당하므로 계절 주기는 4에 해당합니다. 즉, 본 자료에서 첫번째 계절 시차에서의 자기상관계수는 원 계열의 시차 4에서의 ACF 값을 나타낸다고 할 수 있습니다.\n\ntourism %>% \n  features(Trips, feat_acf) %>% \n  select(Region:Purpose, season_acf1)\n\n\n\n\n\n  \n\n\n\n원자료에 대한 ACF를 구해보면 다음과 같이 시차 4에서의 자기상관계수와 동일한 값을 가짐을 알 수 있죠:\n\ntourism %>% \n    ACF(Trips)\n\n\n\n\n\n  \n\n\n\n본 튜토리얼에서는 소개하지 않았지만, feature()를 이용한 시계열 feature extraction과 연계하여 다양한 시각화도 수행할 수 있습니다. 꼭 참고해보시기 바랍니다: https://otexts.com/fpp3/stlfeatures.html"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "4 fable",
    "text": "4 fable\n{fable} 패키지는 tsibble 객체와 함께 tidy한 format으로 시계열 예측 모델링을 수행할 수 있게해줍니다. {tidymodels} 패키지에 대한 이해가 있으신 분들이라면 어렵지 않으실거라 생각합니다. {tidymodels}과 마찬가지로 {fable}은 여러 시계열에 대해 여러 시계열 모형에 대한 추정, 비교, 결합, 예측 등을 가능하게해줍니다.\n본격적인 튜토리얼 시작에 앞서, tourism() 자료를 이용할 것이며, 4가지 여행 목적(“business”, “holiday”, “visiting friends and relatives”, “other reasons”)으로 분해할 수 있는 호주 멜버른(Melbourne)의 일별 여행객 수를 예측하는 것에 관심이 있다고 가정합니다. 각 계열의 첫 번째 관측값은 다음과 같습니다:\n\ntourism_melb <- tourism %>% \n  filter(Region == \"Melbourne\")\ntourism_melb %>% \n    group_by(Purpose) %>% \n    slice(1)\n\n\n\n\n\n  \n\n\n\n우리가 추정하고자 하는 변수는 Trips(일별 여행객 수, 단위: 천)입니다. 해당 계열들의 시도표를 보면, 추세와 약한 계절성이 명확하게 존재함을 알 수 있습니다.\n\ntourism_melb %>% \n  autoplot(Trips)\n\n\n\n\n\n\n\n\n{fable} 패키지에서 폭넓게 쓰이는 시계열 예측 모형은 ETS와 ARIMA 모형입니다. 먼저, ETS 모형은 추세 요소와 계절 요소를 가법, 승법, 감쇠효과 등을 반영하여 시계열을 모델링하는 지수평활법(exponential smoothing)을 통계적 모형으로 확장시킨 것에 해당합니다. 통계적 모형으로의 확장은 오차항 \\(\\epsilon_t\\)에 대해 통계적 분포라 할 수 있는, 평균이 0이고 분산이 \\(\\sigma^2\\)인 가우스 백색잡음 과정(gaussian white noise process)을 가정함으로써 이루어집니다. 즉, ETS 모형의 알파벳 각각은 E(error, 오차), T(trend, 추세), S(seasonal, 계절성)을 나타내며, 각 요소들을 모델링하는 방식(가법, 승법, 가법감쇠(damped), 승법감쇠)에 따라 ETS 모형의 종류가 나뉘어집니다. 아울러, 각 모델은 관측된 자료를 설명하는 측정식(measurement equations)과 시간에 따라 변화하는 관측되지 않은 요소(level, trend, seasonal)들을 설명하는 상태식(state equations)으로 구성되는데, 이러한 이유에서 우리는 ETS 모형을 혁신상태공간모형을 이루는 지수평활법(innovations state space models for exponential smoothing)이라고 표현하기도 합니다(See here for detail). 두 번째로, ARIMA 모형은 시계열의 현재값을 과거값과 과거 예측 오차로 설명하는 대표적인 통계적 시계열 예측모형으로, 자세한 설명은 생략하겠습니다. ARIMA 모형에 대한 개념이 없으신 분들은 여기를 참고해주시기 바랍니다.\n두 모형에 대한 간략한 개념 설명은 이쯤에서 마치기로 하고, 이제 이 모형들을 {fable} 패키지를 이용해 어떻게 적합을 수행하면 되는지 보겠습니다. {fable}을 이용한 모형 적합은 model()을 통해 이루어집니다. model()을 통한 적합 과정은 {tidymodels}와 유사하게 상당히 직관적인 이름의 함수들로 이루어집니다. 먼저, ETS()의 경우는 R에서 일반적으로 사용하는 모형식의 specification를 따라서 각 요소를 반영할 수 있게 해주며, 본 예제에서는 추세 요소만 가법적으로 설정해주고 나머지 요소는 자동으로 선택되도록 하였습니다(AICC를 기준으로, see ?ETS for details). 그리고, ARIMA 모형은 ARIMA() 함수로 적합할 수 있으며, 해당 함수는 {forecast} 패키지의 auto.arima와 유사하게 default 옵션으로 AICC 값을 기준으로 최적의 모형을 선택해 줍니다(see ?ARIMA). model()을 통해 적합이 이루어진 모형 객체는 tidy한 포맥의 모형 테이블로 결과를 반환해줍니다. 이를 이제부터 mable(model table) 객체라 칭하겠습니다:\n\nfit <- tourism_melb %>% \n  model(\n    ets = ETS(Trips ~ trend(\"A\")),\n    arima = ARIMA(Trips)\n  )\nfit\n\n\n\n\n\n  \n\n\n\nmable 객체의 행은 각 시계열로 이루어져있으며, 열은 각 모형의 specification을 나타냅니다. fit이 반환하는 결과를 보면 알 수 있듯이, 적합된 ETS 모형의 추세 요소는 모두 가법적으로 고려되었으며, 나머지 요소들은 각 시계열에 따라서 최적의 성분이 자동으로 선택되었습니다. ARIMA 모형 또한 AICC 값을 기준으로 한 최적의 차수들이 반영되어 모형 적합이 잘 이루어진 것으로 보입니다. 이 mable 객체로 우리는 모델 적합 단계에서 필요한 모든 작업을 tidy한 포맷으로 수행할 수 있습니다.\n먼저, coef() 또는 tidy()를 통해 모형으로부터 추정된 계수들을 추출할 수 있습니다. 아울러, 사전에 select() 함수를 통해 특정 모형에 대한 계수 값만을 뽑을 수도 있습니다:\n\nfit %>%\n  select(Region, State, Purpose, arima) %>%\n  coef()\n\n\n\n\n\n  \n\n\n\ntidy로 수행해도 결과는 같습니다. 다음으로 glance()를 이용하면 모형의 적합 결과를 정보 기준(e.g. AIC, BIC)과 잔차의 분산 등으로 요약해줍니다.\n\nfit %>% \n    glance()\n\n\n\n\n\n  \n\n\n\n만약 하나의 모형으로만 시계열 예측 모델링을 수행하고 있다면, report() 함수를 이용하면 됩니다. 이는 하나의 시계열 예측 모형의 평가를 상당히 만족스러운 포맷으로 제공해줍니다.😊 여행 목적이 “Holiday”일 때 ETS 모형을 적합한 결과 대한 요약을 report()를 통해 진행해봤습니다:\n\nfit %>%\n    filter(Purpose == \"Holiday\") %>%\n    select(ets) %>%\n    report()\n\nSeries: Trips \nModel: ETS(M,A,A) \n  Smoothing parameters:\n    alpha = 0.03084501 \n    beta  = 0.03084499 \n    gamma = 0.0001000967 \n\n  Initial states:\n     l[0]      b[0]     s[0]    s[-1]     s[-2]    s[-3]\n 424.0777 -2.535481 -26.7441 4.256618 -10.10668 32.59417\n\n  sigma^2:  0.011\n\n      AIC      AICc       BIC \n 991.7305  994.3020 1013.1688 \n\n\n아울러, 모형으로부터의 적합값과 잔차는 fitted(), residuals() 각각을 이용해 얻을 수 있습니다:\n\nfit %>%\n    fitted()\nfit %>%\n    residuals()\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n적합값과 잔차를 함께 얻고 싶다면 augment()를 사용하세요:\n\nfit %>% \n    augment()\n\n\n\n\n\n  \n\n\n\n모형간 예측 정확도의 비교는 accuracy()를 이용하면 됩니다. 여러 예측 평가 측도를 제공해줍니다:\n\nfit %>% \n    accuracy() %>% \n    arrange(MASE)\n\n\n\n\n\n  \n\n\n\n참고로, 여기서는 훈련 자료(training data)에 대한 예측 성능에 해당합니다. 본 호주 일별 여행객수에 대한 자료에서는 예측 성능 평가 측도를 MASE로 할 경우, ETS 모형이 여행 목적이 “Other”인 경우를 제외하고는 훨씬 더 좋은 성능을 보이고 있습니다. 향후 시점의 예측은 forecast()로 추가적인 자료에 대한 정보 없이 바로 수행을 할 수 있습니다:\n\nfc <- fit %>% \n    forecast(h = \"5 years\")\nfc\n\n\n\n\n\n  \n\n\n\n향후 시점의 예측 결과는 fable(forecast table)로 요약되며, fable은 예측값의 점 추정치와 예측값의 분포에 대한 정보까지 포함하여 제공해줍니다. 예를 들어, 첫 번째 행의 시계열의 예측값의 분포는 평균이 619, 분산이 3533인 정규분포에 해당합니다. 정규분포를 따르는 이유는, 앞서 ETS의 간략한 소개에서 설명했듯이 오차항에 대해 가우스 백색잡음 과정을 가정했기 때문입니다. 그렇다면, 이러한 예측값의 분포에 따른 구간 추정은 어떤 함수로 수행할 수 있을까요? 예측값의 신뢰구간은 hilo()를 이용하면 됩니다. hilo() 함수는 fable 객체와 함께 동작하며, 원하는 신뢰수준을 반영할 수 있게 해줍니다. 다음은 80%, 95% 각각의 신뢰수준에 대한 구간을 추정한 것입니다:\n\nfc %>%\n    hilo(level = c(80, 95))\n\n\n\n\n\n  \n\n\n\n마지막으로, 예측값에 대한 시각화는 fable 객체에 대해 autoplot()을 적용해주면 됩니다:\n\nfc %>% \n  autoplot(tourism_melb)\n\n\n\n\n\n\n\n\n본 튜토리얼에서 소개한 함수들 외에도 {fable}의 특정 모형 객체들과 함께 동작하는 여러 함수들이 있습니다(e.g. refit(), interpolate(), components(), etc). 튜토리얼에서 소개한 내용외에 자세한 내용이 궁금하시다면 Forecasting: Principles and Practices (3rd Ed.)를 참고해주세요."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable.prophet",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable.prophet",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "5 fable.prophet",
    "text": "5 fable.prophet\n{fable.prophet}은 facebook에서 제안한 단일 시계열 예측모형에 대한 적합 또한 tidy한 인터페이스로 제공해줍니다. prophet은 시계열의 시간 종속적인 특성을 고려하는 기존의 시계열 모형(e.g. 지수평활법, ARIMA 모형)과 달리 curve-fitting(e.g. splines)으로 모형을 적합하며, 시계열을 다음과 같이 세 가지 요소로 분해하고 각 요소를 시간의 함수로 가법적으로 모형화합니다.\n\\[\ny(t) = g(t) + s(t) + h(t) + \\epsilon_t\n\\]\n여기서 \\(g(t)\\)는 비주기적 변화를 모형화하는 추세 함수, \\(s(t)\\)는 주별 또는 연별 계절성과 같은 주기적 변화를 반영하며, \\(h(t)\\)는 불규칙하게 발생할 가능성이 있는 휴일효과(holidays and events effects)를 모형화합니다. 세 요소 중에서도 휴일효과에 대한 반영이 prophet의 상당히 특징적인 부분이라 할 수 있겠으며, 모형에서 조절할 수 있는 모수들이 상당히 많아서 아주 유연하고 디테일하게 모델링이 가능합니다. 도메인 지식이 풍부할수록 prophet을 통한 성능 개선의 가능성은 무궁무진합니다. 본 튜토리얼에서 prophet에 대한 개념 설명은 이쯤에서 간략하게 마치겠습니다. prophet을 이번에 처음 접하시는 분들은 여기를 참고해주시기 바랍니다. 개념 정리와 R을 이용한 튜토리얼 과정을 정리해놓았는데, 여기서 소개할 tidy한 인터페이스의 이해를 위해서 꼭 필요로 될겁니다.\n본 튜토리얼에서 prophet을 이용한 예측 모델링에 이용할 자료는 호주의 카페, 레스토랑 및 케이터링 서비스에 관한 월 매출액 자료(단위: milions $AUD)입니다:\n\ncafe <- tsibbledata::aus_retail %>%\n    filter(Industry == \"Cafes, restaurants and catering services\")\nautoplot(cafe)\n\nPlot variable not specified, automatically selected `.vars = Turnover`\n\n\n\n\n\n\n\n\n\n주별로 나뉜 해당 자료의 각 계열은 증가하는 추세와 그에 따른 연별 계절 패턴이 눈에 보입니다. 또한, 계절 패턴의 경우 계열의 수준(level)에 비례하는 형태를 보이고 있으므로, 계절성을 승법적으로 고려해야할 것입니다. 아울러, 월별 자료의 경우는 휴일 효과의 경우 계절 요소를 통해 모형화가 가능합니다. 휴일효과에 대한 반영은 이번에 진행하지 않을 예정입니다(기존의 prophet 인터페이스에서 수행했던 것과 같이 간단하게 반영, see here for details). 본 자료에 대해 추세 요소는 선형으로 하여(default), 연별 계절성을 승법으로 고려하여 prophet을 적합해보았습니다:\n\nfit <- cafe %>%\n  model(\n    prophet = prophet(Turnover ~ season(\"year\", 4, type = \"multiplicative\"))\n  )\nfit\n\n\n\n\n\n  \n\n\n\n각 계열에 대해 prophet이 잘 적합된 것을 확인할 수 있습니다. 적합된 모형의 각 요소들은 components()로 추출할수 있습니다:\n\ncomponents(fit)\n\n\n\n\n\n  \n\n\n\ncomponents()를 통해 주어지는 객체 자체에 autoplot()을 수행하면 모든 요소에 대한 시각화가 한꺼번에 가능하지만, 추세와 월별 계절 패턴에 대해서만 시각화해보겠습니다.\n\ncomponents(fit) %>%\n  ggplot(aes(x = Month, y = trend, colour = State)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\ncomponents(fit) %>%\n  ggplot(aes(x = month(Month), y = year, \n             colour = State, group = interaction(year(Month), State))) + \n  geom_line() + \n  scale_x_continuous(breaks = 1:12, labels = month.abb) + \n  xlab(\"Month\")\n\n\n\n\n\n\n\n\n연별 계절패턴의 경우 주별로 대개 비슷하나, 북방 지역(the Northern Territory)의 경우 다른 주들과는 크게 다른 계쩔 패턴을 보여주고 있습니다. 마지막으로, prophet의 예측도 forecast()를 이용해 쉽게 수행할 수 있습니다. 향후 2년에 대해 예측해보았습니다:\n\nfc <- fit %>% \n  forecast(h = 24)\ncafe %>% \n  ggplot(aes(x = Month, y = Turnover, colour = State)) + \n  geom_line() + \n  autolayer(fc)\n\n\n\n\n\n\n\n\nForecasting: Principles and Practices (3rd Ed.)에서는 prophet외에도, 벡터 자기회귀모형, 인공신경망 기반의 시계열 예측모형, 붓스트랩 및 배깅 기법을 활용한 시계열 예측 모형 등의 고급 시계열 예측 모형도 제공해줍니다. 관심있으신 분들은 fpp3을 참고해보시기 바랍니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#맺음말",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#맺음말",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "맺음말",
    "text": "맺음말\ntidyverts ecosystem이 전반적으로 작동하는 과정을 소개해 보았습니다. 그러나, 시계열 자료의 예측 모델링 대한 이해와 더불어 tidyverts를 좀 더 디테일하게 활용하기 위해서는, Forecasting: Principles and Practices (3rd Ed.)을 참고하시는게 좋을 것이라 생각합니다. tidyverse와 tidymodels를 통해 데이터를 전처리, 예측모형 개발, 개선 등의 과정에 걸리는 시간을 크게 단축시켰듯이, fpp3을 잘 익혀두면 시계열 예측 모델링에 전반적인 과정에 드는 시간을 상당히 단축시킬 수 있을 겁니다.😊\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version date (UTC) lib source\n dplyr         * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n fable         * 0.3.1   2021-05-16 [1] CRAN (R 4.2.0)\n fable.prophet * 0.1.0   2020-08-20 [1] CRAN (R 4.2.0)\n fabletools    * 0.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n feasts        * 0.2.2   2021-06-03 [1] CRAN (R 4.2.0)\n fpp3          * 0.4.0   2021-02-06 [1] CRAN (R 4.2.0)\n ggplot2       * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n lubridate     * 1.8.0   2021-10-07 [1] CRAN (R 4.2.0)\n nycflights13  * 1.0.2   2021-04-12 [1] CRAN (R 4.2.0)\n purrr         * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n Rcpp          * 1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rmarkdown     * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n sessioninfo   * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble        * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr         * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tsibble       * 1.1.1   2021-12-03 [1] CRAN (R 4.2.0)\n tsibbledata   * 0.4.0   2022-01-07 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "",
    "text": "Photo by Makcus Wincler on Unsplash\ntidymodels ecosystem은 R에서 머신러닝을 tidyverse principle로 수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidymodels은 {caret}1을 완벽하게 대체하며, 더 빠르게 그리고 더 직관적인 코드로 모델링을 수행할 수 있습니다. {tidymodels}는 모델링에 필요한 패키지들의 묶음이라고 보면 됩니다. {tidyverse}처럼 {tidymodels}를 로딩하면 모델링에 쓰이는 여러 패키지의 묶음을 불러와줍니다. 그중에는 {ggplot2}와 {dplyr} 같은 {tidyverse}에 포함되는 패키지들도 있습니다. 본격적으로 튜토리얼을 시작하기 전에 필요한 패키지와 데이터를 먼저 불러오겠습니다.\n본 튜토리얼에서 이용할 toy data는 `diamonds{ggplo2}`💎입니다. 해당 데이터는 다이아몬드의 등급과 크기 및 가격에 관한 정보를 갖습니다:\n다음은 우리가 모델링에 사용할 features(\\(X\\))들의 상관계수 행렬을 시각화 한 것이며, 상관계수 행렬을 다이아몬드의 가격(price, \\(y\\)) 열의 상관계수의 절댓값을 기준으로 내림차순 정렬하여 그린 것입니다.\ntoy data를 이용해 {tidymodels}의 전반적인 진행 과정을 보여주는 예제이기 때문에, 상관계수 행렬 그림은 전체 데이터가 아닌 2,000개만을 샘플링하여 그렸습니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-분할-rsample",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-분할-rsample",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "1 데이터 분할: {rsample}",
    "text": "1 데이터 분할: {rsample}\ntidymodels ecosystem을 구성하는 패키지들 중 가장 먼저 소개할 친구는 데이터 분할에 쓰이는 {rsample}입니다. 본 예제의 마지막 단계에서 시험 자료(test data)를 기반으로 모형의 예측 성능을 평가할 것이기 때문에, 먼저 데이터를 훈련 자료(training data), 시험 자료로 분할해야 합니다. 이번에도 모형 적합 및 교차 검증을 이용한 모수 튜닝 단계에서의 계산 비용 절감을 위해, 훈련 자료의 비율을 10%로 낮게 잡아 데이터를 나눌 것입니다. 다음의 모든 과정은 {rsample} 패키지의 함수들로 진행됩니다. 패키지 또는 함수의 이름이 직관적이고 인간 친화적이면 그 역할을 기억하기 쉬운데, 앞으로 소개할 {tidymodels}를 구성하는 패키지와 패키지를 이루는 함수들의 이름은 대부분 이러한 점을 고려하여 네이밍이 되어있습니다.😊\n\nset.seed(1)\ndia_split <- initial_split(diamonds, prop = .1, strata = price)\ndia_train <- training(dia_split)\ndia_test <- testing(dia_split)\ncat(\"the number of observations in the training set is \", \n    nrow(dia_train), \n    \".\\n\",\n     \"the number of observations in the test set is \", \n    nrow(dia_test), \".\", \n    sep = \"\")\n\nthe number of observations in the training set is 5393.\nthe number of observations in the test set is 48547."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-전처리-및-feature-engineering-recipes",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-전처리-및-feature-engineering-recipes",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "2 데이터 전처리 및 Feature Engineering: {recipes}",
    "text": "2 데이터 전처리 및 Feature Engineering: {recipes}\n다음으로는 {recipes}를 이용하여, 데이터 전처리 및 Feature Engineering을 수행한다. recipe는 요리법이라는 뜻뿐만 아니라 특정 결과를 가져올 듯한 방안의 뜻2도 갖습니다. 이럴 때마다 영어권의 R 유저들이 부럽습니다. 패키지나 함수 이름을 통해 그 역할을 기억하고 필요할 때 꺼내쓰기가 좀 더 편하지 않을까 하는 생각이 드네요. {recipes}의 step_*() 함수들을 이용해 모델링에 사용할 자료를 준비3할 수 있습니다. 다음의 산점도는 다이아몬드의 가격(price)과 carat 사이에 비선형적인 관계가 있음을 암시하며, 이러한 관계는 carat의 다항함수를 변수로 도입하여 모델링에 반영할 수 있습니다.\n\nqplot(carat, price, data = dia_train) +\n  scale_y_continuous(trans = log_trans(), labels = function(x) round(x, -2)) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x, 4)\") +\n  labs(title = \"The degree of the polynomial is a potential tuning parameter\")\n\n\n\n\n\n\n\n\nrecipe()는 자료와 모형식을 인수로 하며, step_*() 함수들을 이용하여 step by step👞으로 다양한 전처리를 수행할 수 있게끔 해줍니다.4 여기서는 \\(y\\)에 로그 변환(step_log())을 수행하고, 연속형 예측변수5에 표준화(중심화 및 척도화, step_normalize()), 범주형 예측변수는 더미 변수화(step_dummy())를 수행합니다. 그리고, step_poly()를 이용해 carat의 2차 효과를 반영해주었습니다. 준비가 끝난 recipe 객체는 prep() 함수를 통해 자료에 수행된 전처리들을 확인할 수 있다.\n\ndia_rec <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = 2)\nprep(dia_rec)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nTraining data contained 5393 data points and no missing data.\n\nOperations:\n\nLog transformation on price [trained]\nCentering and scaling for carat, depth, table, x, y, z [trained]\nDummy variables from cut, color, clarity [trained]\nOrthogonal polynomials on carat [trained]\n\n\nrecipe 객체에 prep()를 적용한 것에 juice()를 수행하면 전처리가 수행된 자료를 추출할 수 있죠.\n\ndia_juiced <- juice(prep(dia_rec))\nglimpse(dia_juiced)\n\nRows: 5,393\nColumns: 25\n$ depth        <dbl> 0.52494063, -0.86779062, -0.51960781, 0.59457719, 0.24639…\n$ table        <dbl> -0.2037831, 1.5902131, 0.6932150, -0.2037831, -0.6522821,…\n$ x            <dbl> -1.5716610, -1.5805830, -1.2772351, -1.3218451, -1.277235…\n$ y            <dbl> -1.6114895, -1.5845446, -1.2612061, -1.3061143, -1.261206…\n$ z            <dbl> -1.5470415, -1.6483712, -1.3154309, -1.2575282, -1.243052…\n$ price        <dbl> 5.872118, 5.877736, 6.003887, 6.003887, 6.313548, 6.31716…\n$ cut_1        <dbl> 3.162278e-01, -1.481950e-18, 6.324555e-01, -1.481950e-18,…\n$ cut_2        <dbl> -0.2672612, -0.5345225, 0.5345225, -0.5345225, 0.5345225,…\n$ cut_3        <dbl> -6.324555e-01, -3.893692e-16, 3.162278e-01, -3.893692e-16…\n$ cut_4        <dbl> -0.4780914, 0.7171372, 0.1195229, 0.7171372, 0.1195229, 0…\n$ color_1      <dbl> 3.779645e-01, -5.669467e-01, 3.779645e-01, 3.779645e-01, …\n$ color_2      <dbl> -5.621884e-17, 5.455447e-01, -5.621884e-17, -5.621884e-17…\n$ color_3      <dbl> -4.082483e-01, -4.082483e-01, -4.082483e-01, -4.082483e-0…\n$ color_4      <dbl> -0.5640761, 0.2417469, -0.5640761, -0.5640761, 0.2417469,…\n$ color_5      <dbl> -4.364358e-01, -1.091089e-01, -4.364358e-01, -4.364358e-0…\n$ color_6      <dbl> -0.19738551, 0.03289758, -0.19738551, -0.19738551, 0.0328…\n$ clarity_1    <dbl> 0.07715167, -0.07715167, -0.38575837, -0.23145502, -0.231…\n$ clarity_2    <dbl> -0.38575837, -0.38575837, 0.07715167, -0.23145502, -0.231…\n$ clarity_3    <dbl> -0.1846372, 0.1846372, 0.3077287, 0.4308202, 0.4308202, -…\n$ clarity_4    <dbl> 0.3626203, 0.3626203, -0.5237849, -0.1208734, -0.1208734,…\n$ clarity_5    <dbl> 0.3209704, -0.3209704, 0.4921546, -0.3637664, -0.3637664,…\n$ clarity_6    <dbl> -0.30772873, -0.30772873, -0.30772873, 0.55391171, 0.5539…\n$ clarity_7    <dbl> -0.59744015, 0.59744015, 0.11948803, -0.35846409, -0.3584…\n$ carat_poly_1 <dbl> -0.01605633, -0.01634440, -0.01432792, -0.01432792, -0.01…\n$ carat_poly_2 <dbl> 0.017042209, 0.017792818, 0.012731517, 0.012731517, 0.012…\n\n\n또한, recipe 객체에 prep()를 적용한 것에 juice()가 아닌 bake()를 수행하면 새로운 자료에 recipe 객체에 수행했던 것과 같은 전처리를 수행할 수 있습니다. 예를 들어, 다음은 시험 자료에 대해 훈련 자료에 수행한 전처리를 수행한 뒤에 해당 자료를 추출하라는 것과 같죠. 시험 자료의 예측을 통한 모형의 성능평가에는 사전에 훈련자료와 동일한 전처리가 필요로되는데, bake()는 이러한 시간을 크게 단축시켜줍니다.\n\nglimpse(\n  bake(prep(dia_rec), dia_test)\n)\n\nRows: 48,547\nColumns: 25\n$ depth        <dbl> -0.1714250, -1.3552466, -3.3747069, 0.4553041, 1.0820331,…\n$ table        <dbl> -1.1007812, 1.5902131, 3.3842094, 0.2447160, 0.2447160, -…\n$ x            <dbl> -1.589505, -1.643037, -1.500285, -1.366455, -1.241547, -1…\n$ y            <dbl> -1.575563, -1.701306, -1.494728, -1.351022, -1.243243, -1…\n$ z            <dbl> -1.604944, -1.778652, -1.778652, -1.315431, -1.141723, -1…\n$ price        <dbl> 5.786897, 5.786897, 5.789960, 5.811141, 5.814131, 5.81711…\n$ cut_1        <dbl> 6.324555e-01, 3.162278e-01, -3.162278e-01, 3.162278e-01, …\n$ cut_2        <dbl> 0.5345225, -0.2672612, -0.2672612, -0.2672612, -0.2672612…\n$ cut_3        <dbl> 3.162278e-01, -6.324555e-01, 6.324555e-01, -6.324555e-01,…\n$ cut_4        <dbl> 0.1195229, -0.4780914, -0.4780914, -0.4780914, -0.4780914…\n$ color_1      <dbl> -3.779645e-01, -3.779645e-01, -3.779645e-01, 3.779645e-01…\n$ color_2      <dbl> 8.914347e-17, 8.914347e-17, 8.914347e-17, -5.621884e-17, …\n$ color_3      <dbl> 4.082483e-01, 4.082483e-01, 4.082483e-01, -4.082483e-01, …\n$ color_4      <dbl> -0.5640761, -0.5640761, -0.5640761, -0.5640761, 0.2417469…\n$ color_5      <dbl> 4.364358e-01, 4.364358e-01, 4.364358e-01, -4.364358e-01, …\n$ color_6      <dbl> -0.19738551, -0.19738551, -0.19738551, -0.19738551, 0.032…\n$ clarity_1    <dbl> -0.38575837, -0.23145502, 0.07715167, -0.07715167, -0.385…\n$ clarity_2    <dbl> 0.07715167, -0.23145502, -0.38575837, -0.38575837, 0.0771…\n$ clarity_3    <dbl> 0.3077287, 0.4308202, -0.1846372, 0.1846372, 0.3077287, -…\n$ clarity_4    <dbl> -0.5237849, -0.1208734, 0.3626203, 0.3626203, -0.5237849,…\n$ clarity_5    <dbl> 0.4921546, -0.3637664, 0.3209704, -0.3209704, 0.4921546, …\n$ clarity_6    <dbl> -0.30772873, 0.55391171, -0.30772873, -0.30772873, -0.307…\n$ clarity_7    <dbl> 0.11948803, -0.35846409, -0.59744015, 0.59744015, 0.11948…\n$ carat_poly_1 <dbl> -0.01634440, -0.01692054, -0.01634440, -0.01461599, -0.01…\n$ carat_poly_2 <dbl> 0.01779282, 0.01932160, 0.01779282, 0.01342699, 0.0120452…"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-정의-및-적합-parsnip",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-정의-및-적합-parsnip",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "3 모형 정의 및 적합: {parsnip}",
    "text": "3 모형 정의 및 적합: {parsnip}\n이제 훈련 자료에 대한 기본적인 전처리가 끝났으므로, {parsnip}을 이용하여 모형을 정의하고 적합하려고 합니다. {parsnip}은 우리나라 말로 연노란색의 긴 뿌리채소를 뜻하는데, 왜 이렇게 네이밍이 된 지는 아직 잘 모르겠습니다. 영어권의 원어민들은 어떻게 생각할지 궁금하네요. {parsnip}은 인기 있는 수많은 머신러닝 알고리즘6을 제공해줍니다. 그리고, 최대 장점은 단일화된 인터페이스로 여러 모형을 적합할 수 있다는 점이죠. 예를 들어, 랜덤포레스트를 제공하는 두 패키지 {ranger}와 {randomForest}에는 고려할 트리의 개수를 지정하는 모수가 존재하는데 해당 옵션의 이름이 각각 ntree, num.trees로 다릅니다. 이는 사용자들에게 꽤 불편한 점일 수 있는데, {parsnip}은 이러한 문제를 해결해줌으로써 두 인터페이스를 모두 기억할 필요가 없게끔 해줍니다.\n{parsnip}에서는 먼저 특정 함수를 통해 모형을 정의하고7, set_mode()로 어떤 문제8를 해결할 것인지 설정한 뒤에, 마지막으로 어떤 시스템 또는 패키지를 이용하여 해당 모형을 적합할지를 set_engine()으로 설정합니다. 여기서는 먼저 stats::lm() 엔진을 이용하여 기본적인 회귀모형으로 적합을 시작해 보겠습니다.\n\nlm_model <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\n본격적인 모형 적합 전에, 앞서 언급했던 {parsnip}의 장점을 확인해보기 위해 랜덤포레스트를 예로 들어보겠습니다. 랜덤포레스트 모형의 적합에는 {ranger} 또는 {randomForest}를 이용할 수 있는데, 서로 조금 다른 인터페이스를 지닌다고 했었습니다. {parsnip}은 다음과 같이 엔진 설정 전에 {parsnip}만의 함수로 먼저 모형을 정의하고 해당 함수에서 모수를 설정함으로써 서로 다른 인터페이스를 통합하여줍니다.\n\nrand_forest(mtry = 3, trees = 500, min_n = 5) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\", importance = \"impurity_corrected\")\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 500\n  min_n = 5\n\nEngine-Specific Arguments:\n  importance = impurity_corrected\n\nComputational engine: ranger \n\n\n이제 다시 회귀모형으로 돌아오겠습니다. 설정했던 기본적인 회귀모형을 전처리를 완료한 훈련 자료에 적합해 줍니다.\n\nlm_fit1 <- fit(lm_model, price ~ ., dia_juiced)\nlm_fit1\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = price ~ ., data = data)\n\nCoefficients:\n (Intercept)         depth         table             x             y  \n   7.7110881     0.0582418     0.0138979     0.8357574     0.2337963  \n           z         cut_1         cut_2         cut_3         cut_4  \n   0.0532288     0.1134826    -0.0282144     0.0315527    -0.0020513  \n     color_1       color_2       color_3       color_4       color_5  \n  -0.4452258    -0.0887138    -0.0090620     0.0071217    -0.0059503  \n     color_6     clarity_1     clarity_2     clarity_3     clarity_4  \n  -0.0001745     0.9025208    -0.2480065     0.1424917    -0.0664178  \n   clarity_5     clarity_6     clarity_7  carat_poly_1  carat_poly_2  \n   0.0265924     0.0031308     0.0245773    -3.1129423    -6.9995161  \n\n\n예제에서 사용되진 않았지만, step_rm()을 이용하여 사전에 모델링에 필요 없는 변수는 제거할 수도 있습니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#적합된-모형-요약-broom",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#적합된-모형-요약-broom",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "4 적합된 모형 요약: {broom}",
    "text": "4 적합된 모형 요약: {broom}\nR에서 여러 모형 객체들의 요약은 summary() 또는 coef()와 같은 함수로 이루어집니다. 그러나, 이러한 함수들의 출력물은 타이디한 포맷9으로 주어지지 않습니다. {broom} 패키지는 적합 된 모형의 요약을 타이디한 포맷으로 제공해줍니다. broom은 빗자루와 같은 브러쉬를 의미하는 명사인데, 적합한 모형을 깨끗하게 쓸어 담는 패키지라고 생각하면 기억하기 쉽지 않을까 싶습니다. 이와 같이 패키지 이름, 함수 이름 하나하나를 신중하게 네이밍하는 일관성은 {tidyverse}, {tidymodels}에 포함되는 패키지들의 공통된 좋은 특징이라 할 수 있다. 실제로 R4DS10 책에서도 Hadley Wickham은 객체의 이름이나 함수의 이름을 설정하는 것에 있어서 어느정도의 시간을 투자하는 것은 전혀 아깝지 않다고 말하기도 했습니다.\n{broom} 패키지를 구성하는 첫 번째 함수로 glance()를 소개합니다. glance는 힐끗 본다는 뜻을 갖는다는 점에서 추측할 수 있듯이, 적합된 모형의 전체적인 정보를 간략히 제공해줍니다.\n\nglance(lm_fit1$fit)\n\n\n\n\n\n  \n\n\n\n적합된 모형의 수정된 \\(R^2\\) 값(adj.r.squared)은 약 98.27%로 상당히 높은 설명력을 보여줍니다. RMSE는 sigma 열에서 확인할 수 있습니다. 다음으로 tidy()는 추정된 모수에 대한 정보를 제공합니다. 다음의 결과에서 우리는 carat의 2차 효과가 유의하게 존재함을 알 수 있습니다. 통계량의 크기를 기준으로 내림차순으로 정렬하여 표시하였습니다.\n\ntidy(lm_fit1) %>% \n    arrange(desc(abs(statistic)))\n\n\n\n\n\n  \n\n\n\n마지막으로 augment()는 모형의 예측값, 적합값 등을 반환해줍니다. augment는 우리나라 말로 어떤 것의 양 또는 값, 크기 등을 늘리는 것11을 뜻하는 동사로, 해당 함수도 이름을 통해 어느정도 그 역할을 가늠할 수 있죠.\n\nlm_predicted <- augment(lm_fit1$fit, data = dia_juiced) %>% \n  rowid_to_column()\nselect(lm_predicted, rowid, price, .fitted:.std.resid)\n\n\n\n\n\n  \n\n\n\n앞서 생성한 lm_predicted 객체를 이용해 적합값과 관측값 간의 산점도를 그려보았습니다. 잔차의 크기가 2 이상인 관측치에 대해서는 해당 관측치의 행 번호를 붙여주었으며, 겹치는 점이 있는 경우를 고려하여 점에 투명도를 주었습니다.\n\nggplot(lm_predicted, aes(.fitted, price)) +\n  geom_point(alpha = .2) +\n  ggrepel::geom_label_repel(aes(label = rowid),\n                            data = lm_predicted %>% filter(abs(.resid) > 2)) +\n  labs(x = \"fitted values\",\n       y = \"observed values\")\n\n\n\n\n\n\n\n\n원자료의 각 행을 의미하는 두 단어 관측값(observed values)과 실제값(actual values)은 서로 통용되니 어떤 용어를 써도 문제가 없습니다. 특히, 머신러닝에서는 이를 데이터포인트(data point)라고 표현하기도 합니다. 3가지 용어 모두 통용되는 말이니 몰랐다면 알아둡시다. 모든 학문에서 그렇겠지만 통계학에서는 특히 정확한 용어 정의가 중요하므로, 비슷한 용어 또는 비슷한 듯 다른 용어들이 있다면 틈틈이 정리하는 습관을 갖는 것이 좋다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-성능-평가-yardstick",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-성능-평가-yardstick",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "5 모형 성능 평가: {yardstick}",
    "text": "5 모형 성능 평가: {yardstick}\n위에서 glance()를 통해 적합된 모형의 성능을 RMSE, \\(R^2\\)를 통해 힐끗 확인할 수 있었습니다. {yardstick}은 모형의 성능에 대한 여러 측도를 계산하기 위한 패키지입니다. 물론, \\(y\\)가 연속형이든 범주형이든 문제없으며 교차 검증(Cross Validation, CV)에서 생산되는 그룹화된 예측값들과도 매끄럽게 잘 작동한다. yardstick은 기준, 척도를 뜻하는 명사에 해당하므로, 기억하기도 쉬울 것이라 생각합니다. 이제는 {rsample}, {parsnip}, {yardstick}으로 교차 검증을 수행하여 좀 더 정확한 RMSE를 추정해봅시다.\n다음 코드 블럭들에서 나타날 긴 파이프라인(pipeline, %>%)들을 정리해서 간략히 나타내면 다음과 같습니다. 천천히 음미해보시기 바랍니다:\n\nrsample::vfold_cv()를 훈련용 자료를 3-fold CV를 수행할 수 있도록 분할\nrsample::analysis()와 rsample::assessment()를 이용해 각 분할에서 모형 훈련용, 평가용 자료를 불러옴\n앞서 만든 모형 적합 전 전처리가 완료된 recipe 객체 dia_rec을 각 fold의 모형 훈련용 자료에 prepped 시킴\npreped한 훈련용 자료를 recipes::juice()로 불러오고, recipes::bake()를 이용해 훈련용 자료에 처리한 것과 같은 처리를 평가용 자료에 수행\nparsnip::fit()으로 3개의 모형 적합용(analysis) 자료 각각에 모형을 적합(훈련)\npredicted()로 훈련시킨 각 모형으로 평가용(assessment) 자료를 예측\n\n\nset.seed(1)\ndia_vfold <- vfold_cv(dia_train, v = 3, strata = price)\ndia_vfold\n\n\n\n\n\n  \n\n\n\n\nlm_fit2 <- mutate(dia_vfold,\n                  df_ana = map(splits, analysis),\n                  df_ass = map(splits, assessment))\nlm_fit2\n\n\n\n\n\n  \n\n\n\n\nlm_fit3 <- lm_fit2 %>% \n  mutate(\n    recipe = map(df_ana, ~prep(dia_rec, training = .x)),\n    df_ana = map(recipe, juice),\n    df_ass = map2(recipe,\n                  df_ass, ~bake(.x, new_data = .y))) %>% \n  mutate(\n    model_fit = map(df_ana, ~fit(lm_model, price ~ ., data = .x))) %>% \n  mutate(\n    model_pred = map2(model_fit, df_ass, ~predict(.x, new_data = .y)))\nselect(lm_fit3, id, recipe:model_pred)\n\n\n\n\n\n  \n\n\n\n여기서 tidymodels ecosystem의 마법을 확인할 수 있습니다. 위 과정에서 확인했다시피, 꽤 복잡한 과정들이 단 하나의 티블 객체 lm_fit2에서 이루어졌습니다. 이렇게 복잡한 작업이 단 하나의 티블 객체만으로 이루어질 수 있었던 이유는, 티블은 리스트-열(list-column)을 가질 수 있기 때문이죠. 덕분에 우리는 R에서 연산이 느린 반복문(e.g. for(), while())을 사용하지 않고, purrr::map()을 loop로 이용하여 반복문을 통한 지루하고 느린 모델링 작업을 완벽한 함수형 프로그래밍으로 수행할 수 있게 되었습니다. R 사용자라면 어디서 한번 쯤은 반복문의 사용은 지양하고, 함수형 프로그래밍을 해야 한다고 들어봤을 것입니다. {tidymodels}이 모델링 과정을 {tidyverse}와 함께 작동할 수 있게 해줌으로써, 한 자료에 대해서 여러 가지 모형의 적합, 교차검증을 통한 모수 튜닝, 예측 성능평가 등의 작업을 통해 경험적으로(empirically) 최적의 모형을 선택하는 수고가 필요한 머신러닝에 드는 시간을 상당히 줄여줬다고 할 수 있습니다.\n이쯤 되면 제가 왜 {tidyverse}를 좋아하고, {tidymodels}의 튜토리얼을 이렇게 상세하게 기술하는지 이해하실 거라고 생각합니다. 이제 평가용 자료로부터 실제 관측값(price)을 추출하여 예측값(.pred)과 비교한 뒤, yardstick::metrics()를 이용해 여러 평가 측도를 계산해보려고 합니다.\n\nlm_preds <- lm_fit3 %>% \n  mutate(res = map2(df_ass, model_pred, ~data.frame(price = .x$price, \n                                                    .pred = .y$.pred))) %>% \n  select(id, res) %>% \n  tidyr::unnest(res) %>% \n  group_by(id)\nlm_preds\n\n\n\n\n\n  \n\n\n\n\nmetrics(lm_preds, truth = price, estimate = .pred)\n\n\n\n\n\n  \n\n\n\n여기서 계산한 평가 측도의 값은 out-of-sample에 대한 성능이므로 모형 적합값에 대해 평가 측도를 계산한 glance(lm_fit1$fit)의 결과와 비교하여 보면 당연히 조금은 떨어지는 성능을 보입니다. metrics()는 연속형 outcome(\\(y\\))에는 위와 같이 RMSE, \\(R^2\\), MAE를 기본적인 측도로 제공해줍니다. 물론, 범주형 outcome에 대해서도 다른 기본적인 측도를 제공해주죠. 또한, 하나의 측도만으로 비교하길 원한다면 rmse()와 같이 RMSE 값만을 제공해주는 함수도 이용할 수 있으며, metric_set()을 이용하면 원하는 metrics들을 직접 커스텀하여 정의할 수도 있습니다.\n3-fold CV를 통해 훈련 자료를 분할 및 전처리하고 예측값을 구하여 RMSE를 계산하는 과정을 담은 앞선 코드블럭들은 {tidyverse}, {tidymodels}에 익숙한 사람이라면 편하게 읽어나가실 수 있을겁니다. 그러나, 코드가 매우 긴 것도 사실입니다. 사실, 위 코드블럭은 다음 섹션에서 소개할 {tune} 패키지를 이용하면 다음과 같이 단 몇 줄로 간결하게 코딩할 수 있습니다.\n\ncontrol <- control_resamples(save_pred = TRUE)\nset.seed(1)\nlm_fit4 <- fit_resamples(lm_model, dia_rec, dia_vfold, control = control)\nlm_fit4 %>% \n    pull(.metrics)\n\n\n\n[[1]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.143 Preprocessor1_Model1\n2 rsq     standard       0.980 Preprocessor1_Model1\n\n[[2]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.127 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1\n\n[[3]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.130 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형의-모수-튜닝-tune-dials",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형의-모수-튜닝-tune-dials",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "6 모형의 모수 튜닝: {tune}, {dials}",
    "text": "6 모형의 모수 튜닝: {tune}, {dials}\ntune은 조정하다12 라는 뜻을 갖는 동사이며, 말 그대로 {tune} 패키지는 모수를 튜닝(조율)하는(e.g. via grid search) 함수들을 제공합니다. 그리고, 어떤 것을 조정하는 다이얼13을 의미하는 이름을 갖는 {dials} 패키지는 {tune}을 통해 튜닝할 모수들을 정하는 역할을 합니다. 즉, {tune}과 {dials}는 대개 함께 쓰이는 패키지라고 보면 됩니다. 본 예제에서는 랜덤포레스트 모형을 튜닝하는 과정을 보여줄 것입니다.\n\n6.1 튜닝을 위한 {parsnip} 모형 객체 준비\n첫 번째로, 랜덤포레스트 모형을 형성할 때 매 트리 적합시 고려할 변수들의 개수를 조정하는 mtry 모수를 조율해줍니다. tune()을 placeholder로 하여 후에 교차검증을 통해 최적의 mtry를 선정할 입니다.\n다음 코드블럭의 출력물은 mtry의 기본 최솟값은 1이고 최댓값은 자료에 의존함을 의미합니다. 어떤 자료를 다루느냐에 따라 feature의 수는 다르므로, 따로 지정하지 않는한 mtry의 최댓값은 자료에 의존하게 됩니다.\n\nrf_model <- rand_forest(mtry = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\nparameters(rf_model)\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[?]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\nmtry()\n\n# Randomly Selected Predictors (quantitative)\nRange: [1, ?]\n\n\n아직 랜덤포레스트 모형의 적합에 쓰이는 모수 값을 결정하지 않았으므로 모형을 훈련 자료에 적합할 준비가 된 상태가 아니라고 할 수 있습니다. 그리고, mtry의 최댓값은 update()를 사용해 원하는 값을 명시할 수도 있고, 또는 finalize()를 사용해 해당 자료가 갖는 예측변수의 수로 지정할 수도 있죠.\n\nrf_model %>% \n  parameters() %>% \n  update(mtry = mtry(c(1L, 5L)))\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[+]\n\n\n\nrf_model %>% \n  parameters() %>% \n  finalize(x = juice(prep(dia_rec)) %>% select(-price)) %>% \n  pull(\"object\")\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, 24]\n\n\n\n\n6.2 튜닝을 위한 자료 준비: {recipes}\n두 번째로 튜닝하고 싶은 것은 carat의 다항식 차수입니다. 2 데이터 전처리 및 Feature Engineering: {recipes}의 그림에서 확인했듯이, 최대 4차까지의 다항식이 자료에 잘 적합 될 수 있음을 알 수 있습니다. 그러나, 우리는 모수 절약의 원칙(priciplt of parsimony)14을 생각할 필요가 있고, 그에 따라 더 간단한 모형도 자료에 잘 적합 될 수 있다는 가능성을 배제해서는 안됩니다. 그래서, carat의 다항식 차수 또한 교차 검증을 통해 최대한 간단하면서 좋은 성능을 내는 carat의 차수를 찾을 것입니다.\n모형의 적합에서 각 모형이 갖는 고유한 초모수15와 달리 예측변수 carat의 차수는 {recipe}를 통해 새로운 레시피 객체를 만들어 튜닝이 진행됩니다. 그 과정은 초모수를 튜닝했던 과정과 유사합니다. 다음과 같이 step_poly()에 tune()을 사용하여 훈련 자료(dia_train())에 대한 2번째 레시피 객체를 만들어 줍니다.\n\ndia_rec2 <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = tune())\n\ndia_rec2 %>% \n  parameters() %>% \n  pull(\"object\")\n\nWarning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\n[[1]]\nPolynomial Degree (quantitative)\nRange: [1, 3]\n\n\n고려하는 다항식의 차수 범위가 기본값으로 설정하여 [1, 3]으로 되어있는데, 이 부분은 다음 섹션에서 {workflows} 패키지를 소개하며 개선할 부분이니 신경 쓰지 않으셔도 됩니다.\n\n\n6.3 모든 것을 결합하기: {workflows}\nworkflow를 직역하면 어떤 작업의 흐름을 뜻하듯이, {workflows} 패키지는 recipe나 model 객체와 같은 머신러닝 파이프라인의 다른 부분이라 할 수 있는 것들을 한 번에 묶어주는 역할을 합니다.\n이를 위해서는 먼저 workflow()를 선언하여 객체를 만들고, 6.2 튜닝을 위한 자료 준비: {recipes}에서 만든 recipe 객체와 6.1 튜닝을 위한 {parsnip} 모형 객체 준비에서 만든 랜덤포레스트 모형 객체를 add_*()로 결합해줍니다.\n\nrf_wflow <- workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(dia_rec2)\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_dummy()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger \n\n\n아직 mtry의 최댓값이 알려져있지 않고 degree의 최댓값이 기본 설정인 3으로 설정되어 있으므로, 두 번째로는 rf_wflow 객체의 모수 설정을 update()로 갱신할 것입니다.\n\nrf_param <- rf_wflow %>% \n  parameters() %>% \n  update(mtry = mtry(range = c(3L, 5L)),\n         degree = degree_int(range = c(2L, 4L)))\n\nWarning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\nrf_param %>% pull(\"object\")\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [3, 5]\n\n[[2]]\nPolynomial Degree (quantitative)\nRange: [2, 4]\n\n\n앞서 말했듯이 교차검증을 통해 튜닝을 수행할 것이기 때문에, 세 번째로는 설정한 모수들의 조합을 만들어야 합니다. 복잡한 튜닝 문제에는 tune_bayes()를 통한 베이지안 최적화(Bayesian optimization)(Silge 와/과 Julia, 일자 없음)가 추천되지만, 해당 예제에서 고려하는 초모수들의 조합 정도는 grid search로도 충분해 보입니다. 다음과 같이 필요로 되는 모든 모수 조합의 grid를 만듭니다.\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid\n\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid %>% \n    paged_table()\n\n\n\n  \n\n\n\n여기서 levels는 grid를 만드는 데 사용되는 각 모수의 수에 대한 정숫값을 조정하는 옵션입니다. default 값이 levels = 3이므로 해당 옵션은 생략해도 문제없을 것입니다. 교차 검증을 통한 모수 튜닝에는 수많은 모형을 적합해야 하는데, 이 예제에서는 9개의 모수 집합과 3개의 folds를 사용하므로 총 \\(3 \\times 9 = 27\\)개의 모형을 적합해야 한다. 27개의 모형을 빠르게 적합하기 위해 병렬처리를 수행하려고 합니다. 이는 {tune} 패키지에서 직접적으로 지원받을 수 있습니다.\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: future\n\n\n\nAttaching package: 'future'\n\n\nThe following object is masked from 'package:rmarkdown':\n\n    run\n\nall_cores <- parallel::detectCores(logical = FALSE) - 1\n\nregisterDoFuture()\ncl <- parallel::makeCluster(all_cores)\nplan(future::cluster, workers = cl)\n\n이제 튜닝을 시작합니다.\n\noptions(future.rng.onMisue = \"ignore\")\nrf_search <- tune_grid(rf_wflow, grid = rf_grid, resamples = dia_vfold,\n                       param_info = rf_param)\n\n튜닝 결과는 autoplot()과 show_best()로 검토할 수 있습니다:\n\nautoplot(rf_search, metric = \"rmse\")\n\n\n\n\n\n\n\n\n\\(x\\) 축은 mtry를 나타내며, 각 선의 색상은 고려한 다항식 차수를 나타냅니다. mtry는 5와 carat의 2차항까지 고려한 초모수 조합이 최적임을 알 수 있습니다. show_best()로도 확인할 수 있습니다:\n\nshow_best(rf_search, \"rmse\", n = 9)\n\n\n\n\n\n  \n\n\n\n\nselect_best(rf_search, metric = \"rmse\")\n\n\n\n\n\n  \n\n\n\n그리고, select_by_one_std_err()을 이용하면 원하는 metric 값의 \\(\\pm 1SE\\)를 고려한 최적의 초모수 조합을 얻을 수도 있죠.\n\nselect_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\n\n\n\n\n\n  \n\n\n\n\n\n6.4 선택한 최적의 모형으로 예측 수행\n6.3 모든 것을 결합하기: {workflows}에서 carat 변수는 2차항으로도 충분히 설명되고, 매 트리 적합 시 고려할 변수의 수는 5개임을 확인할 수 있었습니다. 이제는 해당 초모수 조합을 이용해 훈련 자료에 모형을 적합하고 최종 예측을 수행하려고 합니다. 이번 예제에서는 설정값이 똑같긴 하지만, \\(\\pm 1SE\\)를 고려한 초모수 조합을 모형 적합에 사용하였습니다.\n\nrf_param_final <- select_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\nrf_wflow_final <- finalize_workflow(rf_wflow, rf_param_final)\nrf_wflow_final_fit <- fit(rf_wflow_final, data = dia_train)\n\n이제 적합된 모형객체 rf_wflow_final_fit으로 원하는 unobserved 자료16를 predict()로 예측할 수 있다. 우리에게는 미리 나눠둔 시험 자료 dia_test가 있습니다. 다만, dia_test의 \\(y\\)는 로그변환이 취해지지 않았으므로, predict(rf_wflow_final_fit, new_data = dia_test)가 아닌 {recipe}로 step_log()를 취해주어야 합니다. 여기서는 workflow로부터 추출한 prepped된 recipe 객체를 이용해 시험 자료에 대하여 bake()를 취할 것입니다. 그리고, baked된 시험 자료를 적합한 최종 모형을 통해 예측할하면 되죠. bake()가 이렇게나 편합니다:\n\ndia_rec3 <- pull_workflow_prepped_recipe(rf_wflow_final_fit)\nrf_final_fit <- pull_workflow_fit(rf_wflow_final_fit)\n\ndia_test$.pred <- predict(rf_final_fit,\n                          new_data = bake(dia_rec3, dia_test)) %>% pull(.pred)\ndia_test$logprice <- log(dia_test$price)\n\nmetrics(dia_test, truth = logprice, estimate = .pred)\n\n\n\nWarning: `pull_workflow_prepped_recipe()` was deprecated in workflows 0.2.3.\nPlease use `extract_recipe()` instead.\n\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\n\n\n\n  \n\n\n\n시험 자료에 대한 RMSE는 약 0.11로 교차 검증에서 계산된 RMSE보다는 조금 더 나은 성능을 보여줍니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#맺음말",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#맺음말",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "맺음말",
    "text": "맺음말\n{tidymodels}의 ecosystem은 머신러닝 문제를 풀기 위해 필요한 첫 단계부터 끝까지 함께 작동하는 패키지들의 집합을 한대 묶어 제공해줍니다. 또한, {tidyverse}를 통한 data-wrangling 기능과 훌륭한 시각화 패키지 {ggplot2}와도 함께 작동하는 {tidymodels}은 R을 사용하는 데이터 사이언티스트들에게는 더없이 풍부한 toolbox라 할 수 있을 것 같습니다. 아울러, 해당 튜토리얼에서는 예측 모형들을 결합해주는17 기능을 갖는 패키지 {stacks}에 대한 내용을 다루지 않았는데18, {tidymodels}을 불러올 때 로딩이 되는 패키지는 아니지만, {stacks} 또한 {tidymodels}의 한 부분으로 소개되는 패키지에 해당합니다. 그리고, tidymodels ecosystem을 “머신러닝”에만 국한시키기에는 너무나도 많은 기능들이 업데이트되고 있습니다. 최근엔 반복측정자료분석에 자주 쓰이는 모형 중 하나인 혼합효과모형(linear mixed model)까지 지원하기 시작했습니다:\n\n\nLots of new #rstats package versions! Here’s a summary for the parsnip packages, including the new {multilevelmod} package!https://t.co/rv5Z9izpho— Max Kuhn (@topepos) March 24, 2022\n\n\n\ntidyverse 블로그를 꼭 팔로우업하세요. 본 튜토리얼은 20년 2월에 작성된 글을 기반으로 쓰여졌기 때문에 최신이라고 하긴 어렵습니다.😂 그러나, tidymodels ecosystem의 기본기를 익히기에는 충분할 겁니다. 이 튜토리얼이 {tidymodels}을 배우길 원하는, R로 머신러닝을 수행하길 원하는 우리나라 R 유저들에게 조금이나마 도움이 됐으면 좋겠습니다.\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n corrplot     * 0.92    2021-11-18 [1] CRAN (R 4.2.0)\n dials        * 1.0.0   2022-06-14 [1] CRAN (R 4.2.0)\n doFuture     * 0.12.2  2022-04-26 [1] CRAN (R 4.2.0)\n dplyr        * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n foreach      * 1.5.2   2022-02-02 [1] CRAN (R 4.2.0)\n future       * 1.27.0  2022-07-22 [1] CRAN (R 4.2.0)\n ggplot2      * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n ggrepel      * 0.9.1   2021-01-15 [1] CRAN (R 4.2.0)\n infer        * 1.0.2   2022-06-10 [1] CRAN (R 4.2.0)\n modeldata    * 1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n parsnip      * 1.0.0   2022-06-16 [1] CRAN (R 4.2.0)\n purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n recipes      * 1.0.1   2022-07-07 [1] CRAN (R 4.2.0)\n rmarkdown    * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n rmdformats   * 1.0.4   2022-05-17 [1] CRAN (R 4.2.0)\n rsample      * 1.0.0   2022-06-24 [1] CRAN (R 4.2.0)\n scales       * 1.2.0   2022-04-13 [1] CRAN (R 4.2.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble       * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidymodels   * 1.0.0   2022-07-13 [1] CRAN (R 4.2.0)\n tidyr        * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tune         * 1.0.0   2022-07-07 [1] CRAN (R 4.2.0)\n tweetrmd     * 0.0.9   2022-09-13 [1] Github (gadenbuie/tweetrmd@075102b)\n workflows    * 1.0.0   2022-07-05 [1] CRAN (R 4.2.0)\n workflowsets * 1.0.0   2022-07-12 [1] CRAN (R 4.2.0)\n yardstick    * 1.0.0   2022-06-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html",
    "title": "관심 논문 읽고 요약하기",
    "section": "",
    "text": "Photo by Aaron Burden on Unsplash\n논문 읽기가 초심자에게는 만만치않은 작업인 만큼, 논문을 정리하는 자기만의 방식을 만들어 놓는 것은 참 중요합니다. 저 또한 아직 초심자라고 생각하고 있는데요, 오늘은 제가 관심있는 논문을 읽고 정리하는 방식에 대해 얘기해보려고 합니다. 제가 스스로 터득한 방법을 소개드리는 것은 아닙니다. 논문을 읽고 정리하는 좋은 방식이 있나 싶어 검색을 하던 도중 좋은 글(An 2022)을 발견하게 됐고, 이 글을 바탕으로 저만의 방식을 정립해봤습니다. 좋은 글을 써주신 안수빈님께 감사의 마음을 전합니다.\n논문 요약은 1st read(첫 번째 읽기), 2nd read(두 번째 읽기) 2가지 섹션으로 진행할 것입니다. 1st read의 결과에 따라 2nd read는 진행되지 않을 수도 있습니다."
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#st-read",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#st-read",
    "title": "관심 논문 읽고 요약하기",
    "section": "1st read",
    "text": "1st read\n첫 번째 읽기의 핵심은 빠르게 읽으며 논문의 큰 그림을 파악하는 것이라고 합니다. 5분에서 10분 정도 다음 순서에 따라 읽으라고 권합니다.\n\n제목(title), 초록(abstract), 소개(introduction)를 집중해서 읽으세요.\n섹션(section)과 하위 섹션(subsection)의 세부내용은 무시하고 제목만 읽으세요.\n결론(conclusion)을 읽으세요.\n참고문헌(reference)을 보며 저자가 인용한 논문, 이전에 읽은 논문에 대해 가볍게 체크하세요.\n\n먼저 1st read에서는 논문을 빠르게 읽으면서 파악한 전반적인 그림에 관해 기술합니다. 첫 번째 읽기를 하고나서는 다음의 여섯 가지(5C + 1M)를 답할 줄 알아야 하며, 본 블로그에서 요약할 논문들 또한 다음과 같은 섹션으로 요약하려고 합니다.\n\nCategory: 논문의 종류\nMain Topic: 논문 제목 및 주제\nContext: 다른 페이퍼들과의 관계, 문제를 풀기 위해 사용한 이론적 바탕\nCorrectness: 논문에 필요한 가정의 명확성\nContributions: 논문의 핵심 기여\nClarity: 논문의 가독성, 명료함\n\n논문에 필요한 가정의 명확성(Correctness)은 제 경우 보통 방법론 부분에서 모델에서 요구하는 가정이나 모델링 과정의 각 단계가 합리적인 근거로 진행 되었는지에 관심이 있으므로, 첫 번째 읽기에서 Method 부분을 빠르게 검토해보는 과정이 필요로 될 것 같습니다. 아울러, 논문의 가독성과 명확성(Clarity)에 관한 부분은 잘 아는 분야가 아니라면 감히 기술하기 어려울 것 같습니다. 때때로 생략할 수도 있는 부분입니다.😅 그리고, 논문의 종류(Category)는 이 글(Hong 2012)을 참고하세요. 5C + 1M을 바탕으로 논문을 더 읽을지 말지 선택할 것입니다. 더 읽지 않는 결정을 한다면, 대부분은 다음의 이유일 겁니다.\n\n관심이 없는 내용\n해당 논문을 읽기엔 사전 지식이 부족\n저자의 가정이 모호 또는 불명확\n\n만약, 꼭 읽어야만 하는 논문임에도 해당 논문을 읽기에 사전 지식이 부족하다면, 참고문헌(reference)들을 다시 검토해보면서 관심있는 연구 분야의 핵심 연구라고 생각 되는 것을 찾아내 읽어보는 과정을 가져야 할겁니다. 또는, 논문에 쓰인 방법론에 관한 이해가 안되어 있는 상태라면 해당 방법론의 Method paper나 Review paper를 찾아보는 것도 큰 도움이 될 겁니다."
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#nd-read",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#nd-read",
    "title": "관심 논문 읽고 요약하기",
    "section": "2nd read",
    "text": "2nd read\n2nd read에서는 좀 더 세부적인 내용에 집중하라고 합니다. 단, 증명같은 디테일은 무시한채 말이죠. 핵심 사항을 노트에 적거나 테두리에 본인의 의견을 써놓는 것을 권장합니다. 두 번째 읽기는 약 1시간 정도가 소모됩니다. 처음 접하는 분야의 논문이나 개인의 논문 독해 실력에 따라 훨씬 더 많은 시간이 소요될 수도 있습니다. 다음과 같은 사항에 주목하여 읽으세요.\n\nFigure, Diagram, Table 등 논문 내 다양한 도표와 일러스트레이션을 주의깊게 보세요. 특히, Data Science에 관심이 있는 분들이라면 그래프를 잘 봐야합니다. 그래프의 \\(x\\)축, \\(y\\)축, 테이블의 행과 열이 의미하는 바 등을 확인하고 이를 통해 저자가 주장하고자 하는 바가 무엇인지 한마디로 정리할 줄 알아야합니다. 물론, 이 부분은 저자가 확실하게 주장하고자 하는 바를 가지고 시각화, 테이블 작성를 수행했다는 전제 하에 있습니다.\n아직 읽지 않은 연관 논문을 체크하세요. 이 과정은 논문의 배경 지식 또는 특정 방법론에 관한 Method paper인 경우 해당 방법론의 모티베이션을 공부하는데 도움이 됩니다.\n\n두 번째 읽기가 끝난 상태에서 우리가 바라는 희망사항은 다음과 같습니다:\n\n논문의 핵심 내용 이해\n논문의 핵심 주장에 대해 근거와 함께 요약할 수 있어야 함\n\n그래서, 두 번째 읽기를 끝낸 논문은 다음과 같은 섹션으로 상세한 추가 요약을 수행할 예정입니다.\n\nMain Findings: 논문의 핵심 주장과 뒷받침 근거\nMethods: Main Findings에 사용된 핵심 방법론에 관한 내용\nResults: Main Findings외 다른 연구 결과\nLimitations: 연구의 한계점\n\nMain Findings외 다른 연구 결과에 해당하는 Results와 연구의 한계점(Limitations)는 때때로 생략될 수 있습니다.\n두 번째 읽기는 당신이 관심있어 하지만, 당신의 전문 연구 분야는 아닌 논문에 적합하다고 합니다. 하지만, 저는 제 전문 연구 분야도 위와 같은 두 번째 읽기를 통해 추가적으로 세부적인 요약을 수행할 예정입니다. 전문 연구 분야라면 훨씬 더 빠르게 두 번째 읽기를 할 수 있겠죠. 그러나, 여러 이유로 두 번째 읽기에도 이해가 안될 수도 있습니다:\n\n이 주제나 내용이 새로워서 전문 용어나 약어에 익숙하지 않음\n저자가 사용한 방법론이나 연구 결과를 낼 때 사용된 테크닉이 이해가 안됨\n합리적 근거가 부족한 주장 또는 너무 많은 레퍼런스\n피곤해서!\n\n이럴 때 3가지 선택지를 제안합니다.\n\n논문을 치우세요. 그리고, 해당 논문의 내용이 커리어에 무관하기를 바라세요.\n배경 지식을 공부하고 다시 읽으세요.\n노력해보고 세 번째 읽기를 해보세요.\n\n거인의 어깨 위에 올라서서 세상을 바라보라고 하는데, 거인에 어깨 위에 올라서는 것 조차 참 어렵습니다..😭"
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#맺음말",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#맺음말",
    "title": "관심 논문 읽고 요약하기",
    "section": "맺음말",
    "text": "맺음말\n앞으로 제 블로그에 읽은 논문들을 요약하는 글을 작성하기에 앞서, 논문 요약 방식에 대한 설명이 필요할 것 같아서 쓰게 된 글입니다. 논문 요약 방식에 정답은 없습니다. 각자의 논문 요약 방식에 대해 나눠보는 것도 참 흥미로운 대화 거리가 될 것 같네요. 참고한 글(An 2022)에 더 좋은 내용이 많습니다. 그리고, 해당 글의 세 번째 읽기, 문헌 조사 등 “논문 쓰기”에 도움이 될 만한 내용들 또한 기술이 되어있습니다. 다시 한 번 좋은 글 작성해주신 안수빈님께 감사의 말씀을 전합니다. 저도 아직 많이 부족하지만, 이 글이 첫 논문을 접하는 분들께 조금이나마 도움이 됐으면 합니다.🙏"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "",
    "text": "Prerequisite: 논문 요약 방식"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#st-read",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#st-read",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "1st read",
    "text": "1st read\n\nCategory\n\nResearch paper\n\n\n\nMain Topic\n\n제목\n\nCardiac dyspnea risk zones in the South of France identified by geo-pollution trends study - (Simões 기타 2022)\n\n\n\n주제\n\n프랑스 남부 지역의 Cardiac dyspnea(CD, 이하 심호흡곤란) 발생에 미치는 대기오염원(\\(\\rm{PM}_{10}\\), \\(\\rm{NO}_{2}\\), \\(\\rm{O}_{3}\\)) 영향 평가\n\n\n\n\nContext\n\n선행 연구들에서 대기오염원에 관한 단기 노출이 심근경색(myocardial infarction), 울혈성심부전(congestive heart failure)과 같은 몇몇 심혈관 병리(cardiovascular pathologies)들에 미치는 영향을 평가하긴 했으나, 심호흡곤란의 경우 이러한 관계를 아직 완전히 입증하지 못함\n따라서, 본 연구의 목적은 대기오염원, 기상요인, 심호흡곤란 입원 데이터를 활용해 심호흡곤란 입원 발생 원인에 관한 메커니즘을 알아보고, 이를 예방하기 위한 정책을 개발하는 것에 있음\n본 연구의 주요 방법론은 Distributed lag non linear model(이하, DLNM)과 메타분석(Meta analysis)에 해당함\n\n\n\nCorrectness\n\n기상요인(meteorological factors)들을 공변량(coviariates)으로 활용하는데, 다중공선성(multicollinearity)을 피하기 위해 상관이 존재할만한 두 변수 중 하나의 변수만 모형에 포함시킴\n최대 지연 효과(maximum lag days)는 14일까지 고려하였으나, 이에 관한 합리적 근거는 없다고 보여짐\n\n\n\nContributions\n\n프랑스 남부 전체 지역의 심호흡곤란 입원 발생에 관한 대기오염원의 영향을 평가한 첫 번째 연구\n\\(\\rm{NO}_2\\), \\(\\rm{O}_3\\), \\(\\rm{PM}_{10}\\)에 단기 노출이 심호흡곤란으로 인한 응급실 방문을 증가시킨다는 것에 관한 유의한 증거 제시\n본 논문의 접근 방식은 공중 보건 정책에 관한 예측 도구로서 대기오염원 모니터링을 효과적으로 제안함\n\n\n\nClarity\n\n지금까지 읽어본 바로는 명료하게 잘 쓰인 논문이라 생각됨"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#맺음말",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#맺음말",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "맺음말",
    "text": "맺음말\n본 논문을 통해 실제 각 도시별 DLNM을 이용한 대기오염원 건강영향평가 수행 후, 메타분석으로 오버롤한 결과를 제시할 수 있음을 확인했습니다. 메타분석을 어떻게 진행하였는지에 관한 이론적 부분은 자세하게 기술되어 있지 않아서 두 번째 읽기는 진행하지 않았으나, 도시별 분석 결과를 메타분석을 통해 종합할 수 있다는 것을 확인하는 것으로는 첫 번째 읽기로도 충분했습니다.\n본 논문에 쓰인 메타분석은 일반적으로 임상연구에서 수행하는 메타분석을 다양한 상황에 쓸 수 있도록 일반화하여 확장시킨 형태의 메타분석 방법론이라고 보시면 됩니다. 해당 방법론을 깊이있게 이해하기 위해서는 (Sera 기타 2019)을 참고하시면 됩니다. 해당 논문의 예제 R 소스코드는 여기를 참고하시면 됩니다. 다양한 형태의 분석을 수행한 뒤에 library(mixmeta)를 통해 메타분석을 수행하여 결과를 종합하는 과정을 보여준다는 점에서 큰 의미가 있습니다. 그러나, 정작 제가 필요로하는 DLNM으로 건강영향평가를 도시별로 수행한 뒤에 메타분석을 하는 소스코드는 없다는 점이 조금 아쉬웠습니다.😂 그래서, 추가적으로 (Gasparrini, Armstrong, 와/과 Kenward 2012)에서 제공하는 R 예제 소스코드를 함께 참고했습니다. 확장된 형태의 메타분석인 (Sera 기타 2019)가 나오기 전이라 library(mvmeta)를 통해 분석이 진행되긴 합니다만, library(mixmeta)와 똑같은 로직으로 분석이 진행되기 때문에 해당 소스코드를 함께 참고하시면 도시별 DLNM 분석 결과를 메타분석하는 것을 어렵지 않게 구현하실 수 있을겁니다."
  }
]