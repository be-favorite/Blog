[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "안녕하세요 방태모입니다.\nG마켓 AI Product 팀에서 데이터 분석을 하고있습니다."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "포스트",
    "section": "",
    "text": "새 글이 발행되면 알려드려요.\n\n\n\n포스팅을 독려해주실 수 있어요.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n인과추론 입문하기\n\n\n8 min\n\n\n\nCausal Inference\n\n\n\n인과추론의 필요성과 어려움, Potential Outcome Framework를 통해 Causation을 추정하는 원리\n\n\n\nMay 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 4월\n\n\n5 min\n\n\n\nMemory\n\n\n\nPython 관련 잡설, 고객 관심사의 선제적 반영, 뽀모도로 아웃, 지치지않는 방법\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n통계학, 그게 왜 중요한데?\n\n\n27 min\n\n\n\nColumn\n\n\n\n한 마디로 잘 요약된 직접적인 답을 드릴 순 없겠지만, 간접적인 답을 드리고자 합니다.\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 3월\n\n\n5 min\n\n\n\nMemory\n\n\n\n사내 뉴스룸 인터뷰, PAP 퍼블리셔 3기\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시계열 자료분석을 활용한 고객 관심사의 선제적 반영\n\n\n9 min\n\n\n\nTime Series\n\n\n\n특정 시즌에 반복되는 고객들의 주요 관심사를 랭킹 시스템에 선제적으로 반영하기 위해 사용했던 시계열 자료분석 방법론 소개\n\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuasi Experimental Design\n\n\n5 min\n\n\n\nCausal Inference\n\n\n\nPotential Outcome Framework 관점의 인과추론 방법론 중 하나인 준실험설계 소개\n\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 2023년 2월\n\n\n9 min\n\n\n\nMemory\n\n\n\n모바일 홈 개인화, 유저 행태 분석, 밋업, 통계학, 그로스 해킹\n\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 2023년 1월\n\n\n4 min\n\n\n\nMemory\n\n\n\nStreamlit 캐싱, 시계열 자료분석, 글또 8기, 커피챗\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n면접을 앞두고 분석 경험을 회고하는 나만의 방식\n\n\n3 min\n\n\n\nColumn\n\n\n\n부제: 데이터 분석을 대하는 나의 자세\n\n\n\nFeb 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n연간 회고록: 2022년\n\n\n10 min\n\n\n\nMemory\n\n\n\n많은 변화가 있었던 2022년 회고\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022년 이직 로그\n\n\n9 min\n\n\n\nMemory\n\n\n\nData Scientist 이직 로그\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA/B 테스트 용어 사전\n\n\n3 min\n\n\n\nA/B test\n\n\n\nA/B 테스트를 공부할 때 알아두면 좋은 용어들\n\n\n\nOct 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 7월\n\n\n4 min\n\n\n\nMemory\n\n\n\n특별할 것 없었던, 그렇다고 또 심심하진 않았던 7월의 회고\n\n\n\nOct 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n데이터 과학에 관해\n\n\n5 min\n\n\n\nReview\n\n\n\n데이터 과학과 우리나라 데이터 과학 업계에 관한 생각\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 6월\n\n\n8 min\n\n\n\nMemory\n\n\n\n2022 가천대 길병원 고급통계교육 강의 소회\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 5월\n\n\n5 min\n\n\n\nMemory\n\n\n\n실무에서 1년반 동안 연구과제를 수행하며 겪은 크고 작은 난관들\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민\n\n\n7 min\n\n\n\nStatistics Playbook\n\n\nColumn\n\n\n\n취업 준비와 대학원 진학의 기로에서 고민하고 깨달은 것들\n\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 컬러링 가이드\n\n\n13 min\n\n\n\nVisualization\n\n\nR\n\n\n\n더 적은 수의 컬러로 직관적인 시각화를 수행하는 방법\n\n\n\nAug 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 4월\n\n\n5 min\n\n\n\nMemory\n\n\n\n새로운 스터디, 이력서와 포트폴리오 제작기\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n논문 요약 - Simões et al (2022)\n\n\n1 min\n\n\n\nTime Series\n\n\nPaper\n\n\n\n프랑스 남부 지역의 심호흡곤란 입원 발생에 관한 대기오염원 영향 평가 논문 요약\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n관심 논문 읽고 요약하기\n\n\n4 min\n\n\n\nPaper\n\n\n\n논문을 읽고 요약하는 나만의 방식\n\n\n\nMay 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyverse로 데이터베이스랑 대화하기 - 1편\n\n\n5 min\n\n\n\nSQL\n\n\nR\n\n\n\nR을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyvese principle로 머신러닝 하기\n\n\n18 min\n\n\n\nStatistical/Machine Learning\n\n\nR\n\n\n\ntidymodels ecosystem 소개\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyverse principle로 시계열 자료 분석하기\n\n\n22 min\n\n\n\nTime Series\n\n\nR\n\n\n\ntidyverts ecosystem 소개\n\n\n\nMar 11, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "아카이브",
    "section": "",
    "text": "인과추론 입문하기\n\n\n\n\n\n인과추론의 필요성과 어려움, Potential Outcome Framework를 통해 Causation을 추정하는 원리\n\n\n\n\n\n\nMay 29, 2023\n\n\n\n\n\n\n\n\n월간 회고록: 4월\n\n\n\n\n\nPython 관련 잡설, 고객 관심사의 선제적 반영, 뽀모도로 아웃, 지치지않는 방법\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n통계학, 그게 왜 중요한데?\n\n\n\n\n\n한 마디로 잘 요약된 직접적인 답을 드릴 순 없겠지만, 간접적인 답을 드리고자 합니다.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n월간 회고록: 3월\n\n\n\n\n\n사내 뉴스룸 인터뷰, PAP 퍼블리셔 3기\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n시계열 자료분석을 활용한 고객 관심사의 선제적 반영\n\n\n\n\n\n특정 시즌에 반복되는 고객들의 주요 관심사를 랭킹 시스템에 선제적으로 반영하기 위해 사용했던 시계열 자료분석 방법론 소개\n\n\n\n\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\nQuasi Experimental Design\n\n\n\n\n\nPotential Outcome Framework 관점의 인과추론 방법론 중 하나인 준실험설계 소개\n\n\n\n\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n월간 회고록: 2023년 2월\n\n\n\n\n\n모바일 홈 개인화, 유저 행태 분석, 밋업, 통계학, 그로스 해킹\n\n\n\n\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\n월간 회고록: 2023년 1월\n\n\n\n\n\nStreamlit 캐싱, 시계열 자료분석, 글또 8기, 커피챗\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n면접을 앞두고 분석 경험을 회고하는 나만의 방식\n\n\n\n\n\n부제: 데이터 분석을 대하는 나의 자세\n\n\n\n\n\n\nFeb 7, 2023\n\n\n\n\n\n\n\n\n연간 회고록: 2022년\n\n\n\n\n\n많은 변화가 있었던 2022년 회고\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n2022년 이직 로그\n\n\n\n\n\nData Scientist 이직 로그\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\nA/B 테스트 용어 사전\n\n\n\n\n\nA/B 테스트를 공부할 때 알아두면 좋은 용어들\n\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 7월\n\n\n\n\n\n특별할 것 없었던, 그렇다고 또 심심하진 않았던 7월의 회고\n\n\n\n\n\n\nOct 20, 2022\n\n\n\n\n\n\n\n\n데이터 과학에 관해\n\n\n\n\n\n데이터 과학과 우리나라 데이터 과학 업계에 관한 생각\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 6월\n\n\n\n\n\n2022 가천대 길병원 고급통계교육 강의 소회\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 5월\n\n\n\n\n\n실무에서 1년반 동안 연구과제를 수행하며 겪은 크고 작은 난관들\n\n\n\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\n슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민\n\n\n\n\n\n취업 준비와 대학원 진학의 기로에서 고민하고 깨달은 것들\n\n\n\n\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\nggplot2 컬러링 가이드\n\n\n\n\n\n더 적은 수의 컬러로 직관적인 시각화를 수행하는 방법\n\n\n\n\n\n\nAug 14, 2022\n\n\n\n\n\n\n\n\n월간 회고록: 2022년 4월\n\n\n\n\n\n새로운 스터디, 이력서와 포트폴리오 제작기\n\n\n\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n논문 요약 - Simões et al (2022)\n\n\n\n\n\n프랑스 남부 지역의 심호흡곤란 입원 발생에 관한 대기오염원 영향 평가 논문 요약\n\n\n\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n관심 논문 읽고 요약하기\n\n\n\n\n\n논문을 읽고 요약하는 나만의 방식\n\n\n\n\n\n\nMay 13, 2022\n\n\n\n\n\n\n\n\ntidyverse로 데이터베이스랑 대화하기 - 1편\n\n\n\n\n\nR을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\ntidyvese principle로 머신러닝 하기\n\n\n\n\n\ntidymodels ecosystem 소개\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\ntidyverse principle로 시계열 자료 분석하기\n\n\n\n\n\ntidyverts ecosystem 소개\n\n\n\n\n\n\nMar 11, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html#슬기로운통계생활-객원칼럼",
    "href": "archive.html#슬기로운통계생활-객원칼럼",
    "title": "아카이브",
    "section": "슬기로운통계생활 객원칼럼",
    "text": "슬기로운통계생활 객원칼럼\n유튜브 채널 <슬기로운통계생활>에서 운영하고 있는 블로그에서 객원 작가로 칼럼을 작성하고 있습니다.\n Blog"
  },
  {
    "objectID": "archive.html#연구-아카이브",
    "href": "archive.html#연구-아카이브",
    "title": "아카이브",
    "section": "연구 아카이브",
    "text": "연구 아카이브\n논문과 책, 웹사이트 등을 통해 공부하고 연구한 것들을 아카이브하고 있습니다.\n Archive"
  },
  {
    "objectID": "archive.html#발표-아카이브",
    "href": "archive.html#발표-아카이브",
    "title": "아카이브",
    "section": "발표 아카이브",
    "text": "발표 아카이브\n발표 자료를 아카이브하고 있습니다.\n Archive"
  },
  {
    "objectID": "archive.html#스터디-노트",
    "href": "archive.html#스터디-노트",
    "title": "아카이브",
    "section": "스터디 노트",
    "text": "스터디 노트\n스터디 노트를 아카이브하고 있습니다.\n Tensorflow  Coding-Test  SQL"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "구독",
    "section": "",
    "text": "이메일\n    \n  \n  \n    이름 (옵션)\n    \n  \n  \n    성 (옵션)\n    \n  \n  \n    \n  \n  구독하시는 경우, Revue의 약관 및 개인 정보 보호 정책에 동의하게됩니다. \n  \n\n\n그리고, 여기서 저를 서포트해주실 수도 있어요.😀"
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "",
    "text": "Photo by Caspar Camille Rubin on Unsplash\n실무에서는 Data analyst, Data scientist를 가리지 않고 SQL에 관한 능력을 요구합니다. 우리나라의 채용공고를 둘러보면 Data analyst의 경우 특히 SQL 스킬을 중요하게 요구하는 듯 합니다. 방대한 양의 데이터를 저장하고 관리하기 위해 실무에서는 데이터베이스를 사용합니다. 데이터베이스는 종종 관계형 데이터베이스 시스템1(이하 RDBMS)이라 불리기도 하죠. 그리고, 우리는 SQL2 언어 또는 SQL을 조금 변형한(variant) 언어를 통해 이 데이터베이스에 질의(query)를 합니다. 여기서 변형이라는 말을 사용한 이유는, RDBMS를 제공하는 업체에서 표준화된 SQL을 제공하는 경우도 있지만, 표준화된 SQL을 조금 변형시켜 사용하는 경우도 있기 때문입니다.\n만약 이렇게 특정 업체로부터 제공되는 변형된 RDBMS를 사용해야한다면, 해당 업체에서 사용하는 특정 SQL dialect3를 사용해 쿼리를 작성하는 방법을 이해해야 하실겁니다. 변형된 RDBMS를 예로 들어보자면, PostgreSQL, PrestoDB(AWS의 Athena를 위한) 등이 있습니다. PostgreSQL DB의 JSON 필드는 AWS에서 구조화된 중첩 배열로(array) 수집되므로, 동일한 필드를 쿼리하고자 할 때 다른 쿼리문을 사용합니다.\nR을 사용하는 여러분 모두 잘 아시다시피, R에서는 {dplyr}4 패키지를 통해 이러한 작업을 데이터에 수행할 수 있습니다. {dplyr}이 select(), group_by(), left_join() 등 SQL 문법을 잘 모방하긴 했지만, SQL 문법과 R 문법 사이를 완벽하게 왔다갔다 하기는 어렵습니다. 예를 들자면, {dplyr}의 filter()를 이용해 특정 행을 뽑아올 때, 우리는 R 문법을 따라야하므로 조건문에 =이 아닌 ==을 사용하죠. 이는 SQL 문법과는 완벽히 다른 부분입니다.\n자, 여기서 이러한 상황을 타개할 방법은 무엇일까요. 엄청난 용량의 데이터베이스를 R로 가져올 수는 없습니다. 메모리 베이스인 R에 이 짓을 햇다가는요? 생각도 하기 싫습니다.😰 그럼, RDBMS 환경에서 이러한 무거운 작업을(e.g. computation) 수행하고 필요로 될 때에만 R에다가 가져오면 되지 않을까요? 예를 들면, 집계된 데이터를 가져와서 보고서용 그림을 그린다든지. 이를 가능하게끔 해주는 패키지에 대해 배워보려고 합니다.\n본 튜토리얼에서는 {dplyr}의 데이터베이스 백엔드 버전이라 할 수 있는 {dbplyr} 패키지에 대해 배울거에요. {dbplyr}은 당신의 RDBMS에 R의 tidyverse 문법을 사용한 쿼리문을 직접적으로 사용할 수 있게끔 해줄겁니다.😀"
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결하기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결하기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "1 DB 연결하기",
    "text": "1 DB 연결하기\n먼저 필요한 패키지를 불러오죠. install.packages(\"패키지명\")을 통해 설치할 수 있습니다.\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RSQLite)\nlibrary(odbc)\n\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인 패키지입니다.\n{dbplyr}: {dplyr} 문법을 사용하여 데이터베이스에 질의를 할 수 있게끔 해줍니다.\n{dplyr}: 데이터베이스에 질의할 때 사용할 패키지입니다.\n{RSQLite}: 가벼운 단일 유저용 데이터베이스 SQLite DB에 연결할 수 있게끔 해주는 DBI5 호환 패키지입니다. R-SQLite로 이해하시면 편합니다.\n다른 DBI 용 호환 패키지가 필요할 수도 있습니다. 예를 들어, {RPostgres}는 PostgreSQL RDBMS와 연결을 해주는 패키지입니다.6\n{odbc}: odbc 드라이버를 사용해 RDBMS 인터페이스에 인터페이스할 수 있도록 해주는 DBI 호환 인터페이스입니다.7\n\n\n예제용 토이 DB\nAlison Hill이 The Great British Bake off에서 만든 데이터를 사용하려고 합니다. 본 예제에서 다룰 데이터베이스는 여기서 내려받으세요. {bakeoff} 패키지의 데이터를 이용해 연습에 사용할 SQLite DB를 만들었습니다. 이 튜토리얼의 원 저자 Vebash Naidoo님께 감사의 말을 전합니다.\n\n\nSQLite DB 연결하기\n이제 DB를 SQLite DB에 연결해봅시다. DB와 대화를 나누기 위해서, 우선 연결(connection)을 해줘야합니다. 다음의 작업을 해줄겁니다.\n\nDBI 패키지 로딩: library(DBI)\n연결하기: con <- dbConnect(RSQLite::SQLite(), \"내려받은 db 경로\")\n\n\nlibrary(DBI) # main DB interface\nlibrary(dplyr) \nlibrary(dbplyr) # dplyr back-end for DBs\n\ncon <- dbConnect(drv = RSQLite::SQLite(), # give me a SQLite connection\n        dbname = \"data/great_brit_bakeoff.db\")\nsummary(con) # What do we have?\n\n          Length            Class             Mode \n               1 SQLiteConnection               S4 \n\n\n위와 같은 명령어가 출력되면 DB에 성공적으로 연결된 것입니다."
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-둘러보고-다뤄보기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-둘러보고-다뤄보기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "2 DB 둘러보고 다뤄보기",
    "text": "2 DB 둘러보고 다뤄보기\n자, DB 연결도 했으니 이제 몇 가지 DBI 함수를 이용해 연결한 DB를 둘러보고 다뤄봅시다.\n\nDBI 함수\nDBI 함수들의 이름은 꽤 직관적입니다.\n\ndbListTables(con) # 연결된 테이블 리스트를 보여줘!\n\n [1] \"baker_results\"     \"bakers\"            \"bakes\"            \n [4] \"challenge_results\" \"challenges\"        \"episode_results\"  \n [7] \"episodes\"          \"ratings\"           \"ratings_seasons\"  \n[10] \"results\"           \"seasons\"           \"series\"           \n\n\n\ndbListFields(con, # 연결한 DB로 가서\n      \"bakers\")   # bakes 테이블에 어떤 필드가 있는지 알려줘!\n\n[1] \"series\"     \"baker_full\" \"age\"        \"occupation\" \"hometown\"  \n\n\nDB에 질의는 다음과 같이 수행할 수 있어요.\n\nres <- dbSendQuery(con, \"SELECT * FROM bakers LIMIT 3\") # 쿼리문 실행\n# bakers 테이블에 모든 필드를 가져오는데, 관측치 3개까지만 가져와봐!\ndbFetch(res) # 결과 출력해줘\n\n  series          baker_full age                        occupation\n1      1       Annetha Mills  30                           Midwife\n2      1      David Chambers  31                      Entrepreneur\n3      1 Edward \"Edd\" Kimber  24 Debt collector for Yorkshire Bank\n       hometown\n1         Essex\n2 Milton Keynes\n3      Bradford\n\n\n\ndbClearResult(res) # 결과 지우기\n\n\n\ndplyr 함수\n이제, 우리가 잘하는 {dplyr}의 함수들을 이용해 마음껏 DB와 이야기해보죠.\n\ndplyr::tbl(con, \"테이블명\"): 연결한 DB(con)으로 가서 SELECT * FROM 테이블명 실행해줘.\n\n\ntbl(con, \"bakers\")\n\n# Source:   table<bakers> [?? x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker_full                age occupation                       homet…¹\n    <dbl> <chr>                   <dbl> <chr>                            <chr>  \n 1      1 \"Annetha Mills\"            30 Midwife                          Essex  \n 2      1 \"David Chambers\"           31 Entrepreneur                     Milton…\n 3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yorkshire Ba… Bradfo…\n 4      1 \"Jasminder Randhawa\"       45 Assistant Credit Control Manager Birmin…\n 5      1 \"Jonathan Shepherd\"        25 Research Analyst                 St Alb…\n 6      1 \"Lea Harris\"               51 Retired                          Midlot…\n 7      1 \"Louise Brimelow\"          44 Police Officer                   Manche…\n 8      1 \"Mark Whithers\"            48 Bus Driver                       South …\n 9      1 \"Miranda Gore Browne\"      37 Food buyer for Marks & Spencer   Midhur…\n10      1 \"Ruth Clemens\"             31 Retail manager/Housewife         Poynto…\n# … with more rows, and abbreviated variable name ¹​hometown\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\ntbl(con, \"bakers\") %>% \n    head(3) # \"SELECT * FROM bakers LIMIT 3\"와 동일\n\n# Source:   SQL [3 x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n  series baker_full                age occupation                        homet…¹\n   <dbl> <chr>                   <dbl> <chr>                             <chr>  \n1      1 \"Annetha Mills\"            30 Midwife                           Essex  \n2      1 \"David Chambers\"           31 Entrepreneur                      Milton…\n3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yorkshire Bank Bradfo…\n# … with abbreviated variable name ¹​hometown\n\n\n데이터베이스와 대화를 나눌 때 마다 초기에 연결해둔 con을 사용한다는 점을 유념해주세요. 초기에 불러왔던 con은 아까처럼 일반적인 SQL 쿼리문을 이용해 질의를 할 때 뿐만이 아닌 {dplyr}을 통해 타이디한 파이프라인으로 원하는 테이블을 가져올 때도 사용됩니다.\n자 이제 예시 상황을 하나 들어서 {dplyr}로 원하는 테이블을 가져와보겠습니다. baker_results 테이블에는 각 제빵 대회에 참가한 제빵사(baker)의 세부 정보 필드가 담겨있습니다:\n\ndbListFields(con, \"baker_results\")\n\n [1] \"series\"                    \"baker_full\"               \n [3] \"baker\"                     \"age\"                      \n [5] \"occupation\"                \"hometown\"                 \n [7] \"baker_last\"                \"baker_first\"              \n [9] \"star_baker\"                \"technical_winner\"         \n[11] \"technical_top3\"            \"technical_bottom\"         \n[13] \"technical_highest\"         \"technical_lowest\"         \n[15] \"technical_median\"          \"series_winner\"            \n[17] \"series_runner_up\"          \"total_episodes_appeared\"  \n[19] \"first_date_appeared\"       \"last_date_appeared\"       \n[21] \"first_date_us\"             \"last_date_us\"             \n[23] \"percent_episodes_appeared\" \"percent_technical_top3\"   \n\n\n각 제빵대회 우승자의 출신이 영국의 일부 지역에서 나왔는지, 아니면 다양한 지역으로부터 우상자가 배출되었는지 알고싶은 상황이라고 해봅시다. 우선 다음과 같이 관심있는 필드만 불러와주겠습니다.\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner)\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker     hometown              series_winner\n    <dbl> <chr>     <chr>                         <int>\n 1      1 Annetha   Essex                             0\n 2      1 David     Milton Keynes                     0\n 3      1 Edd       Bradford                          1\n 4      1 Jasminder Birmingham                        0\n 5      1 Jonathan  St Albans                         0\n 6      1 Lea       Midlothian, Scotland              0\n 7      1 Louise    Manchester                        0\n 8      1 Mark      South Wales                       0\n 9      1 Miranda   Midhurst, West Sussex             0\n10      1 Ruth      Poynton, Cheshire                 0\n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n24개 열 중 관심있는 4개 열만 불러왔습니다. 이제 제빵대회에 우승한 사람만 골라낸 뒤(filter()) 우승자들이 영국의 어떤 지역으로 부터 왔는지 지역별로 인원을 구하고(count()) 내림차순 정렬(sort())을 해보죠.\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>%\n  count(hometown, sort = TRUE)\n\n# Source:     SQL [8 x 2]\n# Database:   sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n# Ordered by: desc(n)\n  hometown                              n\n  <chr>                             <int>\n1 Wigan                                 1\n2 West Molesey, Surrey                  1\n3 Ongar, Essex                          1\n4 Market Harborough, Leicestershire     1\n5 Leeds / Luton                         1\n6 Bradford                              1\n7 Barton-Upon-Humber, Lincolnshire      1\n8 Barton-Le-Clay, Bedfordshire          1\n\n\n이 결과에 따르면, 제빵대회 우승자들의 출신 지역은 각기 다르다고 결론을 내릴 수 있겠네요.\n\n\ndplyr 문법을 SQL 쿼리문으로\n앞서 {dplyr}을 이용해 수행한 질의를 SQL 쿼리문으로는 어떻게 작성할까요? 코드 한 줄이면 손쉽게 알 수 있습니다.😀\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>% \n  count(hometown, sort = TRUE) %>% \n  show_query()\n\n<SQL>\nSELECT `hometown`, COUNT(*) AS `n`\nFROM (\n  SELECT `series`, `baker`, `hometown`, `series_winner`\n  FROM `baker_results`\n)\nWHERE (`series_winner` = 1.0)\nGROUP BY `hometown`\nORDER BY `n` DESC\n\n\n멋지지 않습니까? 이제 제가 왜 이 글의 맨 위 요약을 “R을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!”라고 적은 지 아시겠나요? {dplyr}로 작업을 수행하고, SQL 쿼리문으로 변환을 수행해보는 작업은 SQL을 배우는 과정에 꽤 큰 도움이 될겁니다. 직장 또는 기관에서 DB를 관리할 때 모두 같은 업체의 SQL DB를 사용하는 건 아니므로, 이렇게 광범위한 업체들로부터 공급되는 SQL을 알고, 읽는 것은 언제나 중요하기 때문입니다.\n\n\n출력문의 lazy query / ??의 의미\n앞서 테이블, 쿼리를 작성하며 출력물에서 Source: table [?? x 5] 또는 Source: lazy query [?? x 4]와 같은 문장을 확인하실 수 있었을 겁니다.\n\n이런 문장이 출력물에 포함되는 이유\n\n먼저, 우리가 직접적인 RDBMS 상에서가 아닌 R이라는 공간을 빌려 뒤에서(behind the scenes) 작성한 dplyr코드는 우리가 연결하려는 DB의 SQL에 해당하는 dialect로 변환됩니다.\n즉, SQL은 DB에 직접적으로 실행됩니다. 즉, 데이터를 먼저 R로 가져와서 조작하는 것이 아닌 쿼리 자체를 DB에 보내고 DB에서 계산(computation)이 수행됩니다.\n정리하면, dplyr 파이프라인을 사용해 DB에서 쿼리를 실행하면, DB에서 계산을 수행하고 실행된 최종 결과의 전체가 아닌 일부를 R에서 보여주는 식입니다.\n이러한 이유들을 들여다보면 우리는 ??를 이해할 수 있습니다.\n??는 “연결 DB con에서 이 쿼리(파이프라인을 SQL로 변환시킨 것)를 실행했고, 여기 R에서 출력물을 스니펫(snippet)으로만 가져왔는데, 얼마나 많은 수의 행이 있는지에 관한 메타 정보까진 캐치하진 못했어. 그저 출력물에 몇 개의 열이 있다는 것 정도만 캐치했어”라고 이해할 수 있습니다.\n이 튜토리얼은 파트 1 입니다. 다음 파트에서 가져온 테이블에 얼마나 많은 행들이 존재하는 지와 같은 메타 정보들을 R로 어떻게 가져오는지에 대해 알아볼 예정입니다."
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결-해제하기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/tidyverse-1.html#db-연결-해제하기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "3 DB 연결 해제하기",
    "text": "3 DB 연결 해제하기\n작업이 끝나면 연결을 해제하는 것을 잊지마세요!\n\ndbDisconnect(con) # db 연결 닫기\n\n연결 해제가 체크는 dbListTable(con)을 실행해보시면 됩니다. 연결해제가 잘 되었다면 에러문이 출력될겁니다.\n\n다음 파트에서 배울 내용\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인 패키지입니다.\n데이터 R로 가져오기\n\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n dbplyr      * 2.2.1   2022-06-27 [1] CRAN (R 4.2.0)\n dplyr       * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n odbc        * 1.3.3   2021-11-30 [1] CRAN (R 4.2.0)\n RSQLite     * 2.2.15  2022-07-17 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "blog.html#블로그를-지원해주시겠어요",
    "href": "blog.html#블로그를-지원해주시겠어요",
    "title": "포스트",
    "section": "블로그를 지원해주시겠어요?",
    "text": "블로그를 지원해주시겠어요?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "연구 아카이브",
    "section": "",
    "text": "논문과 책, 웹사이트 등을 통해 공부하고 연구한 것들을 아카이브합니다.\n참고 문헌과 스터디 노트, 그리고 재현가능한 소스코드를 함께 제공하고자 합니다."
  },
  {
    "objectID": "research.html#time-series",
    "href": "research.html#time-series",
    "title": "연구 아카이브",
    "section": "Time Series",
    "text": "Time Series\n\n1 추론 모델링 · Regression\n\nSpurious regression\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n여러 시계열로 회귀를 수행할 때, 꼭 주의해야 할 알아두어야할 사항\n🔗 스터디 노트\n🔗 R 튜토리얼: CCF 분석의 허구적 상관 확인 과정 참고\n\n\n\nRegression with ARIMA errors\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\nDistributed lag model\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n\n\n\nDistributed lag non-linear model\n\nGasparrini, Antonio, Benedict Armstrong, and M.G. Kenward. “Distributed Lag Non-Linear Models.” Statistics in Medicine 29 (September 20, 2010): 2224–34. https://doi.org/10.1002/sim.3940.\nGasparrini, Antonio. “Distributed Lag Linear and Non-Linear Models in R: The Package Dlnm.” Journal of Statistical Software 43 (July 1, 2011): 1–20. https://doi.org/10.18637/jss.v043.i08.\n🔗 스터디 노트\n🔗 PPT\n🔗 R 튜토리얼\n\n\n\n\n2 예측모델링 · Forecasting\n\nExponential Smoothing\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n🔗 R 튜토리얼: tidyverse principle로 시계열 자료분석하기\n\n\n\nARIMA model\n\n나종화. R 응용 시계열분석. 자유아카데미. 2020.\n🔗 스터디 노트\n\n\n\nProphet\n\nTaylor, Sean, and Benjamin Letham. Forecasting at Scale, 2017. https://doi.org/10.7287/peerj.preprints.3190v2.\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\nHierarchical Time Series Forecasting\n\nAthanasopoulos, George, Roman A. Ahmed, and Rob J. Hyndman. “Hierarchical Forecasts for Australian Domestic Tourism.” International Journal of Forecasting 25, no. 1 (January 1, 2009): 146–66. https://doi.org/10.1016/j.ijforecast.2008.07.004.\nAthanasopoulos, George, Rob Hyndman, Roman Ahmed, and Han Lin Shang. “Optimal Combination Forecasts for Hierarchical.” Computational Statistics & Data Analysis 55 (September 1, 2011): 2579–89. https://doi.org/10.1016/j.csda.2011.03.006.\nHyndman, Rob J, George Athanasopoulos, and Han Lin Shang. “Hts: An R Package for Forecasting Hierarchical or Grouped Time Series,” n.d., 12.\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\n\n3 Other techniques\n\nIntervention analysis (Interrupted Time Series)\n\nSlides. “Intervention Analysis.” Accessed April 17, 2022. https://slides.com/tonyg/intervention-analysis.\n🔗 참고 자료\n🔗 스터디 노트\n🔗 R 코드\n🔗 R 코드: arimax() 튜토리얼\n\n\n\nDynamic Time Warping (DTW)\n\nBerndt, Donald J., and James Clifford. “Using Dynamic Time Warping to Find Patterns in Time Series.” In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, 359–70. AAAIWS’94. Seattle, WA: AAAI Press, 1994.\n선행 또는 후행하는 시계열, 시차가 존재하나 유사한 패턴이 존재하는 두 시계열을 잡아낼 수 있게끔 해주는 비유사성 측도(거리 측도) 알고리즘\nDTW distance를 이용해 계층적 군집 분석 수행 가능\n🔗 스터디 노트\n🔗 R 튜토리얼\n\n\n\nDiscrete Wavelet Transform (DWT)\n\nGraps, Amara. “An Introduction to Wavelets.” IEEE Comp. Sci. Engi. 2 (February 1, 1995): 50–61. https://doi.org/10.1109/99.388960.\nLi, Daoyuan, Tegawendé F. Bissyandé, Jacques Klein, and Y. L. Traon. “Time Series Classification with Discrete Wavelet Transformed Data: Insights from an Empirical Study.” In SEKE, 2016. https://doi.org/10.18293/SEKE2016-067.\n시계열들을 데이터의 열로 나열하여 classification을 수행할 때, 효과적인 차원 감소 방법\n일종의 시계열 Feature engineering 기법에 해당\n🔗 스터디 노트\n🔗 R 튜토리얼"
  },
  {
    "objectID": "research.html#machine-learning-and-statistical-learning",
    "href": "research.html#machine-learning-and-statistical-learning",
    "title": "연구 아카이브",
    "section": "Machine Learning and Statistical Learning",
    "text": "Machine Learning and Statistical Learning\n\nPrerequisite\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n🔗 스터디 노트: Prerequisite 1 머신러닝 용어 정리\n\n\n\nEnsemble methods\n\nChen, Tianqi, and Carlos Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August 13, 2016, 785–94. https://doi.org/10.1145/2939672.2939785.\nChen, Lilly. “Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained.” Medium, January 2, 2019. https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725.\nMorde, Vishal. “XGBoost Algorithm: Long May She Reign!” Medium, April 8, 2019. https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d.\n“Light GBM vs XGBOOST: Which Algorithm Takes the Crown.” Accessed April 17, 2022. https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/.\nRandom Forest, AdaBoost, Gradient Boosting, XGBoost, Light GBM\n🔗 스터디 노트\n🔗 R 튜토리얼: tidyverse principle로 머신러닝하기\n\n\n\nLogistic regression\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference and Prediction. 2nd ed. Springer, 2009. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\nStatQuest with Josh Starmer. Logistic Regression Details Pt 2: Maximum Likelihood, 2018. https://www.youtube.com/watch?v=BfKanl1aSG0.\nChatterjee, Samprit, and Ali S. Hadi. “Regression Analysis by Example, Fifth Edition.”\n🔗 스터디 노트\n\n\n\nGeneralized Linear Model (GLM) and Generalized Additive Model (GAM)\n\nHayes, Genevieve. “Beyond Linear Regression: An Introduction to GLMs.” Medium, December 24, 2019. https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nGLM\n\n🔗 스터디 노트\n\nGAM\n\n🔗 스터디 노트: Prerequisite 1 선형모형의 한계\n🔗 스터디 노트: Prerequisite 2 다항 회귀와 계단 함수\n🔗 스터디 노트: Prerequisite 3 Regression splines\n🔗 스터디 노트: Prerequisite 4 Smoothing splines\n🔗 스터디 노트: Prerequisite 5 Local regressions\n🔗 스터디 노트: GAMs"
  },
  {
    "objectID": "research.html#deep-learning",
    "href": "research.html#deep-learning",
    "title": "연구 아카이브",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nPrerequisites\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n🔗 스터디 노트: Prerequisite 1 딥러닝의 모티베이션과 역사\n🔗 스터디 노트: Prerequisite 2 선형대수의 여러 객체 소개\n🔗 스터디 노트: Prerequisite 3 행렬의 전치와 브로드캐스팅\n🔗 스터디 노트: Prerequisite 4 행렬과 벡터의 곱연산\n🔗 스터디 노트: Prerequisite 5 선형방정식과 선형종속,span\n🔗 스터디 노트: Prerequisite 6 norms\n🔗 스터디 노트: Prerequisite 7 특별한 종류의 행렬과 벡터\n🔗 스터디 노트: Prerequisite 8 고윳값 분해\n🔗 스터디 노트: Prerequisite 9 특잇값 분해와 일반화 역행렬\n🔗 스터디 노트: Prerequisite 10 Trace 연산자와 행렬식\n🔗 스터디 노트: Prerequisite 11 선형대수를 이용한 주성분 유도\n🔗 스터디 노트: Prerequisite 12 머신러닝 용어 정리"
  },
  {
    "objectID": "research.html#high-dimensional-data-analysis",
    "href": "research.html#high-dimensional-data-analysis",
    "title": "연구 아카이브",
    "section": "High-Dimensional Data Analysis",
    "text": "High-Dimensional Data Analysis\n\nBreheny, Patrick. High-Dimensional Data Analysis. The University of Iowa, 2016. https://myweb.uiowa.edu/pbreheny/7600/s16/index.html.\n\n🔗 R 소스코드 및 예제 Dataset 제공\n\n일반적인 기계학습 기반의 예측 모델링으로 접근하기 어려운 n -> p 또는 n < p 인 자료의 예측 모델링에 관한 방법론(여기서 n은 관측치의 수, p는 예측변수의 수)\n꼭 고차원 자료가 아닌, 회귀모형의 예측 성능을 높이기 위해서도 사용되는 방법론들에 해당\n통계적 가설검정 관점에서 가설 검정시 발생하는 고차원 문제에 관한 솔루션 또한 제공함\n\n\n1 고차원 자료에 관한 예측 모델링\n\nPrerequisites\n\n🔗 스터디 노트: Prerequisite 고차원 자료에 대한 고전적인 회귀분석의 문제점\n\n\n\nRidge regression\n\n🔗 스터디 노트\n\n\n\nLasso regression\n\n🔗 스터디 노트\n\n\n\nBias reduction of Lasso estimator\n\n🔗 스터디 노트\n\n\n\nVariance reduction of Lasso eistimator\n\n🔗 스터디 노트\n\n\n\nPenalized logistic regression\n\n🔗 스터디 노트\n\n\n\nPenalized robust regression\n\n🔗 스터디 노트\n\n\n\n\n2 통계적 가설검정 관점의 고차원 문제\n\nPrerequisites\n\n🔗 스터디 노트: Prerequisite 1 통계적 가설검정의 원리\n🔗 스터디 노트: Prerequisite 2 다중 검정\n\n\n\nFamily-Wise Error Rates (FWER)\n\n🔗 스터디 노트\n\n\n\nFalse Discovery Rates (FDR)\n\n🔗 스터디 노트"
  },
  {
    "objectID": "research.html#statistics",
    "href": "research.html#statistics",
    "title": "연구 아카이브",
    "section": "Statistics",
    "text": "Statistics\n\n통계학, 통계적 가설검정과 관련한 것들을 아카이브 합니다.\n\n\n구간추정의 해석에 대한 고전적 관점(Frequentist)과 베이지안 관점\n\n🔗 스터디 노트\n\n\n\n검정력(power)과 검정력 함수에 대해\n\n🔗 스터디 노트\n\n\n\n자유도(Degrees of Freedom)\n\n🔗 스터디 노트\n\n\n\n표준편차와 표준오차\n\n🔗 스터디 노트\n\n\n\n“대립가설이 옳다.”라는 식의 주장을 지양해야하는 이유\n\n🔗 스터디 노트\n\n\n\n중심극한정리의 의미\n\n🔗 스터디 노트\n🔗 스터디 노트: 중심극한정리에 관한 고찰\n\n\n\nFixed effect와 random effect\n\n🔗 스터디 노트"
  },
  {
    "objectID": "research.html#miscellaneous",
    "href": "research.html#miscellaneous",
    "title": "연구 아카이브",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n결정론적 SIR 모형을 이용한 감염병 유행 모델링\n\n🔗 스터디 노트와 R 튜토리얼"
  },
  {
    "objectID": "research.html#statisticalmachine-learning",
    "href": "research.html#statisticalmachine-learning",
    "title": "연구 아카이브",
    "section": "Statistical/Machine Learning",
    "text": "Statistical/Machine Learning\n\nPrerequisite\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n🔗 스터디 노트: Prerequisite 1 머신러닝 용어 정리\n\n\n\nEnsemble methods\n\nChen, Tianqi, and Carlos Guestrin. “XGBoost: A Scalable Tree Boosting System.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August 13, 2016, 785–94. https://doi.org/10.1145/2939672.2939785.\nChen, Lilly. “Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained.” Medium, January 2, 2019. https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725.\nMorde, Vishal. “XGBoost Algorithm: Long May She Reign!” Medium, April 8, 2019. https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d.\n“Light GBM vs XGBOOST: Which Algorithm Takes the Crown.” Accessed April 17, 2022. https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/.\nRandom Forest, AdaBoost, Gradient Boosting, XGBoost, Light GBM\n🔗 스터디 노트\n🔗 R 튜토리얼: tidyverse principle로 머신러닝하기\n\n\n\nLogistic regression\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference and Prediction. 2nd ed. Springer, 2009. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\nStatQuest with Josh Starmer. Logistic Regression Details Pt 2: Maximum Likelihood, 2018. https://www.youtube.com/watch?v=BfKanl1aSG0.\nChatterjee, Samprit, and Ali S. Hadi. “Regression Analysis by Example, Fifth Edition.”\n🔗 스터디 노트\n\n\n\nGeneralized Linear Model (GLM) and Generalized Additive Model (GAM)\n\nHayes, Genevieve. “Beyond Linear Regression: An Introduction to GLMs.” Medium, December 24, 2019. https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “An Introduction to Statistical Learning.” An Introduction to Statistical Learning. Accessed April 17, 2022. https://www.statlearning.com.\nGLM\n\n🔗 스터디 노트\n\nGAM\n\n🔗 스터디 노트: Prerequisite 1 선형모형의 한계\n🔗 스터디 노트: Prerequisite 2 다항 회귀와 계단 함수\n🔗 스터디 노트: Prerequisite 3 Regression splines\n🔗 스터디 노트: Prerequisite 4 Smoothing splines\n🔗 스터디 노트: Prerequisite 5 Local regressions\n🔗 스터디 노트: GAMs"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "발표 아카이브",
    "section": "",
    "text": "발표자료와 강의자료를 아카이브합니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "",
    "text": "Photo by Agê Barros on Unsplash\ntidyverts ecosystem은 시계열 자료에 관한 분석을 tidyverse principle로 수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidyverse priciple이 데이터 전처리에 있어서 얼마나 많은 업무 생산성을 가져다 주는지 우리는 이미 알고있습니다. 시계열 자료를 자주 다루는 사람이라면 꼭 배워둘 만한 패키지죠.😄 tidyverts ecosystem을 이루는 대부분의 패키지들은 {fpp3}으로 불러올 수 있습니다. {tsibbletalk}은 {shiny}와 함께 동작하는 반응형 그래픽을 제공하는 패키지로 본 튜토리얼에서는 생략하겠습니다:\n위 패키지들이 설치되어 있지 않은 분들은 튜토리얼의 본격적인 시작전에, install.packages(\"패키지명\")을 통해 설치해주시기 바랍니다. 개발 버전을 설치하고 싶으신 분이 있다면 다음의 코드를 이용하세요:"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibble",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibble",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "1 tsibble",
    "text": "1 tsibble\n\n1.1 Get Started\n{tsibble}은 일반적인 시계열 자료를 tibble 형태로 표현할 수 있게해줍니다. 우리는 tsibble()을 통해 tidy한 자료에 대해 수행해왔던 {tidyverse}를 이용한 wrangling을 수행할 수 있습니다. 즉, tidyverse ecosystem이 tibble 객체를 기반으로 동작하듯이, tidyverts ecosytem은 tsibble 객체를 기반으로 동작합니다. tsibble 객체가 갖는 기본적인 원칙은 다음과 같습니다:\n\nindex: 과거부터 현재까지 순서화된 자료값의 관측 시간\nkey: 시간에 따른 관측 단위를 정의하는 변수의 집합\n각 관측치는 index와 key를 통해 유일하게(uniquely) 식별되어야만 함\n각 관측치는 등간격으로 관측된 자료여야만 함\n\n즉, 티블(데이터프레임)을 tsibble로 변환하기(coerce) 위해서는 key와 index를 명시해주어야 합니다. 예를 들어, 다음과 같은 {nycflights13} 패키지의 weather 자료를 이용해보겠습니다:\n\nweather_simple <- nycflights13::weather %>% \n    select(origin, time_hour, temp, humid, precip)\nweather_simple\n\n\n\n\n\n  \n\n\n\norigin을 key로 index를 time_hour로 해주면 될 것 같습니다:\n\nweather_tsbl <- as_tsibble(weather_simple, key = origin, index = time_hour)\nweather_tsbl\n\n\n\n\n\n  \n\n\n\n여기서는 자료 자체가 출발지(origin) 별로 기록된 다중(multiple) 시계열에 해당하므로, key를 origin으로 잡아줬지만, 만약 자료가 단일(univariate) 시계열에 해당한다면 해당 key는 설정을 하지 않으면 됩니다(see package?tsibble and vignette(\"intro-tsibble\") for details). 그리고, 사실 tsibble()은 irregular time interval을 갖는 자료에 대해서도 적용이 가능합니다. as_tsibble은 regular = TRUE 옵션이 default로 설정되는데, 이를 FALSE로 바꿔주면 되며, 이러한 irregular time interval을 갖는 tsibble 객체의 경우는 [!] 표시를 통해 확인할 수 있습니다:\n\nnycflights13::flights %>%\n    mutate(\n      sched_dep_datetime = make_datetime(year, month, day, hour, minute, \n                                         tz = \"America/New_York\")) %>%\n    as_tsibble(\n        key = c(carrier, flight), \n        index = sched_dep_datetime, \n        regular = FALSE\n        )\n\n\n\n\n\n  \n\n\n\n\n\n1.2 Turn impicit missing values into explicit missing values\n간혹 시계열 자료에는 암묵적 결측치(implicit missing values)가 존재하는 경우가 있습니다. 암묵적 결측치가 존재하는 시계열 자료가 일정한 시간 간격으로 수집되었을 경우, 우리는 fill_gaps()를 이용해 암묵적 결측을 명시적으로(explicit) 바꿀 수 있어요. 4년간 수집된 연도별 키위, 체리의 수확량(단위: kg)에 관한 자료를 직접 만들어서 fill_gaps()의 쓰임에 대해 알아봅시다. 본 자료에는 암묵적 결측이 존재합니다:\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest\n\n\n\n\n\n  \n\n\n\n암묵적 결측이란, 예를 들어 위 자료처럼 체리 생산량이 2010년에는 기록되지 않았음에도 불구하고 행이 생략되어있는 것을 말합니다. NA로 명시는 다음과 같이 손쉽게 가능합니다:\n\nfill_gaps(harvest, .full = TRUE)\n\n\n\n\n\n  \n\n\n\n다음의 각각 시작점, 끝점에 대해서만 결측치를 명시할 수도 있습니다:\n\n# at the same starting point across units\nfill_gaps(harvest, .full = start())\n# at the same end point across units\nfill_gaps(harvest, .full = end())\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n.full = FALSE를 설정할 경우(fill_gaps()의 default 옵션에 해당), 각 key 내의 period에서 발생한 결측에 대해서만 명시가 이루어집니다.\n\nfill_gaps(harvest, .full = FALSE)\n\n\n\n\n\n  \n\n\n\n특정값으로의 명시도 손쉽게 수행이 가능해요.\n\nharvest %>% \n    fill_gaps(kilo = 0L)\n\n\n\n\n\n  \n\n\n\n변수에 대해 함수를 적용하여 명시도 가능합니다. sum()을 이용하여 합으로 명시해보았습니다:\n\nharvest %>%\n    fill_gaps(kilo = sum(kilo))\n\n\n\n\n\n  \n\n\n\nkey에 대해 group_by를 통해 각 그룹에 대해 함수를 적용할 수도 있죠. 이번에는 median()을 통해 중위수로 명시해보았습니다:\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo))\n\n\n\n\n\n  \n\n\n\n원 자료 자체에 NA가 존재하는 경우, 적용하고자 하는 함수에 na.rm = TRUE을 설정해주면 됩니다:\n\nharvest[2, 3] <- NA\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo, na.rm = TRUE))\n\n\n\n\n\n  \n\n\n\n마지막으로, fill_gaps()아 tidyr::fill()을 함께 이용하면 암묵적 결측치를 이전 시점의 결측치로 대치할 수 있습니다.\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"down\")\n\n\n\n\n\n  \n\n\n\n반대로, 한 시점 미래의 값으로 대치도 가능합니다.\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"up\")\n\n\n\n\n\n  \n\n\n\n\n\n1.3 Aggregate over calendar periods\nindex_by()와 summarise()를 이용하면 관심있는 변수에 대해 특정 시간 주기(e.g. monthly)에 대해 함수(e.g. 합계: sum(), 평균: mean())를 적용할 수 있어요. index_by는 as.Date(), tsibble::yearweek(), tsibble::yearmonth(), tsibble::yearquarter(), 뿐만 아니라 {lubridate} 계열의 함수와 함께 사용됩니다. 예를 들어, weather 자료의 월별 평균 기온, 총 강수량은 다음과 같이 yearmonth()에 index 변수를 .으로 나타내어 계산할 수 있습니다.\n\nweather_tsbl %>% \n    group_by_key() %>% \n    index_by(year_month = ~yearmonth(.)) %>%\n    summarise(\n        avg_temp = mean(temp, na.rm = TRUE),\n        total_precip = sum(precip, na.rm = TRUE)\n    )\n\n\n\n\n\n  \n\n\n\nindex_by()+summarise()는 irregular time interval을 갖는 tsibble에 대해서도 수행이 가능합니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibbledata",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#tsibbledata",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "2 tsibbledata",
    "text": "2 tsibbledata\n{tsibbledata}는 tsibble 형태의 다양한 예제 자료를 제공해줍니다. 어떤 패키지에 대한 튜토리얼을 진행할 때, 적절한 자료들이 필요로 되는데, 이렇게 예제 자료를 직접적으로 제공해준다는 점에서 R 유저들에 대한 배려가 담겨있다는 생각이 드네요. 예를 들어, 다음의 olympic_running은 4년 주기로 수집된 올림픽 달리기 종목의 성별 최고기록에 관한 자료입니다(see ?olympic_running for details).\n\nolympic_running\n\n\n\n\n\n  \n\n\n\n이 자료를 이용하여 달리기 종목별 최고 기록에 대한 시도표를 성별로 나누어서 그려보았습니다. 참고로, 1916, 1940, 1944년의 경우 세계대전으로 인해 결측 처리되었습니다.\n\nggplot(olympic_running, aes(x = Year, y = Time, colour = Sex)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(~ Length, scales = \"free_y\", nrow = 2) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"Dark2\") + \n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  ylab(\"Running time (seconds)\")"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#feasts",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#feasts",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "3 feasts",
    "text": "3 feasts\n{feasts}는 Feature Extraction And Statistics for Time Series의 약자로, 시계열 자료분석에 쓰이는 여러가지 툴을 제공해줍니다. tsibble 객체와 함께 동작하며, 시계열의 분해, feature 추출(e.g. 추세, 계절성), 시각화 등을 수행할 때 쓰입니다. 아울러, {feasts}를 통한 시계열 자료분석은 다음 섹션에서 소개할 tidyverts ecosystem의 예측 모델링 부분을 담당하는 {fable} 패키지와 긴밀하게 결합하여 사용됩니다.\n\n3.1 Graphics\n시각화는 주로 시계열 자료의 패턴을 이해하기 위한 첫 단계에 많이 이루어집니다. {feasts}는 시계열의 패턴을 {ggplot2}를 사용해 자유롭게 커스텀할 수 있는 그래픽을 제공합니다. 첫 번째로는 gg_season을 이용한 계절성(seasonality) 시각화입니다. 시각화에 사용된 자료 tsibbledata::aus_production은 호주의 맥주, 담배 등의 품목에 관한 분기별 생산지표 추정치에 관한 자료입니다. 맥주의 분기별 생산지표에 관한 계절성 시각화를 수행해보았습니다:\n\naus_production %>% \n  gg_season(Beer)\n\n\n\n\n\n\n\n\n다음으로 gg_subseries()를 이용하면 시계열의 각 season별로 시각화가 가능합니다. 예를 들어, aus_production과 같은 분기별 자료의 경우 분기별 패턴에 대한 시각화를 쉽게 수행할 수 있습니다:\n\naus_production %>% \n  gg_subseries(Beer)\n\n\n\n\n\n\n\n\ngg_lag()를 이용하면 원자료와 시차(lag)의 산점도를 season별로 나누어 그릴 수 있습니다:\n\naus_production %>% \n  filter(year(Quarter) > 1991) %>% \n  gg_lag(Beer, geom = \"point\")\n\n\n\n\n\n\n\n\n분기별 자료의 특성상, lag 4와 8 그림을 보면 각 season별로 원자료와의 관계가 \\(y=x\\) 직선에 잘 놓여있는 것을 캐치할 수 있죠. 마지막으로 ACF 그림도 손쉽게 그릴 수 있습니다:\n\naus_production %>% \n  ACF(Beer) %>% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n3.2 Decompositions\n시계열 분해(decomposition)는 시계열 자료분석에서 흔히 수행되는 작업 중 하나이며, 이는 시계열에 대한 패턴을 이해하는데에 큰 도움을 줍니다. 그리고, 추후 예측 모델링을 정교하게 하는 것에도 상당한 도움을 준다. 즉, 시계열 분해는 본인이 분석하고자 하는 시계열의 패턴을 좀 더 정교하게 캐치하고 예측 성능을 향상시키기 위한 목적으로 꼭 필요로 되는 사전 작업이라고 할 수 있습니다. 본 튜토리얼에서는 {feasts}에서 제공하고 있는 2가지 시계열 분해 방법에 대해 소개하려고 합니다.\n\n3.2.1 Classical decompostion\nclassical decompostion은 1920년대에 고안된 방법입니다. 오래된 방법론인 만큼 요즘 쓰이는 시계열 분해 방법들의 초석이 되는 방법이라고 할 수 있으며, 다른 방법들에 비해 상대적으로 간단하다는 장점이 있습니다. classical decompostion은 가법 분해와 승법 분해가 있습니다. 두 방법은 계절성의 반영 방식에 따라 나뉩니다(e.g. 분기별 자료 \\(m = 4\\), 월별 자료 \\(m = 12\\), 일별 자료 \\(m = 7\\)). 보통 가법 classical decompostion의 경우 계절성이 추세에 따라 무관하게 일정한 크기를 유지할 때 사용하며, 반대로 계절성의 크기가 추세의 크기에 따라 변화하는 경우에는 승법 classical decompostion을 사용합니다. 승법 계절성 classical decompostion는 계절 성분이 연도에 따라 상수라고 가정한채로 진행되며, 승법 계절성에서 계절 성분을 형성하는 \\(m\\)은 계절 지수(seasonal indices)라 불리기도 합니다.\nclassical decompostion의 자세한 분해 과정은 여기를 참고해주시기 바랍니다. 여기서는 바로 R을 이용한 튜토리얼을 진행하겠습니다. 앞서 사용했던 자료의 맥주 생산지표를 가법 classical decomposition을 통해 분해해보겠습니다.\n\ndcmp <- aus_production %>%\n    model(classical_decomposition(Beer, type = \"additive\"))\ncomponents(dcmp)\n\n\n\n\n\n  \n\n\n\n먼저, 분해된 시계열의 요소들은 componenets()로 불러올 수 있습니다. 그리고, 이 components()에 대해 autoplot()을 수행해주면 다음과 같이 시각화를 수행할 수 있습니다:\n\ndcmp %>%\n    components() %>% \n    autoplot() +\n    labs(title = \"Classical additive decomposition of Quarterly production of beer in Australia\")\n\n\n\n\n\n\n\n\n\n\n3.2.2 STL decomposition\nSTL은 “Seasonal and Trend decomposition using Loess”의 준말로 다재다능(versatile)하고 로버스트한 시계열 분해 방법에 해당합니다. 그리고, 여기서 loess란 Local regression의 준말로 자료를 비선형으로 추정하는 방법 중 하나에 해당합니다. STL은 앞서 소개한 classical decomposition, 그리고 {feasts}에서 제공하는 또 다른 시계열 분해 방법 SEATS, X-11과 비교하여 몇몇 이점을 갖는다. 자세한 사항은 여기를 참고해주세요. 본 글은 tidyverts ecosystem에 대한 소개 이므로, deep한 이론 정리는 추후에 fpp3 책을 공부하면서 하나하나 정리해나가겠습니다. 일단 바로 실습으로 넘어가겠습니다.😊 다음은 STL decomposition을 이용하여 시계열의 추세 요소는 window = 7을 통해 좀 더 flexible하게 추정하고, 계절 패턴의 경우는 window = \"periodic\"으로 하여 고정(fixed)되도록 하였습니다(see ?STL for details). 여기서. window란, 창을 말하며 자료를 여러 창으로 잘게 쪼갤수록 더 flexible하고 복잡한 함수를 추정하게 됩니다. splines에 지식이 있는 분들은 이해하기 쉬울거라고 생각합니다.\n\naus_production %>%\n  model(\n    STL(Beer ~ trend(window = 7) + season(window = \"periodic\"),\n        robust = TRUE)) %>%\n  components() %>%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Feature extraction and statistics\n{feast}에서 소개할 마지막 기능은 시계열의 feature(e.g. ACF)와 통계량(e.g. 평균)을 뽑아내는 것입니다. {feast}에서는 feature() 함수를 통해 많은 종류의 features들에 대한 정보를 제공합니다만, 본 튜토리얼에서는 시계열의 평균, 분위수, ACF를 뽑아내는 방법에 대해서만 소개하겠습니다(see ?feature for details). 그 외 다른 features들에 관심이 있으시다면, 여기를 참고해주세요.\n\n3.3.1 Some simple statistics\n먼저, 시계열의 평균과 분위수를 뽑는 방법에 대해 소개하겠습니다. 평균, 분위수 등 시계열의 기본적인 통계량은 feature()와 R의 기본 함수(e.g. mean(), median())들을 이용해 간편하게 계산할 수 있습니다. 여기서 이용할 자료 tourism()은 지역, 주, 목적별로 나눠진 1998-2016년 분기별 호주 여행객수에 관한 자료로, 지역, 주, 여행 목적별 여행객 수의 전체 평균과 분위수를 계산해봤습니다:\n\ntourism %>%\n    features(Trips, \n             list(mean = mean, quantile))\n\n\n\n\n\n  \n\n\n\n\n\n3.3.2 ACF features\nACF에 관한 정보는 feat_acf()를 이용하면 됩니다. feat_acf()는 기본적으로 ACF와 관련한 6가지 또는 최대 7가지의 features를 제공해줍니다(see ?feat_acf() for details):\n\n원 계열의 1차 자기상관계수\n원 계열의 1차-10차 자기상관계수의 제곱합\n1차 차분 계열의 1차 자기상관계수\n1차 차분 계열의 1차-10차 자기상관계수의 제곱합\n2차 차분 계열의 1차 자기상관계수\n2차 차분 계열의 1차-10차 자기상관계수의 제곱합\n(계절 시계열에 대해) 첫번째 계절 시차(seasonal lag)에서의 자기상관계수\n\n\ntourism %>% \n  features(Trips, feat_acf)\n\n\n\n\n\n  \n\n\n\n맨 마지막 열이 첫번째 계절 시차에서의 자기상관계수를 나타내는데, 본 자료의 경우 분기별 자료에 해당하므로 계절 주기는 4에 해당합니다. 즉, 본 자료에서 첫번째 계절 시차에서의 자기상관계수는 원 계열의 시차 4에서의 ACF 값을 나타낸다고 할 수 있습니다.\n\ntourism %>% \n  features(Trips, feat_acf) %>% \n  select(Region:Purpose, season_acf1)\n\n\n\n\n\n  \n\n\n\n원자료에 대한 ACF를 구해보면 다음과 같이 시차 4에서의 자기상관계수와 동일한 값을 가짐을 알 수 있죠:\n\ntourism %>% \n    ACF(Trips)\n\n\n\n\n\n  \n\n\n\n본 튜토리얼에서는 소개하지 않았지만, feature()를 이용한 시계열 feature extraction과 연계하여 다양한 시각화도 수행할 수 있습니다. 꼭 참고해보시기 바랍니다: https://otexts.com/fpp3/stlfeatures.html"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "4 fable",
    "text": "4 fable\n{fable} 패키지는 tsibble 객체와 함께 tidy한 format으로 시계열 예측 모델링을 수행할 수 있게해줍니다. {tidymodels} 패키지에 대한 이해가 있으신 분들이라면 어렵지 않으실거라 생각합니다. {tidymodels}과 마찬가지로 {fable}은 여러 시계열에 대해 여러 시계열 모형에 대한 추정, 비교, 결합, 예측 등을 가능하게해줍니다.\n본격적인 튜토리얼 시작에 앞서, tourism() 자료를 이용할 것이며, 4가지 여행 목적(“business”, “holiday”, “visiting friends and relatives”, “other reasons”)으로 분해할 수 있는 호주 멜버른(Melbourne)의 일별 여행객 수를 예측하는 것에 관심이 있다고 가정합니다. 각 계열의 첫 번째 관측값은 다음과 같습니다:\n\ntourism_melb <- tourism %>% \n  filter(Region == \"Melbourne\")\ntourism_melb %>% \n    group_by(Purpose) %>% \n    slice(1)\n\n\n\n\n\n  \n\n\n\n우리가 추정하고자 하는 변수는 Trips(일별 여행객 수, 단위: 천)입니다. 해당 계열들의 시도표를 보면, 추세와 약한 계절성이 명확하게 존재함을 알 수 있습니다.\n\ntourism_melb %>% \n  autoplot(Trips)\n\n\n\n\n\n\n\n\n{fable} 패키지에서 폭넓게 쓰이는 시계열 예측 모형은 ETS와 ARIMA 모형입니다. 먼저, ETS 모형은 추세 요소와 계절 요소를 가법, 승법, 감쇠효과 등을 반영하여 시계열을 모델링하는 지수평활법(exponential smoothing)을 통계적 모형으로 확장시킨 것에 해당합니다. 통계적 모형으로의 확장은 오차항 \\(\\epsilon_t\\)에 대해 통계적 분포라 할 수 있는, 평균이 0이고 분산이 \\(\\sigma^2\\)인 가우스 백색잡음 과정(gaussian white noise process)을 가정함으로써 이루어집니다. 즉, ETS 모형의 알파벳 각각은 E(error, 오차), T(trend, 추세), S(seasonal, 계절성)을 나타내며, 각 요소들을 모델링하는 방식(가법, 승법, 가법감쇠(damped), 승법감쇠)에 따라 ETS 모형의 종류가 나뉘어집니다. 아울러, 각 모델은 관측된 자료를 설명하는 측정식(measurement equations)과 시간에 따라 변화하는 관측되지 않은 요소(level, trend, seasonal)들을 설명하는 상태식(state equations)으로 구성되는데, 이러한 이유에서 우리는 ETS 모형을 혁신상태공간모형을 이루는 지수평활법(innovations state space models for exponential smoothing)이라고 표현하기도 합니다(See here for detail). 두 번째로, ARIMA 모형은 시계열의 현재값을 과거값과 과거 예측 오차로 설명하는 대표적인 통계적 시계열 예측모형으로, 자세한 설명은 생략하겠습니다. ARIMA 모형에 대한 개념이 없으신 분들은 여기를 참고해주시기 바랍니다.\n두 모형에 대한 간략한 개념 설명은 이쯤에서 마치기로 하고, 이제 이 모형들을 {fable} 패키지를 이용해 어떻게 적합을 수행하면 되는지 보겠습니다. {fable}을 이용한 모형 적합은 model()을 통해 이루어집니다. model()을 통한 적합 과정은 {tidymodels}와 유사하게 상당히 직관적인 이름의 함수들로 이루어집니다. 먼저, ETS()의 경우는 R에서 일반적으로 사용하는 모형식의 specification를 따라서 각 요소를 반영할 수 있게 해주며, 본 예제에서는 추세 요소만 가법적으로 설정해주고 나머지 요소는 자동으로 선택되도록 하였습니다(AICC를 기준으로, see ?ETS for details). 그리고, ARIMA 모형은 ARIMA() 함수로 적합할 수 있으며, 해당 함수는 {forecast} 패키지의 auto.arima와 유사하게 default 옵션으로 AICC 값을 기준으로 최적의 모형을 선택해 줍니다(see ?ARIMA). model()을 통해 적합이 이루어진 모형 객체는 tidy한 포맥의 모형 테이블로 결과를 반환해줍니다. 이를 이제부터 mable(model table) 객체라 칭하겠습니다:\n\nfit <- tourism_melb %>% \n  model(\n    ets = ETS(Trips ~ trend(\"A\")),\n    arima = ARIMA(Trips)\n  )\nfit\n\n\n\n\n\n  \n\n\n\nmable 객체의 행은 각 시계열로 이루어져있으며, 열은 각 모형의 specification을 나타냅니다. fit이 반환하는 결과를 보면 알 수 있듯이, 적합된 ETS 모형의 추세 요소는 모두 가법적으로 고려되었으며, 나머지 요소들은 각 시계열에 따라서 최적의 성분이 자동으로 선택되었습니다. ARIMA 모형 또한 AICC 값을 기준으로 한 최적의 차수들이 반영되어 모형 적합이 잘 이루어진 것으로 보입니다. 이 mable 객체로 우리는 모델 적합 단계에서 필요한 모든 작업을 tidy한 포맷으로 수행할 수 있습니다.\n먼저, coef() 또는 tidy()를 통해 모형으로부터 추정된 계수들을 추출할 수 있습니다. 아울러, 사전에 select() 함수를 통해 특정 모형에 대한 계수 값만을 뽑을 수도 있습니다:\n\nfit %>%\n  select(Region, State, Purpose, arima) %>%\n  coef()\n\n\n\n\n\n  \n\n\n\ntidy로 수행해도 결과는 같습니다. 다음으로 glance()를 이용하면 모형의 적합 결과를 정보 기준(e.g. AIC, BIC)과 잔차의 분산 등으로 요약해줍니다.\n\nfit %>% \n    glance()\n\n\n\n\n\n  \n\n\n\n만약 하나의 모형으로만 시계열 예측 모델링을 수행하고 있다면, report() 함수를 이용하면 됩니다. 이는 하나의 시계열 예측 모형의 평가를 상당히 만족스러운 포맷으로 제공해줍니다.😊 여행 목적이 “Holiday”일 때 ETS 모형을 적합한 결과 대한 요약을 report()를 통해 진행해봤습니다:\n\nfit %>%\n    filter(Purpose == \"Holiday\") %>%\n    select(ets) %>%\n    report()\n\nSeries: Trips \nModel: ETS(M,A,A) \n  Smoothing parameters:\n    alpha = 0.03084501 \n    beta  = 0.03084499 \n    gamma = 0.0001000967 \n\n  Initial states:\n     l[0]      b[0]     s[0]    s[-1]     s[-2]    s[-3]\n 424.0777 -2.535481 -26.7441 4.256618 -10.10668 32.59417\n\n  sigma^2:  0.011\n\n      AIC      AICc       BIC \n 991.7305  994.3020 1013.1688 \n\n\n아울러, 모형으로부터의 적합값과 잔차는 fitted(), residuals() 각각을 이용해 얻을 수 있습니다:\n\nfit %>%\n    fitted()\nfit %>%\n    residuals()\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n적합값과 잔차를 함께 얻고 싶다면 augment()를 사용하세요:\n\nfit %>% \n    augment()\n\n\n\n\n\n  \n\n\n\n모형간 예측 정확도의 비교는 accuracy()를 이용하면 됩니다. 여러 예측 평가 측도를 제공해줍니다:\n\nfit %>% \n    accuracy() %>% \n    arrange(MASE)\n\n\n\n\n\n  \n\n\n\n참고로, 여기서는 훈련 자료(training data)에 대한 예측 성능에 해당합니다. 본 호주 일별 여행객수에 대한 자료에서는 예측 성능 평가 측도를 MASE로 할 경우, ETS 모형이 여행 목적이 “Other”인 경우를 제외하고는 훨씬 더 좋은 성능을 보이고 있습니다. 향후 시점의 예측은 forecast()로 추가적인 자료에 대한 정보 없이 바로 수행을 할 수 있습니다:\n\nfc <- fit %>% \n    forecast(h = \"5 years\")\nfc\n\n\n\n\n\n  \n\n\n\n향후 시점의 예측 결과는 fable(forecast table)로 요약되며, fable은 예측값의 점 추정치와 예측값의 분포에 대한 정보까지 포함하여 제공해줍니다. 예를 들어, 첫 번째 행의 시계열의 예측값의 분포는 평균이 619, 분산이 3533인 정규분포에 해당합니다. 정규분포를 따르는 이유는, 앞서 ETS의 간략한 소개에서 설명했듯이 오차항에 대해 가우스 백색잡음 과정을 가정했기 때문입니다. 그렇다면, 이러한 예측값의 분포에 따른 구간 추정은 어떤 함수로 수행할 수 있을까요? 예측값의 신뢰구간은 hilo()를 이용하면 됩니다. hilo() 함수는 fable 객체와 함께 동작하며, 원하는 신뢰수준을 반영할 수 있게 해줍니다. 다음은 80%, 95% 각각의 신뢰수준에 대한 구간을 추정한 것입니다:\n\nfc %>%\n    hilo(level = c(80, 95))\n\n\n\n\n\n  \n\n\n\n마지막으로, 예측값에 대한 시각화는 fable 객체에 대해 autoplot()을 적용해주면 됩니다:\n\nfc %>% \n  autoplot(tourism_melb)\n\n\n\n\n\n\n\n\n본 튜토리얼에서 소개한 함수들 외에도 {fable}의 특정 모형 객체들과 함께 동작하는 여러 함수들이 있습니다(e.g. refit(), interpolate(), components(), etc). 튜토리얼에서 소개한 내용외에 자세한 내용이 궁금하시다면 Forecasting: Principles and Practices (3rd Ed.)를 참고해주세요."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable.prophet",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#fable.prophet",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "5 fable.prophet",
    "text": "5 fable.prophet\n{fable.prophet}은 facebook에서 제안한 단일 시계열 예측모형에 대한 적합 또한 tidy한 인터페이스로 제공해줍니다. prophet은 시계열의 시간 종속적인 특성을 고려하는 기존의 시계열 모형(e.g. 지수평활법, ARIMA 모형)과 달리 curve-fitting(e.g. splines)으로 모형을 적합하며, 시계열을 다음과 같이 세 가지 요소로 분해하고 각 요소를 시간의 함수로 가법적으로 모형화합니다.\n\\[\ny(t) = g(t) + s(t) + h(t) + \\epsilon_t\n\\]\n여기서 \\(g(t)\\)는 비주기적 변화를 모형화하는 추세 함수, \\(s(t)\\)는 주별 또는 연별 계절성과 같은 주기적 변화를 반영하며, \\(h(t)\\)는 불규칙하게 발생할 가능성이 있는 휴일효과(holidays and events effects)를 모형화합니다. 세 요소 중에서도 휴일효과에 대한 반영이 prophet의 상당히 특징적인 부분이라 할 수 있겠으며, 모형에서 조절할 수 있는 모수들이 상당히 많아서 아주 유연하고 디테일하게 모델링이 가능합니다. 도메인 지식이 풍부할수록 prophet을 통한 성능 개선의 가능성은 무궁무진합니다. 본 튜토리얼에서 prophet에 대한 개념 설명은 이쯤에서 간략하게 마치겠습니다. prophet을 이번에 처음 접하시는 분들은 여기를 참고해주시기 바랍니다. 개념 정리와 R을 이용한 튜토리얼 과정을 정리해놓았는데, 여기서 소개할 tidy한 인터페이스의 이해를 위해서 꼭 필요로 될겁니다.\n본 튜토리얼에서 prophet을 이용한 예측 모델링에 이용할 자료는 호주의 카페, 레스토랑 및 케이터링 서비스에 관한 월 매출액 자료(단위: milions $AUD)입니다:\n\ncafe <- tsibbledata::aus_retail %>%\n    filter(Industry == \"Cafes, restaurants and catering services\")\nautoplot(cafe)\n\nPlot variable not specified, automatically selected `.vars = Turnover`\n\n\n\n\n\n\n\n\n\n주별로 나뉜 해당 자료의 각 계열은 증가하는 추세와 그에 따른 연별 계절 패턴이 눈에 보입니다. 또한, 계절 패턴의 경우 계열의 수준(level)에 비례하는 형태를 보이고 있으므로, 계절성을 승법적으로 고려해야할 것입니다. 아울러, 월별 자료의 경우는 휴일 효과의 경우 계절 요소를 통해 모형화가 가능합니다. 휴일효과에 대한 반영은 이번에 진행하지 않을 예정입니다(기존의 prophet 인터페이스에서 수행했던 것과 같이 간단하게 반영, see here for details). 본 자료에 대해 추세 요소는 선형으로 하여(default), 연별 계절성을 승법으로 고려하여 prophet을 적합해보았습니다:\n\nfit <- cafe %>%\n  model(\n    prophet = prophet(Turnover ~ season(\"year\", 4, type = \"multiplicative\"))\n  )\nfit\n\n\n\n\n\n  \n\n\n\n각 계열에 대해 prophet이 잘 적합된 것을 확인할 수 있습니다. 적합된 모형의 각 요소들은 components()로 추출할수 있습니다:\n\ncomponents(fit)\n\n\n\n\n\n  \n\n\n\ncomponents()를 통해 주어지는 객체 자체에 autoplot()을 수행하면 모든 요소에 대한 시각화가 한꺼번에 가능하지만, 추세와 월별 계절 패턴에 대해서만 시각화해보겠습니다.\n\ncomponents(fit) %>%\n  ggplot(aes(x = Month, y = trend, colour = State)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\ncomponents(fit) %>%\n  ggplot(aes(x = month(Month), y = year, \n             colour = State, group = interaction(year(Month), State))) + \n  geom_line() + \n  scale_x_continuous(breaks = 1:12, labels = month.abb) + \n  xlab(\"Month\")\n\n\n\n\n\n\n\n\n연별 계절패턴의 경우 주별로 대개 비슷하나, 북방 지역(the Northern Territory)의 경우 다른 주들과는 크게 다른 계쩔 패턴을 보여주고 있습니다. 마지막으로, prophet의 예측도 forecast()를 이용해 쉽게 수행할 수 있습니다. 향후 2년에 대해 예측해보았습니다:\n\nfc <- fit %>% \n  forecast(h = 24)\ncafe %>% \n  ggplot(aes(x = Month, y = Turnover, colour = State)) + \n  geom_line() + \n  autolayer(fc)\n\n\n\n\n\n\n\n\nForecasting: Principles and Practices (3rd Ed.)에서는 prophet외에도, 벡터 자기회귀모형, 인공신경망 기반의 시계열 예측모형, 붓스트랩 및 배깅 기법을 활용한 시계열 예측 모형 등의 고급 시계열 예측 모형도 제공해줍니다. 관심있으신 분들은 fpp3을 참고해보시기 바랍니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#맺음말",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/tidy-tools-for-timeseries-tidyverts.html#맺음말",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "맺음말",
    "text": "맺음말\ntidyverts ecosystem이 전반적으로 작동하는 과정을 소개해 보았습니다. 그러나, 시계열 자료의 예측 모델링 대한 이해와 더불어 tidyverts를 좀 더 디테일하게 활용하기 위해서는, Forecasting: Principles and Practices (3rd Ed.)을 참고하시는게 좋을 것이라 생각합니다. tidyverse와 tidymodels를 통해 데이터를 전처리, 예측모형 개발, 개선 등의 과정에 걸리는 시간을 크게 단축시켰듯이, fpp3을 잘 익혀두면 시계열 예측 모델링에 전반적인 과정에 드는 시간을 상당히 단축시킬 수 있을 겁니다.😊\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version date (UTC) lib source\n dplyr         * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n fable         * 0.3.1   2021-05-16 [1] CRAN (R 4.2.0)\n fable.prophet * 0.1.0   2020-08-20 [1] CRAN (R 4.2.0)\n fabletools    * 0.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n feasts        * 0.2.2   2021-06-03 [1] CRAN (R 4.2.0)\n fpp3          * 0.4.0   2021-02-06 [1] CRAN (R 4.2.0)\n ggplot2       * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n lubridate     * 1.8.0   2021-10-07 [1] CRAN (R 4.2.0)\n nycflights13  * 1.0.2   2021-04-12 [1] CRAN (R 4.2.0)\n purrr         * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n Rcpp          * 1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rmarkdown     * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n sessioninfo   * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble        * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr         * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tsibble       * 1.1.1   2021-12-03 [1] CRAN (R 4.2.0)\n tsibbledata   * 0.4.0   2022-01-07 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "",
    "text": "Photo by Makcus Wincler on Unsplash\ntidymodels ecosystem은 R에서 머신러닝을 tidyverse principle로 수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidymodels은 {caret}1을 완벽하게 대체하며, 더 빠르게 그리고 더 직관적인 코드로 모델링을 수행할 수 있습니다. {tidymodels}는 모델링에 필요한 패키지들의 묶음이라고 보면 됩니다. {tidyverse}처럼 {tidymodels}를 로딩하면 모델링에 쓰이는 여러 패키지의 묶음을 불러와줍니다. 그중에는 {ggplot2}와 {dplyr} 같은 {tidyverse}에 포함되는 패키지들도 있습니다. 본격적으로 튜토리얼을 시작하기 전에 필요한 패키지와 데이터를 먼저 불러오겠습니다.\n본 튜토리얼에서 이용할 toy data는 `diamonds{ggplo2}`💎입니다. 해당 데이터는 다이아몬드의 등급과 크기 및 가격에 관한 정보를 갖습니다:\n다음은 우리가 모델링에 사용할 features(\\(X\\))들의 상관계수 행렬을 시각화 한 것이며, 상관계수 행렬을 다이아몬드의 가격(price, \\(y\\)) 열의 상관계수의 절댓값을 기준으로 내림차순 정렬하여 그린 것입니다.\ntoy data를 이용해 {tidymodels}의 전반적인 진행 과정을 보여주는 예제이기 때문에, 상관계수 행렬 그림은 전체 데이터가 아닌 2,000개만을 샘플링하여 그렸습니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-분할-rsample",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-분할-rsample",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "1 데이터 분할: {rsample}",
    "text": "1 데이터 분할: {rsample}\ntidymodels ecosystem을 구성하는 패키지들 중 가장 먼저 소개할 친구는 데이터 분할에 쓰이는 {rsample}입니다. 본 예제의 마지막 단계에서 시험 자료(test data)를 기반으로 모형의 예측 성능을 평가할 것이기 때문에, 먼저 데이터를 훈련 자료(training data), 시험 자료로 분할해야 합니다. 이번에도 모형 적합 및 교차 검증을 이용한 모수 튜닝 단계에서의 계산 비용 절감을 위해, 훈련 자료의 비율을 10%로 낮게 잡아 데이터를 나눌 것입니다. 다음의 모든 과정은 {rsample} 패키지의 함수들로 진행됩니다. 패키지 또는 함수의 이름이 직관적이고 인간 친화적이면 그 역할을 기억하기 쉬운데, 앞으로 소개할 {tidymodels}를 구성하는 패키지와 패키지를 이루는 함수들의 이름은 대부분 이러한 점을 고려하여 네이밍이 되어있습니다.😊\n\nset.seed(1)\ndia_split <- initial_split(diamonds, prop = .1, strata = price)\ndia_train <- training(dia_split)\ndia_test <- testing(dia_split)\ncat(\"the number of observations in the training set is \", \n    nrow(dia_train), \n    \".\\n\",\n     \"the number of observations in the test set is \", \n    nrow(dia_test), \".\", \n    sep = \"\")\n\nthe number of observations in the training set is 5393.\nthe number of observations in the test set is 48547."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-전처리-및-feature-engineering-recipes",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#데이터-전처리-및-feature-engineering-recipes",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "2 데이터 전처리 및 Feature Engineering: {recipes}",
    "text": "2 데이터 전처리 및 Feature Engineering: {recipes}\n다음으로는 {recipes}를 이용하여, 데이터 전처리 및 Feature Engineering을 수행한다. recipe는 요리법이라는 뜻뿐만 아니라 특정 결과를 가져올 듯한 방안의 뜻2도 갖습니다. 이럴 때마다 영어권의 R 유저들이 부럽습니다. 패키지나 함수 이름을 통해 그 역할을 기억하고 필요할 때 꺼내쓰기가 좀 더 편하지 않을까 하는 생각이 드네요. {recipes}의 step_*() 함수들을 이용해 모델링에 사용할 자료를 준비3할 수 있습니다. 다음의 산점도는 다이아몬드의 가격(price)과 carat 사이에 비선형적인 관계가 있음을 암시하며, 이러한 관계는 carat의 다항함수를 변수로 도입하여 모델링에 반영할 수 있습니다.\n\nqplot(carat, price, data = dia_train) +\n  scale_y_continuous(trans = log_trans(), labels = function(x) round(x, -2)) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x, 4)\") +\n  labs(title = \"The degree of the polynomial is a potential tuning parameter\")\n\n\n\n\n\n\n\n\nrecipe()는 자료와 모형식을 인수로 하며, step_*() 함수들을 이용하여 step by step👞으로 다양한 전처리를 수행할 수 있게끔 해줍니다.4 여기서는 \\(y\\)에 로그 변환(step_log())을 수행하고, 연속형 예측변수5에 표준화(중심화 및 척도화, step_normalize()), 범주형 예측변수는 더미 변수화(step_dummy())를 수행합니다. 그리고, step_poly()를 이용해 carat의 2차 효과를 반영해주었습니다. 준비가 끝난 recipe 객체는 prep() 함수를 통해 자료에 수행된 전처리들을 확인할 수 있다.\n\ndia_rec <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = 2)\nprep(dia_rec)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nTraining data contained 5393 data points and no missing data.\n\nOperations:\n\nLog transformation on price [trained]\nCentering and scaling for carat, depth, table, x, y, z [trained]\nDummy variables from cut, color, clarity [trained]\nOrthogonal polynomials on carat [trained]\n\n\nrecipe 객체에 prep()를 적용한 것에 juice()를 수행하면 전처리가 수행된 자료를 추출할 수 있죠.\n\ndia_juiced <- juice(prep(dia_rec))\nglimpse(dia_juiced)\n\nRows: 5,393\nColumns: 25\n$ depth        <dbl> 0.52494063, -0.86779062, -0.51960781, 0.59457719, 0.24639…\n$ table        <dbl> -0.2037831, 1.5902131, 0.6932150, -0.2037831, -0.6522821,…\n$ x            <dbl> -1.5716610, -1.5805830, -1.2772351, -1.3218451, -1.277235…\n$ y            <dbl> -1.6114895, -1.5845446, -1.2612061, -1.3061143, -1.261206…\n$ z            <dbl> -1.5470415, -1.6483712, -1.3154309, -1.2575282, -1.243052…\n$ price        <dbl> 5.872118, 5.877736, 6.003887, 6.003887, 6.313548, 6.31716…\n$ cut_1        <dbl> 3.162278e-01, -1.481950e-18, 6.324555e-01, -1.481950e-18,…\n$ cut_2        <dbl> -0.2672612, -0.5345225, 0.5345225, -0.5345225, 0.5345225,…\n$ cut_3        <dbl> -6.324555e-01, -3.893692e-16, 3.162278e-01, -3.893692e-16…\n$ cut_4        <dbl> -0.4780914, 0.7171372, 0.1195229, 0.7171372, 0.1195229, 0…\n$ color_1      <dbl> 3.779645e-01, -5.669467e-01, 3.779645e-01, 3.779645e-01, …\n$ color_2      <dbl> -5.621884e-17, 5.455447e-01, -5.621884e-17, -5.621884e-17…\n$ color_3      <dbl> -4.082483e-01, -4.082483e-01, -4.082483e-01, -4.082483e-0…\n$ color_4      <dbl> -0.5640761, 0.2417469, -0.5640761, -0.5640761, 0.2417469,…\n$ color_5      <dbl> -4.364358e-01, -1.091089e-01, -4.364358e-01, -4.364358e-0…\n$ color_6      <dbl> -0.19738551, 0.03289758, -0.19738551, -0.19738551, 0.0328…\n$ clarity_1    <dbl> 0.07715167, -0.07715167, -0.38575837, -0.23145502, -0.231…\n$ clarity_2    <dbl> -0.38575837, -0.38575837, 0.07715167, -0.23145502, -0.231…\n$ clarity_3    <dbl> -0.1846372, 0.1846372, 0.3077287, 0.4308202, 0.4308202, -…\n$ clarity_4    <dbl> 0.3626203, 0.3626203, -0.5237849, -0.1208734, -0.1208734,…\n$ clarity_5    <dbl> 0.3209704, -0.3209704, 0.4921546, -0.3637664, -0.3637664,…\n$ clarity_6    <dbl> -0.30772873, -0.30772873, -0.30772873, 0.55391171, 0.5539…\n$ clarity_7    <dbl> -0.59744015, 0.59744015, 0.11948803, -0.35846409, -0.3584…\n$ carat_poly_1 <dbl> -0.01605633, -0.01634440, -0.01432792, -0.01432792, -0.01…\n$ carat_poly_2 <dbl> 0.017042209, 0.017792818, 0.012731517, 0.012731517, 0.012…\n\n\n또한, recipe 객체에 prep()를 적용한 것에 juice()가 아닌 bake()를 수행하면 새로운 자료에 recipe 객체에 수행했던 것과 같은 전처리를 수행할 수 있습니다. 예를 들어, 다음은 시험 자료에 대해 훈련 자료에 수행한 전처리를 수행한 뒤에 해당 자료를 추출하라는 것과 같죠. 시험 자료의 예측을 통한 모형의 성능평가에는 사전에 훈련자료와 동일한 전처리가 필요로되는데, bake()는 이러한 시간을 크게 단축시켜줍니다.\n\nglimpse(\n  bake(prep(dia_rec), dia_test)\n)\n\nRows: 48,547\nColumns: 25\n$ depth        <dbl> -0.1714250, -1.3552466, -3.3747069, 0.4553041, 1.0820331,…\n$ table        <dbl> -1.1007812, 1.5902131, 3.3842094, 0.2447160, 0.2447160, -…\n$ x            <dbl> -1.589505, -1.643037, -1.500285, -1.366455, -1.241547, -1…\n$ y            <dbl> -1.575563, -1.701306, -1.494728, -1.351022, -1.243243, -1…\n$ z            <dbl> -1.604944, -1.778652, -1.778652, -1.315431, -1.141723, -1…\n$ price        <dbl> 5.786897, 5.786897, 5.789960, 5.811141, 5.814131, 5.81711…\n$ cut_1        <dbl> 6.324555e-01, 3.162278e-01, -3.162278e-01, 3.162278e-01, …\n$ cut_2        <dbl> 0.5345225, -0.2672612, -0.2672612, -0.2672612, -0.2672612…\n$ cut_3        <dbl> 3.162278e-01, -6.324555e-01, 6.324555e-01, -6.324555e-01,…\n$ cut_4        <dbl> 0.1195229, -0.4780914, -0.4780914, -0.4780914, -0.4780914…\n$ color_1      <dbl> -3.779645e-01, -3.779645e-01, -3.779645e-01, 3.779645e-01…\n$ color_2      <dbl> 8.914347e-17, 8.914347e-17, 8.914347e-17, -5.621884e-17, …\n$ color_3      <dbl> 4.082483e-01, 4.082483e-01, 4.082483e-01, -4.082483e-01, …\n$ color_4      <dbl> -0.5640761, -0.5640761, -0.5640761, -0.5640761, 0.2417469…\n$ color_5      <dbl> 4.364358e-01, 4.364358e-01, 4.364358e-01, -4.364358e-01, …\n$ color_6      <dbl> -0.19738551, -0.19738551, -0.19738551, -0.19738551, 0.032…\n$ clarity_1    <dbl> -0.38575837, -0.23145502, 0.07715167, -0.07715167, -0.385…\n$ clarity_2    <dbl> 0.07715167, -0.23145502, -0.38575837, -0.38575837, 0.0771…\n$ clarity_3    <dbl> 0.3077287, 0.4308202, -0.1846372, 0.1846372, 0.3077287, -…\n$ clarity_4    <dbl> -0.5237849, -0.1208734, 0.3626203, 0.3626203, -0.5237849,…\n$ clarity_5    <dbl> 0.4921546, -0.3637664, 0.3209704, -0.3209704, 0.4921546, …\n$ clarity_6    <dbl> -0.30772873, 0.55391171, -0.30772873, -0.30772873, -0.307…\n$ clarity_7    <dbl> 0.11948803, -0.35846409, -0.59744015, 0.59744015, 0.11948…\n$ carat_poly_1 <dbl> -0.01634440, -0.01692054, -0.01634440, -0.01461599, -0.01…\n$ carat_poly_2 <dbl> 0.01779282, 0.01932160, 0.01779282, 0.01342699, 0.0120452…"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-정의-및-적합-parsnip",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-정의-및-적합-parsnip",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "3 모형 정의 및 적합: {parsnip}",
    "text": "3 모형 정의 및 적합: {parsnip}\n이제 훈련 자료에 대한 기본적인 전처리가 끝났으므로, {parsnip}을 이용하여 모형을 정의하고 적합하려고 합니다. {parsnip}은 우리나라 말로 연노란색의 긴 뿌리채소를 뜻하는데, 왜 이렇게 네이밍이 된 지는 아직 잘 모르겠습니다. 영어권의 원어민들은 어떻게 생각할지 궁금하네요. {parsnip}은 인기 있는 수많은 머신러닝 알고리즘6을 제공해줍니다. 그리고, 최대 장점은 단일화된 인터페이스로 여러 모형을 적합할 수 있다는 점이죠. 예를 들어, 랜덤포레스트를 제공하는 두 패키지 {ranger}와 {randomForest}에는 고려할 트리의 개수를 지정하는 모수가 존재하는데 해당 옵션의 이름이 각각 ntree, num.trees로 다릅니다. 이는 사용자들에게 꽤 불편한 점일 수 있는데, {parsnip}은 이러한 문제를 해결해줌으로써 두 인터페이스를 모두 기억할 필요가 없게끔 해줍니다.\n{parsnip}에서는 먼저 특정 함수를 통해 모형을 정의하고7, set_mode()로 어떤 문제8를 해결할 것인지 설정한 뒤에, 마지막으로 어떤 시스템 또는 패키지를 이용하여 해당 모형을 적합할지를 set_engine()으로 설정합니다. 여기서는 먼저 stats::lm() 엔진을 이용하여 기본적인 회귀모형으로 적합을 시작해 보겠습니다.\n\nlm_model <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\n본격적인 모형 적합 전에, 앞서 언급했던 {parsnip}의 장점을 확인해보기 위해 랜덤포레스트를 예로 들어보겠습니다. 랜덤포레스트 모형의 적합에는 {ranger} 또는 {randomForest}를 이용할 수 있는데, 서로 조금 다른 인터페이스를 지닌다고 했었습니다. {parsnip}은 다음과 같이 엔진 설정 전에 {parsnip}만의 함수로 먼저 모형을 정의하고 해당 함수에서 모수를 설정함으로써 서로 다른 인터페이스를 통합하여줍니다.\n\nrand_forest(mtry = 3, trees = 500, min_n = 5) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\", importance = \"impurity_corrected\")\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 500\n  min_n = 5\n\nEngine-Specific Arguments:\n  importance = impurity_corrected\n\nComputational engine: ranger \n\n\n이제 다시 회귀모형으로 돌아오겠습니다. 설정했던 기본적인 회귀모형을 전처리를 완료한 훈련 자료에 적합해 줍니다.\n\nlm_fit1 <- fit(lm_model, price ~ ., dia_juiced)\nlm_fit1\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = price ~ ., data = data)\n\nCoefficients:\n (Intercept)         depth         table             x             y  \n   7.7110881     0.0582418     0.0138979     0.8357574     0.2337963  \n           z         cut_1         cut_2         cut_3         cut_4  \n   0.0532288     0.1134826    -0.0282144     0.0315527    -0.0020513  \n     color_1       color_2       color_3       color_4       color_5  \n  -0.4452258    -0.0887138    -0.0090620     0.0071217    -0.0059503  \n     color_6     clarity_1     clarity_2     clarity_3     clarity_4  \n  -0.0001745     0.9025208    -0.2480065     0.1424917    -0.0664178  \n   clarity_5     clarity_6     clarity_7  carat_poly_1  carat_poly_2  \n   0.0265924     0.0031308     0.0245773    -3.1129423    -6.9995161  \n\n\n예제에서 사용되진 않았지만, step_rm()을 이용하여 사전에 모델링에 필요 없는 변수는 제거할 수도 있습니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#적합된-모형-요약-broom",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#적합된-모형-요약-broom",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "4 적합된 모형 요약: {broom}",
    "text": "4 적합된 모형 요약: {broom}\nR에서 여러 모형 객체들의 요약은 summary() 또는 coef()와 같은 함수로 이루어집니다. 그러나, 이러한 함수들의 출력물은 타이디한 포맷9으로 주어지지 않습니다. {broom} 패키지는 적합 된 모형의 요약을 타이디한 포맷으로 제공해줍니다. broom은 빗자루와 같은 브러쉬를 의미하는 명사인데, 적합한 모형을 깨끗하게 쓸어 담는 패키지라고 생각하면 기억하기 쉽지 않을까 싶습니다. 이와 같이 패키지 이름, 함수 이름 하나하나를 신중하게 네이밍하는 일관성은 {tidyverse}, {tidymodels}에 포함되는 패키지들의 공통된 좋은 특징이라 할 수 있다. 실제로 R4DS10 책에서도 Hadley Wickham은 객체의 이름이나 함수의 이름을 설정하는 것에 있어서 어느정도의 시간을 투자하는 것은 전혀 아깝지 않다고 말하기도 했습니다.\n{broom} 패키지를 구성하는 첫 번째 함수로 glance()를 소개합니다. glance는 힐끗 본다는 뜻을 갖는다는 점에서 추측할 수 있듯이, 적합된 모형의 전체적인 정보를 간략히 제공해줍니다.\n\nglance(lm_fit1$fit)\n\n\n\n\n\n  \n\n\n\n적합된 모형의 수정된 \\(R^2\\) 값(adj.r.squared)은 약 98.27%로 상당히 높은 설명력을 보여줍니다. RMSE는 sigma 열에서 확인할 수 있습니다. 다음으로 tidy()는 추정된 모수에 대한 정보를 제공합니다. 다음의 결과에서 우리는 carat의 2차 효과가 유의하게 존재함을 알 수 있습니다. 통계량의 크기를 기준으로 내림차순으로 정렬하여 표시하였습니다.\n\ntidy(lm_fit1) %>% \n    arrange(desc(abs(statistic)))\n\n\n\n\n\n  \n\n\n\n마지막으로 augment()는 모형의 예측값, 적합값 등을 반환해줍니다. augment는 우리나라 말로 어떤 것의 양 또는 값, 크기 등을 늘리는 것11을 뜻하는 동사로, 해당 함수도 이름을 통해 어느정도 그 역할을 가늠할 수 있죠.\n\nlm_predicted <- augment(lm_fit1$fit, data = dia_juiced) %>% \n  rowid_to_column()\nselect(lm_predicted, rowid, price, .fitted:.std.resid)\n\n\n\n\n\n  \n\n\n\n앞서 생성한 lm_predicted 객체를 이용해 적합값과 관측값 간의 산점도를 그려보았습니다. 잔차의 크기가 2 이상인 관측치에 대해서는 해당 관측치의 행 번호를 붙여주었으며, 겹치는 점이 있는 경우를 고려하여 점에 투명도를 주었습니다.\n\nggplot(lm_predicted, aes(.fitted, price)) +\n  geom_point(alpha = .2) +\n  ggrepel::geom_label_repel(aes(label = rowid),\n                            data = lm_predicted %>% filter(abs(.resid) > 2)) +\n  labs(x = \"fitted values\",\n       y = \"observed values\")\n\n\n\n\n\n\n\n\n원자료의 각 행을 의미하는 두 단어 관측값(observed values)과 실제값(actual values)은 서로 통용되니 어떤 용어를 써도 문제가 없습니다. 특히, 머신러닝에서는 이를 데이터포인트(data point)라고 표현하기도 합니다. 3가지 용어 모두 통용되는 말이니 몰랐다면 알아둡시다. 모든 학문에서 그렇겠지만 통계학에서는 특히 정확한 용어 정의가 중요하므로, 비슷한 용어 또는 비슷한 듯 다른 용어들이 있다면 틈틈이 정리하는 습관을 갖는 것이 좋다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-성능-평가-yardstick",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형-성능-평가-yardstick",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "5 모형 성능 평가: {yardstick}",
    "text": "5 모형 성능 평가: {yardstick}\n위에서 glance()를 통해 적합된 모형의 성능을 RMSE, \\(R^2\\)를 통해 힐끗 확인할 수 있었습니다. {yardstick}은 모형의 성능에 대한 여러 측도를 계산하기 위한 패키지입니다. 물론, \\(y\\)가 연속형이든 범주형이든 문제없으며 교차 검증(Cross Validation, CV)에서 생산되는 그룹화된 예측값들과도 매끄럽게 잘 작동한다. yardstick은 기준, 척도를 뜻하는 명사에 해당하므로, 기억하기도 쉬울 것이라 생각합니다. 이제는 {rsample}, {parsnip}, {yardstick}으로 교차 검증을 수행하여 좀 더 정확한 RMSE를 추정해봅시다.\n다음 코드 블럭들에서 나타날 긴 파이프라인(pipeline, %>%)들을 정리해서 간략히 나타내면 다음과 같습니다. 천천히 음미해보시기 바랍니다:\n\nrsample::vfold_cv()를 훈련용 자료를 3-fold CV를 수행할 수 있도록 분할\nrsample::analysis()와 rsample::assessment()를 이용해 각 분할에서 모형 훈련용, 평가용 자료를 불러옴\n앞서 만든 모형 적합 전 전처리가 완료된 recipe 객체 dia_rec을 각 fold의 모형 훈련용 자료에 prepped 시킴\npreped한 훈련용 자료를 recipes::juice()로 불러오고, recipes::bake()를 이용해 훈련용 자료에 처리한 것과 같은 처리를 평가용 자료에 수행\nparsnip::fit()으로 3개의 모형 적합용(analysis) 자료 각각에 모형을 적합(훈련)\npredicted()로 훈련시킨 각 모형으로 평가용(assessment) 자료를 예측\n\n\nset.seed(1)\ndia_vfold <- vfold_cv(dia_train, v = 3, strata = price)\ndia_vfold\n\n\n\n\n\n  \n\n\n\n\nlm_fit2 <- mutate(dia_vfold,\n                  df_ana = map(splits, analysis),\n                  df_ass = map(splits, assessment))\nlm_fit2\n\n\n\n\n\n  \n\n\n\n\nlm_fit3 <- lm_fit2 %>% \n  mutate(\n    recipe = map(df_ana, ~prep(dia_rec, training = .x)),\n    df_ana = map(recipe, juice),\n    df_ass = map2(recipe,\n                  df_ass, ~bake(.x, new_data = .y))) %>% \n  mutate(\n    model_fit = map(df_ana, ~fit(lm_model, price ~ ., data = .x))) %>% \n  mutate(\n    model_pred = map2(model_fit, df_ass, ~predict(.x, new_data = .y)))\nselect(lm_fit3, id, recipe:model_pred)\n\n\n\n\n\n  \n\n\n\n여기서 tidymodels ecosystem의 마법을 확인할 수 있습니다. 위 과정에서 확인했다시피, 꽤 복잡한 과정들이 단 하나의 티블 객체 lm_fit2에서 이루어졌습니다. 이렇게 복잡한 작업이 단 하나의 티블 객체만으로 이루어질 수 있었던 이유는, 티블은 리스트-열(list-column)을 가질 수 있기 때문이죠. 덕분에 우리는 R에서 연산이 느린 반복문(e.g. for(), while())을 사용하지 않고, purrr::map()을 loop로 이용하여 반복문을 통한 지루하고 느린 모델링 작업을 완벽한 함수형 프로그래밍으로 수행할 수 있게 되었습니다. R 사용자라면 어디서 한번 쯤은 반복문의 사용은 지양하고, 함수형 프로그래밍을 해야 한다고 들어봤을 것입니다. {tidymodels}이 모델링 과정을 {tidyverse}와 함께 작동할 수 있게 해줌으로써, 한 자료에 대해서 여러 가지 모형의 적합, 교차검증을 통한 모수 튜닝, 예측 성능평가 등의 작업을 통해 경험적으로(empirically) 최적의 모형을 선택하는 수고가 필요한 머신러닝에 드는 시간을 상당히 줄여줬다고 할 수 있습니다.\n이쯤 되면 제가 왜 {tidyverse}를 좋아하고, {tidymodels}의 튜토리얼을 이렇게 상세하게 기술하는지 이해하실 거라고 생각합니다. 이제 평가용 자료로부터 실제 관측값(price)을 추출하여 예측값(.pred)과 비교한 뒤, yardstick::metrics()를 이용해 여러 평가 측도를 계산해보려고 합니다.\n\nlm_preds <- lm_fit3 %>% \n  mutate(res = map2(df_ass, model_pred, ~data.frame(price = .x$price, \n                                                    .pred = .y$.pred))) %>% \n  select(id, res) %>% \n  tidyr::unnest(res) %>% \n  group_by(id)\nlm_preds\n\n\n\n\n\n  \n\n\n\n\nmetrics(lm_preds, truth = price, estimate = .pred)\n\n\n\n\n\n  \n\n\n\n여기서 계산한 평가 측도의 값은 out-of-sample에 대한 성능이므로 모형 적합값에 대해 평가 측도를 계산한 glance(lm_fit1$fit)의 결과와 비교하여 보면 당연히 조금은 떨어지는 성능을 보입니다. metrics()는 연속형 outcome(\\(y\\))에는 위와 같이 RMSE, \\(R^2\\), MAE를 기본적인 측도로 제공해줍니다. 물론, 범주형 outcome에 대해서도 다른 기본적인 측도를 제공해주죠. 또한, 하나의 측도만으로 비교하길 원한다면 rmse()와 같이 RMSE 값만을 제공해주는 함수도 이용할 수 있으며, metric_set()을 이용하면 원하는 metrics들을 직접 커스텀하여 정의할 수도 있습니다.\n3-fold CV를 통해 훈련 자료를 분할 및 전처리하고 예측값을 구하여 RMSE를 계산하는 과정을 담은 앞선 코드블럭들은 {tidyverse}, {tidymodels}에 익숙한 사람이라면 편하게 읽어나가실 수 있을겁니다. 그러나, 코드가 매우 긴 것도 사실입니다. 사실, 위 코드블럭은 다음 섹션에서 소개할 {tune} 패키지를 이용하면 다음과 같이 단 몇 줄로 간결하게 코딩할 수 있습니다.\n\ncontrol <- control_resamples(save_pred = TRUE)\nset.seed(1)\nlm_fit4 <- fit_resamples(lm_model, dia_rec, dia_vfold, control = control)\nlm_fit4 %>% \n    pull(.metrics)\n\n\n\n[[1]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.143 Preprocessor1_Model1\n2 rsq     standard       0.980 Preprocessor1_Model1\n\n[[2]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.127 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1\n\n[[3]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.130 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형의-모수-튜닝-tune-dials",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#모형의-모수-튜닝-tune-dials",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "6 모형의 모수 튜닝: {tune}, {dials}",
    "text": "6 모형의 모수 튜닝: {tune}, {dials}\ntune은 조정하다12 라는 뜻을 갖는 동사이며, 말 그대로 {tune} 패키지는 모수를 튜닝(조율)하는(e.g. via grid search) 함수들을 제공합니다. 그리고, 어떤 것을 조정하는 다이얼13을 의미하는 이름을 갖는 {dials} 패키지는 {tune}을 통해 튜닝할 모수들을 정하는 역할을 합니다. 즉, {tune}과 {dials}는 대개 함께 쓰이는 패키지라고 보면 됩니다. 본 예제에서는 랜덤포레스트 모형을 튜닝하는 과정을 보여줄 것입니다.\n\n6.1 튜닝을 위한 {parsnip} 모형 객체 준비\n첫 번째로, 랜덤포레스트 모형을 형성할 때 매 트리 적합시 고려할 변수들의 개수를 조정하는 mtry 모수를 조율해줍니다. tune()을 placeholder로 하여 후에 교차검증을 통해 최적의 mtry를 선정할 입니다.\n다음 코드블럭의 출력물은 mtry의 기본 최솟값은 1이고 최댓값은 자료에 의존함을 의미합니다. 어떤 자료를 다루느냐에 따라 feature의 수는 다르므로, 따로 지정하지 않는한 mtry의 최댓값은 자료에 의존하게 됩니다.\n\nrf_model <- rand_forest(mtry = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\nparameters(rf_model)\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[?]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\nmtry()\n\n# Randomly Selected Predictors (quantitative)\nRange: [1, ?]\n\n\n아직 랜덤포레스트 모형의 적합에 쓰이는 모수 값을 결정하지 않았으므로 모형을 훈련 자료에 적합할 준비가 된 상태가 아니라고 할 수 있습니다. 그리고, mtry의 최댓값은 update()를 사용해 원하는 값을 명시할 수도 있고, 또는 finalize()를 사용해 해당 자료가 갖는 예측변수의 수로 지정할 수도 있죠.\n\nrf_model %>% \n  parameters() %>% \n  update(mtry = mtry(c(1L, 5L)))\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[+]\n\n\n\nrf_model %>% \n  parameters() %>% \n  finalize(x = juice(prep(dia_rec)) %>% select(-price)) %>% \n  pull(\"object\")\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, 24]\n\n\n\n\n6.2 튜닝을 위한 자료 준비: {recipes}\n두 번째로 튜닝하고 싶은 것은 carat의 다항식 차수입니다. 2 데이터 전처리 및 Feature Engineering: {recipes}의 그림에서 확인했듯이, 최대 4차까지의 다항식이 자료에 잘 적합 될 수 있음을 알 수 있습니다. 그러나, 우리는 모수 절약의 원칙(priciplt of parsimony)14을 생각할 필요가 있고, 그에 따라 더 간단한 모형도 자료에 잘 적합 될 수 있다는 가능성을 배제해서는 안됩니다. 그래서, carat의 다항식 차수 또한 교차 검증을 통해 최대한 간단하면서 좋은 성능을 내는 carat의 차수를 찾을 것입니다.\n모형의 적합에서 각 모형이 갖는 고유한 초모수15와 달리 예측변수 carat의 차수는 {recipe}를 통해 새로운 레시피 객체를 만들어 튜닝이 진행됩니다. 그 과정은 초모수를 튜닝했던 과정과 유사합니다. 다음과 같이 step_poly()에 tune()을 사용하여 훈련 자료(dia_train())에 대한 2번째 레시피 객체를 만들어 줍니다.\n\ndia_rec2 <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = tune())\n\ndia_rec2 %>% \n  parameters() %>% \n  pull(\"object\")\n\nWarning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\n[[1]]\nPolynomial Degree (quantitative)\nRange: [1, 3]\n\n\n고려하는 다항식의 차수 범위가 기본값으로 설정하여 [1, 3]으로 되어있는데, 이 부분은 다음 섹션에서 {workflows} 패키지를 소개하며 개선할 부분이니 신경 쓰지 않으셔도 됩니다.\n\n\n6.3 모든 것을 결합하기: {workflows}\nworkflow를 직역하면 어떤 작업의 흐름을 뜻하듯이, {workflows} 패키지는 recipe나 model 객체와 같은 머신러닝 파이프라인의 다른 부분이라 할 수 있는 것들을 한 번에 묶어주는 역할을 합니다.\n이를 위해서는 먼저 workflow()를 선언하여 객체를 만들고, 6.2 튜닝을 위한 자료 준비: {recipes}에서 만든 recipe 객체와 6.1 튜닝을 위한 {parsnip} 모형 객체 준비에서 만든 랜덤포레스트 모형 객체를 add_*()로 결합해줍니다.\n\nrf_wflow <- workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(dia_rec2)\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_dummy()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger \n\n\n아직 mtry의 최댓값이 알려져있지 않고 degree의 최댓값이 기본 설정인 3으로 설정되어 있으므로, 두 번째로는 rf_wflow 객체의 모수 설정을 update()로 갱신할 것입니다.\n\nrf_param <- rf_wflow %>% \n  parameters() %>% \n  update(mtry = mtry(range = c(3L, 5L)),\n         degree = degree_int(range = c(2L, 4L)))\n\nWarning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\nrf_param %>% pull(\"object\")\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [3, 5]\n\n[[2]]\nPolynomial Degree (quantitative)\nRange: [2, 4]\n\n\n앞서 말했듯이 교차검증을 통해 튜닝을 수행할 것이기 때문에, 세 번째로는 설정한 모수들의 조합을 만들어야 합니다. 복잡한 튜닝 문제에는 tune_bayes()를 통한 베이지안 최적화(Bayesian optimization)(Silge 와/과 Julia, 일자 없음)가 추천되지만, 해당 예제에서 고려하는 초모수들의 조합 정도는 grid search로도 충분해 보입니다. 다음과 같이 필요로 되는 모든 모수 조합의 grid를 만듭니다.\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid\n\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid %>% \n    paged_table()\n\n\n\n  \n\n\n\n여기서 levels는 grid를 만드는 데 사용되는 각 모수의 수에 대한 정숫값을 조정하는 옵션입니다. default 값이 levels = 3이므로 해당 옵션은 생략해도 문제없을 것입니다. 교차 검증을 통한 모수 튜닝에는 수많은 모형을 적합해야 하는데, 이 예제에서는 9개의 모수 집합과 3개의 folds를 사용하므로 총 \\(3 \\times 9 = 27\\)개의 모형을 적합해야 한다. 27개의 모형을 빠르게 적합하기 위해 병렬처리를 수행하려고 합니다. 이는 {tune} 패키지에서 직접적으로 지원받을 수 있습니다.\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: future\n\n\n\nAttaching package: 'future'\n\n\nThe following object is masked from 'package:rmarkdown':\n\n    run\n\nall_cores <- parallel::detectCores(logical = FALSE) - 1\n\nregisterDoFuture()\ncl <- parallel::makeCluster(all_cores)\nplan(future::cluster, workers = cl)\n\n이제 튜닝을 시작합니다.\n\noptions(future.rng.onMisue = \"ignore\")\nrf_search <- tune_grid(rf_wflow, grid = rf_grid, resamples = dia_vfold,\n                       param_info = rf_param)\n\n튜닝 결과는 autoplot()과 show_best()로 검토할 수 있습니다:\n\nautoplot(rf_search, metric = \"rmse\")\n\n\n\n\n\n\n\n\n\\(x\\) 축은 mtry를 나타내며, 각 선의 색상은 고려한 다항식 차수를 나타냅니다. mtry는 5와 carat의 2차항까지 고려한 초모수 조합이 최적임을 알 수 있습니다. show_best()로도 확인할 수 있습니다:\n\nshow_best(rf_search, \"rmse\", n = 9)\n\n\n\n\n\n  \n\n\n\n\nselect_best(rf_search, metric = \"rmse\")\n\n\n\n\n\n  \n\n\n\n그리고, select_by_one_std_err()을 이용하면 원하는 metric 값의 \\(\\pm 1SE\\)를 고려한 최적의 초모수 조합을 얻을 수도 있죠.\n\nselect_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\n\n\n\n\n\n  \n\n\n\n\n\n6.4 선택한 최적의 모형으로 예측 수행\n6.3 모든 것을 결합하기: {workflows}에서 carat 변수는 2차항으로도 충분히 설명되고, 매 트리 적합 시 고려할 변수의 수는 5개임을 확인할 수 있었습니다. 이제는 해당 초모수 조합을 이용해 훈련 자료에 모형을 적합하고 최종 예측을 수행하려고 합니다. 이번 예제에서는 설정값이 똑같긴 하지만, \\(\\pm 1SE\\)를 고려한 초모수 조합을 모형 적합에 사용하였습니다.\n\nrf_param_final <- select_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\nrf_wflow_final <- finalize_workflow(rf_wflow, rf_param_final)\nrf_wflow_final_fit <- fit(rf_wflow_final, data = dia_train)\n\n이제 적합된 모형객체 rf_wflow_final_fit으로 원하는 unobserved 자료16를 predict()로 예측할 수 있다. 우리에게는 미리 나눠둔 시험 자료 dia_test가 있습니다. 다만, dia_test의 \\(y\\)는 로그변환이 취해지지 않았으므로, predict(rf_wflow_final_fit, new_data = dia_test)가 아닌 {recipe}로 step_log()를 취해주어야 합니다. 여기서는 workflow로부터 추출한 prepped된 recipe 객체를 이용해 시험 자료에 대하여 bake()를 취할 것입니다. 그리고, baked된 시험 자료를 적합한 최종 모형을 통해 예측할하면 되죠. bake()가 이렇게나 편합니다:\n\ndia_rec3 <- pull_workflow_prepped_recipe(rf_wflow_final_fit)\nrf_final_fit <- pull_workflow_fit(rf_wflow_final_fit)\n\ndia_test$.pred <- predict(rf_final_fit,\n                          new_data = bake(dia_rec3, dia_test)) %>% pull(.pred)\ndia_test$logprice <- log(dia_test$price)\n\nmetrics(dia_test, truth = logprice, estimate = .pred)\n\n\n\nWarning: `pull_workflow_prepped_recipe()` was deprecated in workflows 0.2.3.\nPlease use `extract_recipe()` instead.\n\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\n\n\n\n  \n\n\n\n시험 자료에 대한 RMSE는 약 0.11로 교차 검증에서 계산된 RMSE보다는 조금 더 나은 성능을 보여줍니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#맺음말",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/tidyvese-principle.html#맺음말",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "맺음말",
    "text": "맺음말\n{tidymodels}의 ecosystem은 머신러닝 문제를 풀기 위해 필요한 첫 단계부터 끝까지 함께 작동하는 패키지들의 집합을 한대 묶어 제공해줍니다. 또한, {tidyverse}를 통한 data-wrangling 기능과 훌륭한 시각화 패키지 {ggplot2}와도 함께 작동하는 {tidymodels}은 R을 사용하는 데이터 사이언티스트들에게는 더없이 풍부한 toolbox라 할 수 있을 것 같습니다. 아울러, 해당 튜토리얼에서는 예측 모형들을 결합해주는17 기능을 갖는 패키지 {stacks}에 대한 내용을 다루지 않았는데18, {tidymodels}을 불러올 때 로딩이 되는 패키지는 아니지만, {stacks} 또한 {tidymodels}의 한 부분으로 소개되는 패키지에 해당합니다. 그리고, tidymodels ecosystem을 “머신러닝”에만 국한시키기에는 너무나도 많은 기능들이 업데이트되고 있습니다. 최근엔 반복측정자료분석에 자주 쓰이는 모형 중 하나인 혼합효과모형(linear mixed model)까지 지원하기 시작했습니다:\n\n\nLots of new #rstats package versions! Here’s a summary for the parsnip packages, including the new {multilevelmod} package!https://t.co/rv5Z9izpho— Max Kuhn (@topepos) March 24, 2022\n\n\n\ntidyverse 블로그를 꼭 팔로우업하세요. 본 튜토리얼은 20년 2월에 작성된 글을 기반으로 쓰여졌기 때문에 최신이라고 하긴 어렵습니다.😂 그러나, tidymodels ecosystem의 기본기를 익히기에는 충분할 겁니다. 이 튜토리얼이 {tidymodels}을 배우길 원하는, R로 머신러닝을 수행하길 원하는 우리나라 R 유저들에게 조금이나마 도움이 됐으면 좋겠습니다.\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n corrplot     * 0.92    2021-11-18 [1] CRAN (R 4.2.0)\n dials        * 1.0.0   2022-06-14 [1] CRAN (R 4.2.0)\n doFuture     * 0.12.2  2022-04-26 [1] CRAN (R 4.2.0)\n dplyr        * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n foreach      * 1.5.2   2022-02-02 [1] CRAN (R 4.2.0)\n future       * 1.27.0  2022-07-22 [1] CRAN (R 4.2.0)\n ggplot2      * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n ggrepel      * 0.9.1   2021-01-15 [1] CRAN (R 4.2.0)\n infer        * 1.0.2   2022-06-10 [1] CRAN (R 4.2.0)\n modeldata    * 1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n parsnip      * 1.0.0   2022-06-16 [1] CRAN (R 4.2.0)\n purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n recipes      * 1.0.1   2022-07-07 [1] CRAN (R 4.2.0)\n rmarkdown    * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n rmdformats   * 1.0.4   2022-05-17 [1] CRAN (R 4.2.0)\n rsample      * 1.0.0   2022-06-24 [1] CRAN (R 4.2.0)\n scales       * 1.2.0   2022-04-13 [1] CRAN (R 4.2.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble       * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidymodels   * 1.0.0   2022-07-13 [1] CRAN (R 4.2.0)\n tidyr        * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tune         * 1.0.0   2022-07-07 [1] CRAN (R 4.2.0)\n tweetrmd     * 0.0.9   2022-09-13 [1] Github (gadenbuie/tweetrmd@075102b)\n workflows    * 1.0.0   2022-07-05 [1] CRAN (R 4.2.0)\n workflowsets * 1.0.0   2022-07-12 [1] CRAN (R 4.2.0)\n yardstick    * 1.0.0   2022-06-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html",
    "title": "관심 논문 읽고 요약하기",
    "section": "",
    "text": "Photo by Aaron Burden on Unsplash\n논문 읽기가 초심자에게는 만만치않은 작업인 만큼, 논문을 정리하는 자기만의 방식을 만들어 놓는 것은 참 중요합니다. 저 또한 아직 초심자라고 생각하고 있는데요, 오늘은 제가 관심있는 논문을 읽고 정리하는 방식에 대해 얘기해보려고 합니다. 제가 스스로 터득한 방법을 소개드리는 것은 아닙니다. 논문을 읽고 정리하는 좋은 방식이 있나 싶어 검색을 하던 도중 좋은 글(An 2022)을 발견하게 됐고, 이 글을 바탕으로 저만의 방식을 정립해봤습니다. 좋은 글을 써주신 안수빈님께 감사의 마음을 전합니다.\n논문 요약은 1st read(첫 번째 읽기), 2nd read(두 번째 읽기) 2가지 섹션으로 진행할 것입니다. 1st read의 결과에 따라 2nd read는 진행되지 않을 수도 있습니다."
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#st-read",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#st-read",
    "title": "관심 논문 읽고 요약하기",
    "section": "1st read",
    "text": "1st read\n첫 번째 읽기의 핵심은 빠르게 읽으며 논문의 큰 그림을 파악하는 것이라고 합니다. 5분에서 10분 정도 다음 순서에 따라 읽으라고 권합니다.\n\n제목(title), 초록(abstract), 소개(introduction)를 집중해서 읽으세요.\n섹션(section)과 하위 섹션(subsection)의 세부내용은 무시하고 제목만 읽으세요.\n결론(conclusion)을 읽으세요.\n참고문헌(reference)을 보며 저자가 인용한 논문, 이전에 읽은 논문에 대해 가볍게 체크하세요.\n\n먼저 1st read에서는 논문을 빠르게 읽으면서 파악한 전반적인 그림에 관해 기술합니다. 첫 번째 읽기를 하고나서는 다음의 여섯 가지(5C + 1M)를 답할 줄 알아야 하며, 본 블로그에서 요약할 논문들 또한 다음과 같은 섹션으로 요약하려고 합니다.\n\nCategory: 논문의 종류\nMain Topic: 논문 제목 및 주제\nContext: 다른 페이퍼들과의 관계, 문제를 풀기 위해 사용한 이론적 바탕\nCorrectness: 논문에 필요한 가정의 명확성\nContributions: 논문의 핵심 기여\nClarity: 논문의 가독성, 명료함\n\n논문에 필요한 가정의 명확성(Correctness)은 제 경우 보통 방법론 부분에서 모델에서 요구하는 가정이나 모델링 과정의 각 단계가 합리적인 근거로 진행 되었는지에 관심이 있으므로, 첫 번째 읽기에서 Method 부분을 빠르게 검토해보는 과정이 필요로 될 것 같습니다. 아울러, 논문의 가독성과 명확성(Clarity)에 관한 부분은 잘 아는 분야가 아니라면 감히 기술하기 어려울 것 같습니다. 때때로 생략할 수도 있는 부분입니다.😅 그리고, 논문의 종류(Category)는 이 글(Hong 2012)을 참고하세요. 5C + 1M을 바탕으로 논문을 더 읽을지 말지 선택할 것입니다. 더 읽지 않는 결정을 한다면, 대부분은 다음의 이유일 겁니다.\n\n관심이 없는 내용\n해당 논문을 읽기엔 사전 지식이 부족\n저자의 가정이 모호 또는 불명확\n\n만약, 꼭 읽어야만 하는 논문임에도 해당 논문을 읽기에 사전 지식이 부족하다면, 참고문헌(reference)들을 다시 검토해보면서 관심있는 연구 분야의 핵심 연구라고 생각 되는 것을 찾아내 읽어보는 과정을 가져야 할겁니다. 또는, 논문에 쓰인 방법론에 관한 이해가 안되어 있는 상태라면 해당 방법론의 Method paper나 Review paper를 찾아보는 것도 큰 도움이 될 겁니다."
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#nd-read",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#nd-read",
    "title": "관심 논문 읽고 요약하기",
    "section": "2nd read",
    "text": "2nd read\n2nd read에서는 좀 더 세부적인 내용에 집중하라고 합니다. 단, 증명같은 디테일은 무시한채 말이죠. 핵심 사항을 노트에 적거나 테두리에 본인의 의견을 써놓는 것을 권장합니다. 두 번째 읽기는 약 1시간 정도가 소모됩니다. 처음 접하는 분야의 논문이나 개인의 논문 독해 실력에 따라 훨씬 더 많은 시간이 소요될 수도 있습니다. 다음과 같은 사항에 주목하여 읽으세요.\n\nFigure, Diagram, Table 등 논문 내 다양한 도표와 일러스트레이션을 주의깊게 보세요. 특히, Data Science에 관심이 있는 분들이라면 그래프를 잘 봐야합니다. 그래프의 \\(x\\)축, \\(y\\)축, 테이블의 행과 열이 의미하는 바 등을 확인하고 이를 통해 저자가 주장하고자 하는 바가 무엇인지 한마디로 정리할 줄 알아야합니다. 물론, 이 부분은 저자가 확실하게 주장하고자 하는 바를 가지고 시각화, 테이블 작성를 수행했다는 전제 하에 있습니다.\n아직 읽지 않은 연관 논문을 체크하세요. 이 과정은 논문의 배경 지식 또는 특정 방법론에 관한 Method paper인 경우 해당 방법론의 모티베이션을 공부하는데 도움이 됩니다.\n\n두 번째 읽기가 끝난 상태에서 우리가 바라는 희망사항은 다음과 같습니다:\n\n논문의 핵심 내용 이해\n논문의 핵심 주장에 대해 근거와 함께 요약할 수 있어야 함\n\n그래서, 두 번째 읽기를 끝낸 논문은 다음과 같은 섹션으로 상세한 추가 요약을 수행할 예정입니다.\n\nMain Findings: 논문의 핵심 주장과 뒷받침 근거\nMethods: Main Findings에 사용된 핵심 방법론에 관한 내용\nResults: Main Findings외 다른 연구 결과\nLimitations: 연구의 한계점\n\nMain Findings외 다른 연구 결과에 해당하는 Results와 연구의 한계점(Limitations)는 때때로 생략될 수 있습니다.\n두 번째 읽기는 당신이 관심있어 하지만, 당신의 전문 연구 분야는 아닌 논문에 적합하다고 합니다. 하지만, 저는 제 전문 연구 분야도 위와 같은 두 번째 읽기를 통해 추가적으로 세부적인 요약을 수행할 예정입니다. 전문 연구 분야라면 훨씬 더 빠르게 두 번째 읽기를 할 수 있겠죠. 그러나, 여러 이유로 두 번째 읽기에도 이해가 안될 수도 있습니다:\n\n이 주제나 내용이 새로워서 전문 용어나 약어에 익숙하지 않음\n저자가 사용한 방법론이나 연구 결과를 낼 때 사용된 테크닉이 이해가 안됨\n합리적 근거가 부족한 주장 또는 너무 많은 레퍼런스\n피곤해서!\n\n이럴 때 3가지 선택지를 제안합니다.\n\n논문을 치우세요. 그리고, 해당 논문의 내용이 커리어에 무관하기를 바라세요.\n배경 지식을 공부하고 다시 읽으세요.\n노력해보고 세 번째 읽기를 해보세요.\n\n거인의 어깨 위에 올라서서 세상을 바라보라고 하는데, 거인에 어깨 위에 올라서는 것 조차 참 어렵습니다..😭"
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#맺음말",
    "href": "posts/2022-05-13-how-to-review-a-paper/how-to-review-a-paper.html#맺음말",
    "title": "관심 논문 읽고 요약하기",
    "section": "맺음말",
    "text": "맺음말\n앞으로 제 블로그에 읽은 논문들을 요약하는 글을 작성하기에 앞서, 논문 요약 방식에 대한 설명이 필요할 것 같아서 쓰게 된 글입니다. 논문 요약 방식에 정답은 없습니다. 각자의 논문 요약 방식에 대해 나눠보는 것도 참 흥미로운 대화 거리가 될 것 같네요. 참고한 글(An 2022)에 더 좋은 내용이 많습니다. 그리고, 해당 글의 세 번째 읽기, 문헌 조사 등 “논문 쓰기”에 도움이 될 만한 내용들 또한 기술이 되어있습니다. 다시 한 번 좋은 글 작성해주신 안수빈님께 감사의 말씀을 전합니다. 저도 아직 많이 부족하지만, 이 글이 첫 논문을 접하는 분들께 조금이나마 도움이 됐으면 합니다.🙏"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "",
    "text": "Prerequisite: 논문 요약 방식"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#st-read",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#st-read",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "1st read",
    "text": "1st read\n\nCategory\n\nResearch paper\n\n\n\nMain Topic\n\n제목\n\nCardiac dyspnea risk zones in the South of France identified by geo-pollution trends study - (Simões 기타 2022)\n\n\n\n주제\n\n프랑스 남부 지역의 Cardiac dyspnea(CD, 이하 심호흡곤란) 발생에 미치는 대기오염원(\\(\\rm{PM}_{10}\\), \\(\\rm{NO}_{2}\\), \\(\\rm{O}_{3}\\)) 영향 평가\n\n\n\n\nContext\n\n선행 연구들에서 대기오염원에 관한 단기 노출이 심근경색(myocardial infarction), 울혈성심부전(congestive heart failure)과 같은 몇몇 심혈관 병리(cardiovascular pathologies)들에 미치는 영향을 평가하긴 했으나, 심호흡곤란의 경우 이러한 관계를 아직 완전히 입증하지 못함\n따라서, 본 연구의 목적은 대기오염원, 기상요인, 심호흡곤란 입원 데이터를 활용해 심호흡곤란 입원 발생 원인에 관한 메커니즘을 알아보고, 이를 예방하기 위한 정책을 개발하는 것에 있음\n본 연구의 주요 방법론은 Distributed lag non linear model(이하, DLNM)과 메타분석(Meta analysis)에 해당함\n\n\n\nCorrectness\n\n기상요인(meteorological factors)들을 공변량(coviariates)으로 활용하는데, 다중공선성(multicollinearity)을 피하기 위해 상관이 존재할만한 두 변수 중 하나의 변수만 모형에 포함시킴\n최대 지연 효과(maximum lag days)는 14일까지 고려하였으나, 이에 관한 합리적 근거는 없다고 보여짐\n\n\n\nContributions\n\n프랑스 남부 전체 지역의 심호흡곤란 입원 발생에 관한 대기오염원의 영향을 평가한 첫 번째 연구\n\\(\\rm{NO}_2\\), \\(\\rm{O}_3\\), \\(\\rm{PM}_{10}\\)에 단기 노출이 심호흡곤란으로 인한 응급실 방문을 증가시킨다는 것에 관한 유의한 증거 제시\n본 논문의 접근 방식은 공중 보건 정책에 관한 예측 도구로서 대기오염원 모니터링을 효과적으로 제안함\n\n\n\nClarity\n\n지금까지 읽어본 바로는 명료하게 잘 쓰인 논문이라 생각됨"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#맺음말",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/paper-review-simes-et-al-2022.html#맺음말",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "맺음말",
    "text": "맺음말\n본 논문을 통해 실제 각 도시별 DLNM을 이용한 대기오염원 건강영향평가 수행 후, 메타분석으로 오버롤한 결과를 제시할 수 있음을 확인했습니다. 메타분석을 어떻게 진행하였는지에 관한 이론적 부분은 자세하게 기술되어 있지 않아서 두 번째 읽기는 진행하지 않았으나, 도시별 분석 결과를 메타분석을 통해 종합할 수 있다는 것을 확인하는 것으로는 첫 번째 읽기로도 충분했습니다.\n본 논문에 쓰인 메타분석은 일반적으로 임상연구에서 수행하는 메타분석을 다양한 상황에 쓸 수 있도록 일반화하여 확장시킨 형태의 메타분석 방법론이라고 보시면 됩니다. 해당 방법론을 깊이있게 이해하기 위해서는 (Sera 기타 2019)을 참고하시면 됩니다. 해당 논문의 예제 R 소스코드는 여기를 참고하시면 됩니다. 다양한 형태의 분석을 수행한 뒤에 library(mixmeta)를 통해 메타분석을 수행하여 결과를 종합하는 과정을 보여준다는 점에서 큰 의미가 있습니다. 그러나, 정작 제가 필요로하는 DLNM으로 건강영향평가를 도시별로 수행한 뒤에 메타분석을 하는 소스코드는 없다는 점이 조금 아쉬웠습니다.😂 그래서, 추가적으로 (Gasparrini, Armstrong, 와/과 Kenward 2012)에서 제공하는 R 예제 소스코드를 함께 참고했습니다. 확장된 형태의 메타분석인 (Sera 기타 2019)가 나오기 전이라 library(mvmeta)를 통해 분석이 진행되긴 합니다만, library(mixmeta)와 똑같은 로직으로 분석이 진행되기 때문에 해당 소스코드를 함께 참고하시면 도시별 DLNM 분석 결과를 메타분석하는 것을 어렵지 않게 구현하실 수 있을겁니다."
  },
  {
    "objectID": "posts/2022-06-08-monthly-memory-202204/memory-202204.html",
    "href": "posts/2022-06-08-monthly-memory-202204/memory-202204.html",
    "title": "월간 회고록: 2022년 4월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash"
  },
  {
    "objectID": "posts/2022-06-08-monthly-memory-202204/memory-202204.html#새로운-스터디를-시작하다",
    "href": "posts/2022-06-08-monthly-memory-202204/memory-202204.html#새로운-스터디를-시작하다",
    "title": "월간 회고록: 2022년 4월",
    "section": "새로운 스터디를 시작하다",
    "text": "새로운 스터디를 시작하다\n올해 3월부터 SQL 스터디, Python 코딩테스트, Tensorflow 스터디를 시작했습니다. 올 초부터 다양한 기업의 Data Scientist 채용 공고를 둘러봤고, 아무래도 이 세 가지는 꼭 필요로 된다고 느꼈습니다. “왜 이제 와서 시작하냐?” 하는 생각을 가지시는 분들이 많으실 것 같습니다. 작년에 대학원을 졸업했고 실무에서 1년차를 넘긴 지금에서야 말이죠. 지금부터 그 이야기를 풀어보려고 합니다. 사실, 지금 생각해보면 대학원 때 시작했어야할 것을 이제서야 시작한다는게.. 참 많이 늦은 감있습니다. 하지만, 늦었을 때가 가장 빠른?..뭐 이런 말로 위로를 삼아봅니다..\n사실 코딩테스트는 학부생 시절 대학원에 진학하기 전에 잠깐 취업 준비를 해보면서, 대학원을 졸업하고 취업 준비를 하면서 몇 번 치뤘던 적이 있습니다. R이 주 언어인 사람에게 다행스러웠던 것은 이때 치뤘던 코딩테스트들에서는 다행히 R을 지원해줬었다는 점이죠. 두시간 세시간 붙잡고 알고리즘 한두문제를 겨우 풀어서 제출했던 기억이 있습니다. 함수를 다 짜서 제출하면 뭐하나요, 뭣도 모르고 입력을 받아야하는 input()도 안해서 테스트케이스는 다 틀리는데 말이죠.😅 네, 당연히 항상 결과는 불합격이었습니다.\n\n그래서, 이제서야 시작한 이유는?..\n참 부끄럽지만 “내가 이걸 왜 준비해야하지?”라는 고집같은 생각을 했습니다. 내가 개발자도 아니고, Data science를 하고 싶은 사람인데 굳이 알고리즘 문제를 왜 잘 풀어내야하지? 왜 이런 것을 요구하는 걸까? 하는 생각을 했었죠. 지금 생각하면 참 바보같습니다. 아시다시피 요즘 나오는 여러분들이 이름만 대면 알만한 대기업, 빅 플랫폼 기업, 금융 기관의 Data Scientist 나 Data Analyst 채용 공고를 보시면 면접 전형 전에 꼭 코딩테스트가 포함되어 있습니다.1 극 소수의 대기업에서는 면접 전형 전 코딩테스트 대신 사전 과제 또는 Data Analyst의 경우 SQL 쿼리 테스트를 진행하는 경우도 있긴 합니다만, 코딩테스트가 포함된 형태의 채용 전형은 앞으로 기업들 사이에서 더더욱 확대될 것이라고 봅니다.\n과거에는 코딩테스트 공부는 거들떠보지 않았던 제가 지금에서야 공부를 시작한 이유는 “내가 이걸 왜 준비해야하지?”와 같이 어리석은 고집같은 생각을 버리고 그간 여러 생각을 해왔기 때문입니다. 먼저 “과연 내가 Data Science를 수행하기 위해 가고싶은 마음 속 업계 또는 기업만을 위해서 한 노력이 있는가?”에 대해 생각했고, 수많은 지원자를 평가해야만하는 기업과 실무자의 입장을 생각하기 시작하면서 제 관점은 많이 바뀌기 시작했습니다. 대기업, 우리가 이름만 대면 알만한 핫한 기업에는 수많은 지원자가 몰립니다. 그들의 입장에서 생각해보면, 다른 전형 없이 서류전형에서 각 지원자들의 서류를 세세하게 평가하여 바로 면접 전형을 진행하는 것은 결코 불가능합니다. 그래서, 코딩테스트와 같이 객관적인 평가 기준으로 지원자들을 한 번 걸러내는 작업이 필요로 되는 것이라 생각합니다. 공기업 채용 전형에서의 NCS, 사기업 채용 전형에서의 적성 평가2와 같은 것과 같은 맥락으로, 개발 직군에게는 코딩테스트라는 것이 존재하는 것이죠. 과거에는 이러한 형태의 채용 전형을 이해하고 싶지 않았습니다. 기업의 Culture fit과 얼마나 맞는지에 관한 인성 검사와 같은 것들은 꼭 필요로 된다고 생각했지만, NCS, 직무적성검사, 코딩테스트 같은 것들은 실질적인 직무 수행 능력과 직결이 되는 것도 아닌데, 왜 치뤄야 하는지에 대해 이해가 안됐었죠. 지금은 백 번 이해합니다. 오하려 과거에 되도 않는 고집을 피우며 코딩테스트 공부를 거들떠보지 않았던 저를 참 한심하게 생각하고있습니다.🤬\n아무튼 이러한 모티베이션에서 코딩테스트를 시작했고, SQL을 현업에서 다루고 있지만 SQL 쿼리테스트 스터디도 시작을 했습니다. 아울러, Tensorflow의 경우는 수많은 Data Scientist 채용공고를 둘러본 결과, torch나 tensorflow 등과 같은 머신러닝 프레임워크 하나 정도는 다룰 줄 알아야 될 것 같음을 느껴 시작하게 됐습니다. 여러 프레임워크 중 Tensorflow를 선택한 이유는, 현재 M1 GPU를 지원해주는 유일한 프레임워크이기 때문입니다. 그마저도 싱글코어긴 합니다..(사실 torch를 배워보고 싶었는데,,) 그리고, R의 {tidymodels}을 통해 머신러닝을 수행할 수 있긴 합니다만, 우리나라 업계의 Data Scientist 채용 공고에서 아직 R의 {tidymodels}를 기재해놓은 공고는 본 적이 없습니다. Python의 scikit-learn을 요구하는 경우는 종종 봤지만 말이죠. 참 씁쓸하네요..😭 개인적으로 {tidymodels}은 scikit-learn과 비교하기 미안할 정도로 더 좋은 패키지인데 말이죠. 아무튼, Tensorflow 스터디는 4월에 아카이브를 만들어 놓고, 업무와 다른 일을 핑계로 아직도 제대로 시작하지 않고 있네요.. 마침 Deep Learning with R, Second Edition이 곧 출판을 앞두고 있다는 소식을 들었는데, 이 책으로 스터디를 진행할까 합니다. 아니, 해야죠!\n\n🔗SQL 스터디\n🔗Python 코딩테스트 스터디\n🔗Tensorflow 스터디"
  },
  {
    "objectID": "posts/2022-06-08-monthly-memory-202204/memory-202204.html#이력서-포트폴리오-제작기",
    "href": "posts/2022-06-08-monthly-memory-202204/memory-202204.html#이력서-포트폴리오-제작기",
    "title": "월간 회고록: 2022년 4월",
    "section": "이력서, 포트폴리오 제작기",
    "text": "이력서, 포트폴리오 제작기\n기존에는 canva로 이력서와 경력기술서를 관리하고, 포트폴리오는 애플 키노트로 관리하고 있었는데 하나의 툴로 관리하고 싶었어요. R 마크다운과 노션 중에 고민하다가 노션으로 택했습니다. R Markdown에 비해 웹 공유도 편하고, PDF 변환, 그리고 무엇보다 디자인적인 요소가 훨씬 낫다고 생각했습니다. 그리고, R Markdown으로 관리했을 때 얻을 수 있는 베네핏도 딱히 없다고 생각했고, 이력서뿐만이 아니라 포트폴리오까지 함께 관리하기엔 노션이 확실히 편합니다. 이번에 노션으로 이력서와 포트폴리오를 다시 쭉 작성하며 참고해봤던 자료들입니다:\n\n🔗개발자 이력서 작성하기\n🔗eo - 최고의 직장에서 깨달은 내 몸값을 높이는 스킬 | 커리어 액셀러레이터 김나이\n🔗Data Scientist 김단아님 노션 Resume\n\n참고할만한 노션 Resume의 99%는 개발자 이력서이고 나머지는 통계학과 외에 다른 백그라운드로 Data Science를 하시는 분들의 이력서 뿐인데, 김단아님은 저와 같은 통계학 백그라운드로 Data Science를 하시는 분이라 참 많은 도움이 됐습니다. 이력서, 포트폴리오를 만들고 다듬는데에 대략 4일정도 걸린 것 같습니다. 이미 작성된 이력서, 경력기술서, 포트폴리오가 있었음에도 불구하고, 지겹고 힘들더군요.😪 참고했던 글, 영상 들에서 공통적으로 주장하는 이력서와 포트폴리오의 주요 포인트는 다음과 같습니다:\n\n내가 “어떤 것을 했다.”와 같이 팩트만 펼처 놓는 것이 아닌, 나의 강점을 펼치고 상대방을 설득할 수 있도록 기술하자\n이력서는 영화 예고편과 같다. 짧고 간결하게 꼭 보여주고 싶은 것들만 컴팩트하게 담자\n경험과 직무를 연결하자\n가능하다면 숫자로 성과를 드러내라\n\n숫자로 표현할 수 없다면, 그 일을 왜 했는지, 타겟이 누구였는지 디테일하게 담아보자\n\n개발 직군의 경우 다룰줄 아는 Tool의 수준을 나타내는 것은 지양하자\n\nTool의 수준에는 주관이 개입하기 마련이고, 객관적인 기준이 없기 때문\n개인적으로 주 언어정도를 표기하는 것은 나쁘지않다고 봄\n나머지 본인이 다루는 각 Tool의 수준은 포트폴리오에서 자연스럽게 드러나야함\n\n경력 기술, 포트폴리오 작성 시 Data Privacy, Research Privacy, 업무 상 비밀은 꼭 지켜야 함\n\n이를 지키지 않으면 이력서를 평가하는 사람 입장에서도 큰 (-)가 될 수 있음\nPrivacy를 지키기 위해 마스킹이 필요한 부분은 꼭 마스킹하여 기술하자\n\n\n버려야 하는 내용은 과감하게 버려야하는데, 이게 참 어려웠던 것 같습니다. Privacy를 지키는 일도 매우 중요한데, 꽤 귀찮았고요.😅\n이직을 계획하고 계신 분들이 아니여도 이력서, 포트폴리오를 틈틈히 정리해두는 습관은 꼭 필요합니다. 이력서와 포트폴리오가 꼭 필요한 상황에 닥쳐서 한꺼번에 지금까지 해온 것들을 정리하는 작업은 정말 힘든 일입니다. 정말 많은 시간이 소요될 것이고, 사람의 기억력에는 한계가 있기 때문에 틈틈히 주기적으로 이력서와 포트폴리오를 관리해온 사람에 비해 좋은 퀄리티를 갖기도 힘들 것입니다. 더군다나, 요새는 “평생직장”이 아닌 “평생직업”을 바라보고 살아가야하는 세상이기에 본인 PR을 할 줄 알아야합니다. 과장 좀 보태서 이야기 해보면, 본인이 한 것은 100인데 50으로 밖에 포장을 못하는 사람이 있는 반면, 본인이 한 것은 70인데 100만큼 포장할 줄 아는 사람이 있습니다. 본인이 어디쯤 위치하는 사람인지 곰곰이 생각해보시기 바랍니다. 그래서, 커리어를 쌓아가는 데에 있어서 본인이 이루어 낸 것들을 주기적으로 잘 정리하고 포장하는 것은 기본 중의 기본이라 생각합니다. 이직 계획과는 무관하게 적어도 분기에 1번 정도는 이력서와 포트폴리오의 유지보수에 시간을 투자하는 것을 적극 권장합니다. 나라는 상품을 취업 시장에 내놓는데, 다른 상품들과의 차별점을 꾀하기 위해 이정도 노력은 꼭 필요하지 않겠습니까? 이런 노력 없이도 남들보다 훨씬 더 뛰어난 무언가를 갖고 있는 인재가 아닌 이상 말이죠.\n마지막으로 4일 간의 끈질긴 작업 끝에 완성한 제 이력서와 포트폴리오 링크를 첨부하면서 회고를 마칩니다. 앞서 말씀드렸던 사항들을 최대한 지키려고 노력했지만, 잘 지켜졌는지.. 틈틈히 들여다 보고 유지보수 해나가려고 합니다.\n\n🔗방태모의 이력서"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html",
    "title": "ggplot2 컬러링 가이드",
    "section": "",
    "text": "Photo by David Pisnoy on Unsplash\n오늘은 ggplot2에서 더 적은 수의 컬러로 더 직관적인 시각화를 가능하게끔 해주는 4가지 방식에 대해 소개해보려고 합니다. ggplot2를 바탕으로 진행되는 예제이긴 하나, 본 글에서 소개할 방식들에 담겨있는 아이디어는 언어, 시각화 라이브러리를 막론하고 적용이 가능할거라고 봅니다.물론, ggplot2만큼 짧고 가독성 좋은 코드로 구현이 가능할지는 미지수이지만요.😁 본 글에서 ggplot2의 그래프 문법 1이 갖는 강력한 힘을 확인하실 수 있을 겁니다.\n대중들은 말이 아닌 그림을 기억합니다. 그래서, 잘 만들어진 데이터 시각화는 강력한 힘을 갖습니다. 종종 데이터 시각화를 하시다가 지나치게 많은 색을 사용하게 되어 오히려 전달력이 떨어진다는 느낌을 받은 적이 있지 않으신가요? 그렇다면 이 글이 도움이 되실 수도 있겠습니다.😀"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#준비하기",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#준비하기",
    "title": "ggplot2 컬러링 가이드",
    "section": "준비하기",
    "text": "준비하기\n본격적인 시작 전 몇 가지 준비를 하고자 합니다. ggplot2에서 제공하는 다양한 테마 중 theme_minimal()을 사용할 예정이고, 폰트, 그림 제목과 색 등에 몇 가지 조정을 줄 예정입니다:\n\nshowtext 패키지를 통해 Fira Sans font 설정\n그림 제목은 기본 좌측 정렬, 색맹(color-blind)까지 고려한 Okabe Ito 컬러 팔레트 사용\n\n본 글의 작성에 참고한 원 글의 저자는 Fundamentals of Data Visualization by Claus Wilke를 읽은 뒤, Okabe Ito 컬러 팔레트를 선호하게 되었다고 합니다.\n\n\n\nlibrary(tidyverse)\nlibrary(showtext)\nfont_add_google(\"Fira Sans\", \"firasans\")\nshowtext_auto()\n\ntheme_customs <- theme(\n  text = element_text(family = 'firasans', size = 16),\n  plot.title.position = 'plot',\n  plot.title = element_text(\n    face = 'bold', \n    colour = thematic::okabe_ito(8)[6],\n    margin = margin(t = 2, r = 0, b = 7, l = 0, unit = \"mm\")\n  )\n)\n\ntheme_set(theme_minimal() + theme_customs)"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#음영을-활용하자",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#음영을-활용하자",
    "title": "ggplot2 컬러링 가이드",
    "section": "음영을 활용하자",
    "text": "음영을 활용하자\nggplot2 패키지에서 제공하는 mpg 데이터셋을 이용해 연도별 자동차 종류의 빈도를 시각화해봅시다. 전에 충분히 보셨을만한 데이터라 생각해서, 데이터셋에 관한 설명은 스킵하겠습니다. 이런 방식으로 시각화를 해보신 경험이 있으실겁니다:\n\nmpg |> \n  ggplot(aes(x = factor(year), fill = class)) +\n  geom_bar() +\n  labs(x = \"year\")\n\n\n\n\n\n\n\n\n자동차 종류가 많다보니 무려 7개의 컬러를 시각화에 사용하였습니다. 이 그림이 틀렸다고 할 수는 없습니다. 다만, 좋은 시각화라고 할 수 있는지에 대해 한 번 생각해보자는 겁니다. 제가 보기에 이렇게나 많은 수의 컬러를 사용하는 시각화는 꽤나 정신없어 보인다고 느껴집니다.여러 수준을 갖는 범주형 변수에 관한 컬러링에 있어서 더 적은, 최대 3개 정도의 컬러만 사용하여 시각화를 수행하는 방법은 없을까요? 이제 그 아이디어를 소개하고자 합니다. 자동차 종류를 구분하기 위해 색조(hues) 뿐만이 아닌, 음영(shades)을 활용하는 것이죠. 3가지 컬러만을 사용해 투명도를 줌으로써 7개의 자동차 종류를 구분해보겠습니다. 미리 말씀드리자면, minivan을 단독 하나의 그룹으로 설정해주어 이를 중심으로 투명도가 줄어들고 늘어나게끔 만드는 것이 키 아이디어입니다.\n이를 위해서는 우선 데이터에 자동차의 종류를 3가지로 구분짓는 새로운 그룹 변수를 생성해주어야 합니다:\n\n# Group classes into three groups (to reduce colors to 3)\ndat <- mpg |> \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  )\n\n이를 바탕으로 우선 먼저 시각화를 해보죠:\n\nshades_plt <- dat |> \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar() +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Show shades, not hues'\n  )\nshades_plt\n\n\n\n\n\n\n\n\n색조와 음영까지 활용해 3가지 컬러정도로 줄이긴 했지만, 아직 전달력은 매우 떨어집니다. 우리 눈으로 색조와 음영을 조합해 그림의 자동차 종류를 구분해내는 것은 꽤 어렵죠. 투명도와 색상을 직접 조정해보겠습니다. 투명도는 suv -> minivan까지 점차 줄어들고, minvan 이후부터는 다시 줄어든 양만큼 투명도가 늘어나도록 설정을 해주려고 합니다:\n\n# Color-blind safe colors\ncolors <-  thematic::okabe_ito(3)\n# Possible levels of transparency (one for each class)\nalpha_max <- 1\nalpha_min <- 0.7\nalpha_vals <- c(\n  seq(alpha_max, alpha_min, length.out = 4), \n  seq(alpha_min, alpha_max, length.out = 4)[-1]\n)\nalpha_vals\n\n[1] 1.0 0.9 0.8 0.7 0.8 0.9 1.0\n\n\n\n# Tweak previous plot\nshades_plt <- shades_plt +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals)\nshades_plt\n\n\n\n\n\n\n\n\n아까보다는 좀 낫습니다. 여기서 좀 더 개선을 해보자구요. 우측 범례를 하나로 좀 통합해서 설정하면 좋을 것 같은데요. 7개로 구분되어 있는 투명도에 컬러를 입혀줘서 말이죠. 꽤나 어려운 작업일 것 같지만, ggplot2에서는 아주 손쉬운 작업입니다. guides() 함수를 통해 가능합니다. fill에 관한 범례(class_group)는 삭제를 한 뒤에, guide_legend()를 통해 alpha에 관한 범례에 fill의 색상을 가져와 우리가 사전에 설정한 각각 3개 그룹(class_group)의 컬러를 덮어씌워(override) 줄겁니다. 말이 조금 복잡해보이지만, 코드를 보면 더 쉽게 이해하실 수 있습니다:\n\nshades_plt <- shades_plt +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(\n      override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]\n      )\n    )\n  ) \nshades_plt"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#음영만으로는-부족해",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#음영만으로는-부족해",
    "title": "ggplot2 컬러링 가이드",
    "section": "음영만으로는 부족해",
    "text": "음영만으로는 부족해\n충분히 괜찮은 시각화를 했지만, 아쉬운 부분이 하나 있습니다. 인접한 컬러 블록들에서는 자동차 종류 구분이 쪼~금 불편해 보입니다. 이 문제 또한 손쉽게 해결해줄 수 있어요. 블록마다 선 하나씩만 그어주면 말이죠. 앞선 시각화 코드에 geom_bar() 한 줄이면 해결할 수 있습니다. 이 또한 그래프 문법의 힘이죠.😄\n\ndat |> \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar(col = 'white') + # Add lines for distinction\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]))\n  ) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Group categories together by color, \\nbut keep showing them'\n  )"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#전달하고-싶은-내용만-강조하자",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#전달하고-싶은-내용만-강조하자",
    "title": "ggplot2 컬러링 가이드",
    "section": "전달하고 싶은 내용만 강조하자",
    "text": "전달하고 싶은 내용만 강조하자\n이제 조금 다른 이야기를 해보려고 합니다. 꼭 위와 같이 모든 범주에 대해 컬러를 줄 필요는 없는 상황도 있지 않을까요? 예를 들자면, 우리가 시각화를 통해 꼭 강조해서 전달하고 싶은 내용이 있을 때처럼요. 이번에 사용할 예시 데이터는 흥미로운 다양한 데이터셋을 제공해주는 Our World in Data에서 가져왔습니다. 미국인들을 대상으로 설문조사를 수행한 자료인데요. 본 자료를 통해 우리가 알아보고자 하는 바는 “우리는 과연 일생동안 누구와 시간을 많이 보내는가?”입니다. 여러 나이대의 미국인들을 대상으로 하루에 평균적으로 누구와 얼마나 시간을 보내는지에 대해 조사한 자료라고 할 수 있겠습니다. 위 링크의 차트에 아래 우측 탭을 보시면 Download를 눌러서 데이터를 받으실 수 있습니다:\n이 자료를 바탕으로 다음과 같은 그림을 그려볼 수 있습니다. 과연 나이에 따라 우리가 시간을 함께 보내는 대상은 어떤 식의 패턴을 보이며 변화할까요? 일반적으로는 다음과 같이 시각화를\n\n# Some data wrangling\ntime_data <- read_csv(\"./time-spent-with-relationships-by-age-us.csv\") |> \n  rename_with(\n    ~c('Entitity', 'Code', 'Age', 'alone', 'friends', 'children', 'parents', \n       'partner', 'coworkers')\n  ) |> \n  pivot_longer(\n    cols = alone:coworkers, \n    names_to = 'person',\n    values_to = 'minutes'\n  ) |> \n  janitor::clean_names() |> \n  filter(age <= 80)\n\nRows: 67 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (7): Year, Time spent alone, by age of respondent (United States), Time ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Color-blind safe colors\ncolors <- thematic::okabe_ito(7)[-6]\n\n# Line plot\np <- time_data |> \n  ggplot(aes(x = age, y = minutes, col = person)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = colors) +\n  coord_cartesian(xlim = c(15, 81), expand = F) +\n  scale_y_continuous(minor_breaks = NULL) +\n  labs(x = 'Age (in years)', y = 'Minutes', col = 'Time spent')\np\n\n\n\n\n\n\n\n\n보통 이렇게들 시각화하곤 하죠. 이런 종류의 그림은 스파게티 플롯(spaghetti plot)이라고 표현하기도 합니다. 우리는 또 수많은 컬러에 직면했습니다. 아울러, 이 그림 한 장만 놓고 봤을때는 무슨 말을 전달하고자 하는지 파악하기가 참 힘듭니다. 제 눈엔 우선 2가지 인사이트가 보입니다:\n\n우리는 일생동안 혼자서 가장 많은 시간을 보내게 된다.\n40대 즈음해서 아이와 보내는 시간은 줄어들면서, 혼자 보내는 시간이 많아진다.\n\n이와 같이 만약 우리가 전달하고자 하는 인사이트가 확실한 상태라면, 중요한 부분만 강조함으로써 이 지저분한 스파게티 플롯을 전달력 있는 깔끔한 스파게티 플롯으로 만들어 줄 수 있습니다. ggplothighlight 패키지가 그 해결책이 되어줍니다. 패키지 안의 ggplothighlight() 함수를 이용해 레이어를 하나더 얹어서, 필터링을 해줄 수 있어요. 아주 편리한 패키지죠. 특정 조건을 만족하지 않는 데이터 포인트는 모조리 회색으로 표현이 됩니다. 먼저 첫 번째 인사이트를 그림으로 표현해봅시다. 코드 1줄 정도만 추가해주면 가능합니다.\n\nlibrary(gghighlight)\nalone_plt <- p + \n  gghighlight(person == 'alone', use_direct_label = F) +\n  labs(title = 'Emphasize just one or a few categories')\nalone_plt\n\n\n\n\n\n\n\n\n이 그림에 텍스트를 추가하여 우리가 하고싶은 이야기를 좀 더 강조할 수도 있습니다:\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  )\n\n\n\n\n\n\n\n\n하고 싶은 이야기가 하나가 아니라 여러개라면 어떤 방법이 있을까요? 전혀 문제가 되지 않습니다.😀 gghighlight()를 사용해 그저 여러 조건 넣어주기만 하면 됩니다.\n\nage_40_plt <- p + \n  gghighlight(\n    person %in% c('alone', 'children'), \n    age >= 38, \n    use_direct_label = F\n  ) +\n  geom_segment(x = 38, xend = 38, y = -Inf, yend = 300, linetype = 2, col = 'grey20') +\n  labs(title = 'Emphasize just one or a few categories') \n\nage_40_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 403,\n    label = 'Around the age of 40, we spend \\nless time with children and \\nmore time alone.',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 0.85,\n    size = 5.5\n  )"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#라벨링-활용하기",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#라벨링-활용하기",
    "title": "ggplot2 컬러링 가이드",
    "section": "라벨링 활용하기",
    "text": "라벨링 활용하기\n앞서 본 모든 그림들에서는 그림의 이해를 돕기위한 범례(legend)가 우측에 자리하고 있었습니다. 범례는 그림에서 꽤나 큰 공간을 차지합니다. 아울러, 범례는 그림에 집중도를 떨어뜨릴 수 있죠. 그림의 이해를 위해서는 필연적으로 범례와 그림을 번갈아가며 봐야하니까요. 이 문제를 해결할 방법은 없을까요? 범례를 없애고 그림에 라벨링을 통해 우리가 하고자 하는 이야기를 전달하면 어떨까요? 싱글 레이블에 대해서는 annotate(), 다중 레이블에 대해서는 geom_text()를 이용해 라벨링을 하면 되는데요. 우선 annotate()를 활용해 스파게티 플롯 예제를 개선시켜 보겠습니다:\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  ) +\n  annotate(\n    'text', \n    x = 70, \n    y = 420, \n    label = 'alone',\n    hjust = 0,\n    vjust = 0,\n    size = 7,\n    family = 'firasans',\n    color = colors[1]\n  ) +\n  labs(title = 'Label directly') +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n이러한 방식으로 공간도 세이브하고, 그림에 집중도도 훨씬 높혀줄 수 있죠. 범례와 플롯을 눈으로 왔다갔다 할 필요도 없고요. 여기서 조금 더 개선을 해볼까요? 현재 위 그림에는 alone이라는 단어가 중복으로 들어가있죠. 강조한 선 아래 alone을 없애고 선과 동일한 색상을 좌측 문장의 alone에 넣어주는 것은 어떨까요? 더 매력적인 시각화가 될 것만 같다는 생각이 들지 않나요?\n이를 위해서는 ggtext 패키지를 활용해 HTML 문법을 이용해야합니다. annotation()의 text geom을 richtext geom으로 바꾸고, 우리가 컬러를 반영하고자 하는 텍스트에 대해서는 HTML 코드를 포함하는 문자열을 만들어 주는 과정이 필요합니다. 말이 좀 복잡해보이지만, 코드는 꽤 간단합니다:\n\nlibrary(ggtext)\ncolor_alone <- glue::glue(\n  \"We spend a lot of time <span style = 'color:{colors[1]};'>alone</span>...\"\n)\ncolor_alone\n\nWe spend a lot of time <span style = 'color:#E69F00;'>alone</span>...\n\n\n\nalone_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 400,\n    label = color_alone,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 6,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n멋지지 않습니까? HTML 문법에 익숙하지 않은 분들은 컬러링을 넣어주는 형태를 기억하시기 바랍니다. 이런 식으로 직접적으로 라벨링 하는 방식은 스파게티 플롯 예제의 두 번째 인사이트를 나타내는 그림에 대해서도 손쉽게 적용이 가능합니다.\n\nage_40_text <- glue::glue(\n  \"Around the age of 40, we spent <br> less time with \n  <span style = 'color:{colors[2]};'>children</span> \n  and <br> more time <span style = 'color:{colors[1]};'>alone</span>.\"\n)\n\nage_40_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 350,\n    label = age_40_text,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 1.25,\n    size = 5,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n이런 완성도 있는 시각화는 청중 또는 대중들에게 우리가 전달하고자 하는 이야기를 직관적으로 전달해줍니다. 범례와 플롯을 눈으로 왔다갔다 하는 피로를 덜어주는 것은 덤이고요.\n마지막으로, 우리가 초기에 했던 예제인 막대그래프(bar chart) 예제에도 이를 적용해봅시다. 해당 예제의 경우 다중 레이블에 해당하므로 annotate()이 아닌 geom_text()가 필요로 됩니다. 그런데, 어떤 이유에서인지.. 자동차 종류의 각 레이블 위치가 정렬이 안되는 문제가 있었습니다. 불가피하게 연도별, 자동차 종류별 막대그래프의 높이를 누적빈도(csum)로 계산하고, 레이블이 들어갈 위치(n)를 적당하게 잡아주는 작업을 수행했습니다.🤯 코드가 조금 복잡해 보이긴 하나, 출력된 결과를 확인하시면 어떤 작업을 진행했는지 쉽게 이해하실 수 있을겁니다.\n\nmanual_counts <- mpg |> \n  count(year, class) |> \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  ) \nmanual_counts\n\n# A tibble: 14 × 4\n   year  class          n class_group\n   <fct> <chr>      <int> <chr>      \n 1 1999  2seater        2 grp1       \n 2 1999  compact       25 grp1       \n 3 1999  midsize       20 grp1       \n 4 1999  minivan        6 grp2       \n 5 1999  pickup        16 grp3       \n 6 1999  subcompact    19 grp3       \n 7 1999  suv           29 grp3       \n 8 2008  2seater        3 grp1       \n 9 2008  compact       22 grp1       \n10 2008  midsize       21 grp1       \n11 2008  minivan        5 grp2       \n12 2008  pickup        17 grp3       \n13 2008  subcompact    16 grp3       \n14 2008  suv           33 grp3       \n\nlabels <- manual_counts |> \n  mutate(class = factor(class)) |>  \n  group_by(year) |> \n  arrange(year, desc(class)) |> \n  mutate(\n    csum = cumsum(n), \n    n = (lag(csum, default = 0) + csum) / 2\n  )\nlabels\n\n# A tibble: 14 × 5\n# Groups:   year [2]\n   year  class          n class_group  csum\n   <fct> <fct>      <dbl> <chr>       <int>\n 1 1999  suv         14.5 grp3           29\n 2 1999  subcompact  38.5 grp3           48\n 3 1999  pickup      56   grp3           64\n 4 1999  minivan     67   grp2           70\n 5 1999  midsize     80   grp1           90\n 6 1999  compact    102.  grp1          115\n 7 1999  2seater    116   grp1          117\n 8 2008  suv         16.5 grp3           33\n 9 2008  subcompact  41   grp3           49\n10 2008  pickup      57.5 grp3           66\n11 2008  minivan     68.5 grp2           71\n12 2008  midsize     81.5 grp1           92\n13 2008  compact    103   grp1          114\n14 2008  2seater    116.  grp1          117\n\n\n레이블이 들어갈 자리를 계산하는 키 아이디어는 lag()를 통해서 계산한 누적빈도를 0값을 시작으로해서 한칸씩 당겨주고, 계산해둔 누적 빈도(csum)를 더하여 2로 나눠주는 것입니다. 이 작업을 수행하면 막대그래프를 구성하는 각 칸의 중간 높이를 계산할 수 있는 것이죠.\n아울러, 우리가 본 시각화에서 한 가지 더 극복해야할 난관은 바로 자동차의 종류 중 2seater의 빈도가 매우 작아서 레이블이 들어갈 자리가 없는 점이 었습니다. 그래서, 2seater의 경우 레이블을 막대의 맨 위에 표시되도록 하였습니다. 이러한 모든 난관들을 극복하고 완성한 그림을 공개합니다.\n\nmanual_counts |> \n  ggplot(aes(x = year, y = n, fill = class_group)) +\n  geom_col(aes(alpha = class), col = 'white') +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Label directly'\n  ) +\n  # Add all but one label\n  geom_text(\n    data = labels |> filter(class != '2seater'),\n    aes(label = class), \n    col = 'white',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  # Add 2seater label\n  geom_text(\n    data = labels |> filter(class == '2seater'),\n    aes(y = n + 3, label = class), \n    col = 'black',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#맺음-말",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/coloring-guide-for-ggplot2.html#맺음-말",
    "title": "ggplot2 컬러링 가이드",
    "section": "맺음 말",
    "text": "맺음 말\n분석을 시작하며 혼자 가볍게 EDA를 하는 단계에서 본 예제와 같이 시각화를 개선해나가는 작업은 필요로 되지 않을겁니다. 오히려 시간 낭비일수도 있구요. 그러나, 내가 얻은 인사이트를 전달하는 자리 또는 데이터를 기반으로 누군가를 설득해야하는 자리에서는 이 글에서 제공하는 몇 가지 방법이 꽤나 도움이 될 것이라고 생각합니다. 물론, 전달하고자 하는 내용이 한 눈에 들어도록 시각화를 수행하는 작업은 의외로 쉬울 때도 있지만, 꽤나 까다로운 과정을 거쳐야하는 상황도 존재합니다. 실무에서는 이와는 또다른 예상치 못한 까다로운 문제들을 겪는 상황들이 있을 수도 있구요. 다만, 본 글에서 그림의 퀄리티를 단계단계 개선해나간 바와 같이 전달하고자 하는 내용을 명확히하고 충분한 시간을 숙고해 그림을 개선해 나간다면, 뭐든 해결할 수 있을 것이라고 봅니다. 하고자 하는 시각화를 구현하지 못해낸다고 하더라도 그 과정 속에서 배우는 것은 분명히 존재할 것입니다. 처음부터 완벽하게 아름다운 시각화를 해낼 수 있는 사람은 없다는 것을 기억하셨으면 합니다.😁\n이번 포스팅을 준비하며 참고했던 글은 올해 봤던 데이터 시각화 관련 아티클 중 제게 가장 큰 임팩트를 주는 글이었습니다. 누구나 하는 평범한 시각화를 비범하게 만들어주는 글이라고 표현하면 적절할까요? 많은 사람들이 알았으면 하는 내용이라, 8월 서울 R 미트업에서 본 내용을 주제로 발표를 하기도 했습니다. 지금 이 글을 읽고 계신 여러분들에게도 좋은 인사이트를 줄 수 있는 글이 되었으면 합니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html",
    "href": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "",
    "text": "Photo by Christian Erfurt on Unsplash\n유튜브 채널 슬기로운 통계생활에서 운영하는 블로그에 기고했던 칼럼들을 최신화하여 다시 적어보려고 합니다. 첫 번째 칼럼 주제는 대학원에 대한 고민입니다. 저는 늘 고민과 생각이 많은 사람인데요.😂 때는 제가 통계학과 학부 4학년이던 2018년으로 거슬러 올라갑니다. 4학년 1학기 때는 학내 교환학생 프로그램에 신청하여 한 학기를 영국의 쉐필드대학(The University of Sheffield)에서 보내게 됩니다.\n아쉽게도 이 곳에서 통계학 전공 과목을 들을 기회는 없었습니다. 영어와 여러 가지 교양 과목을 수강했고, 시간이 많았던 때라 실컷 놀면서 자연스레 진로에 대한 고민을 다시 한 번 깊게 해보게 되었습니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#진로에-대한-고민",
    "href": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#진로에-대한-고민",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "진로에 대한 고민",
    "text": "진로에 대한 고민\n당시 저는 통계학 전공을 살려 Data Scientist라 표현되는 직업을 갖고 싶었습니다. 그래서, 다음과 같은 두 가지 선택지에서 고민하기 시작했습니다.\n\n취업 준비\n통계학 대학원 진학\n\n어중이떠중이 기질이 있었던 저는 깊은 고민 끝에 2년이라는 시간을 통계학 대학원에 투자할 용기가 없어, 4학년 1학기를 쉐필드에서 마치고 한국으로 돌아가 취업 준비를 해보기로 결심했습니다. 학부 졸업 요건과 취업에 필요한 기본 요건1은 준비가 되어있었고, 무엇보다 마음 속에 지금 상태로 취업 준비를 해봐도 되겠다는 알 수 없는 자신감이 있었습니다. 그 이유는 지금 생각해보면 정말 별것 아닌 것들 때문이었죠. 기본적인 소양에 불과한 평균 평점(3.93/4.5)과 전공 평균 평점(4.1/4.5), 그리고 지금 다시 돌아보면 정말 형편없었던 R 숙련도에 대한 자부심은 제게 “이정도면 취업 준비를 해봐도 되지 않을까?” 하는 근거없는 자신감을 갖게 했죠. 이렇게 취업 준비를 결심하고 채용 공고를 들여다보면서 시간을 보내는 와중에, 계속해서 눈에 밟히던 두 가지 키워드가 있었습니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#호기심을-불러일으킨-두-가지-키워드",
    "href": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#호기심을-불러일으킨-두-가지-키워드",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "호기심을 불러일으킨 두 가지 키워드",
    "text": "호기심을 불러일으킨 두 가지 키워드\n2016년 구글 딥마인드 팀이 개발한 바둑 AI 알파고가 이세돌과의 바둑 대국에서 압도적으로 승리를 거두며, 수십 년에 걸쳐 발전해온 딥러닝이라는 기술은\u001c 마치 최근 개발된 혁신적인 신기술인냥 세상의 주목을 받게 되었습니다. 매스컴의 주목에 따라 뉴스에서 종종 등장하던 두 단어 “머신러닝”과 “딥러닝”은 제게 또다른 호기심을 심어주었습니다.\n\n\n\n알파고와 대결한 이세돌 9단\n\n\n당시 학부 전공 과목으로 데이터마이닝을 수강한 상태였던터라 이러한 호기심 매우 자연스러운 현상이었던 것 같습니다. 그러나, 당시 통계학 학부 4학년에 불과하던 제게 머신러닝, 딥러닝과 같은 키워드는 머릿속에 큰 그림은 커녕 기존에 배웠던 전공 과목2들과 자연스러운 비교를 하면서 혼란을 가중시킬 뿐이였죠. 내가 배웠던 것들과 두 키워드는 어떤 관련이 있는지 알고 싶었고, 심지어는 “전자와 후자 중 어떤 것이 더 나은 방법론인가?” 와 같이 지금 생각해보면 참 바보 같은 생각을 했었습니다. 이러한 생각들은 Data Scientist의 꿈이 있었던 사람에게 왠지 모를 두려움과 불안감을 심어주었습니다. 그래서, 이것 저것 찾아보며 두 기술에 대해 이해해보려고 노력했습니다. 이 과정에서 문득 “과연 내가 이 상태로 실무에 나가서 호기심이 있는 기술, 또는 직무에 꼭 필요로 되는 기술이 있을 때 이러한 기술들을 독학하여 실무에 적용할 수 있을까?” 하는 생각을 했습니다. 4학년 2학기 졸업예정자 신분으로서 Data Scientist 직무로의 취업을 성공한다고 한들, 직무를 잘 수행해내며 스스로 발전할 수 있을지에 대한 의구심이 생겼죠. 그래서, 마음속에서는 대학원 진학에 대한 열망이 다시 한 번 피어오르고 있었습니다.\n대학원 진학이라는 길이 머릿속을 떠나지 않았습니다. 그래서, 4학년 2학기 딱 한 학기만 학부 졸업 예정자로서 취업 준비를 하며 제가 다니던 본교의 통계학 대학원 진학 준비를 병행해서 해보기로 했습니다. 당연히 취업 준비 결과는 참담했습니다:\n\n\n\n(학부 4학년 2학기) 2018년 하반기 채용 지원 결과\n\n\n그때 제 수준을 지금 생각해보면 이러한 결과는 당연했다는 생각이 드네요.😂"
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#참담한-취업-실패에서-배운-것들",
    "href": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#참담한-취업-실패에서-배운-것들",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "참담한 취업 실패에서 배운 것들",
    "text": "참담한 취업 실패에서 배운 것들\n취업 준비 결과 실질적으로 손에 쥔 것은 없었지만, 수많은 채용공고를 보고 자기소개서를 쓰며 얻은 것과 배운 것들은 많았습니다:\n\n1 2018년까지 내가 해온 활동에 대한 정리\n대학원 졸업을 앞두고 내가 해온 활동에 대한 정리를 시작했다면, 졸업 논문 작업과 겹쳐 취업 준비에 매우 어려움을 겪었을거라 생각합니다.\n\n\n2 자기소개서를 쓰는 방식\n당시 썼던 자기소개서들을 올해 이직 준비를하며 썼던 자기소개서들과 비교해보면, 과거의 제가 썼던 자기소개서는 정말 형편없었습니다. 그러나, 첫 자기소개서를 대학원을 졸업하던 시기에 쓰기 시작했다면 그야말로 아찔하네요.\n\n\n3 우리나라 기업에서 Data Scientist/Analyst 채용시 원하는 구체적인 역량\n당시 완벽하게 깨우치지는 못했지만 수많은 채용공고를 들여다보니 준비해야할 방향이 조금이나마 보였던 것 같습니다. 취업이나 이직을 준비하시는 분들이 아니더라도 업계의 인재 영입 동향 파악을 위해 틈틈히 채용공고를 들여다보시는 것을 추천드립니다. 이번에 이직 준비를 하서면서도 우리나라 기업에서 낸 수많은 Data Scientist/Analyst 채용 공고를 들여다보았는데, 우리나라의 분석 직군들의 직무들도 점차 세분화 되어 잘 정립되어 가고 있다는 느낌을 받을 수 있었습니다. 물론, 여전히 채용 공고를 아무리 읽어 봐도 무슨 일을 하게 될 지 알 수 없는 그런 공고들도 종종 보였으나, 이건 어느 직무에서든 종종 보이는 성의없게 쓰여진 채용공고이므로 별 의미를 두지 않았습니다. 세분화되어 잘 정립되어 가고 있는 우리나라 분석 직군의 세부 직무들을 자세하게 알아보고 싶은 분들께는 변성윤님이 올려주신 🔗유튜브 영상을 추천드립니다.\n\n\n4 석사학위에 대한 필요성\n제 머릿 속에 대학원 진학이라는 키워드가 계속해서 맴돌았기 때문일지도 모르겠습니다. 2018년 당시 석박사 채용을 통해서만 Data Scientist 직무를 뽑는 경우도 종종있어 지원조차 못하는 기업들이 있었고3, 4년제 대졸 신입사원 채용으로 뽑더라도 우대사항에는 늘 석사학위 보유자 키워드가 함께 자리하고 있었습니다. Data Scientist/Analyst 채용 시 통계학 학사와 석사가 경쟁하면 기업 입장에서는 기본적으로 어떤 지원자가 더 매력적이겠습니까? 학위를 뛰어넘을만한 좋은 경력이나 포트폴리오를 갖고 있지 않는 이상 석사 학위 보유자를 선호할 것입니다. 단, 학위 자체가 어떤 특정한 어드벤티지를 준다고는 생각하지 않았습니다. 그만큼 석사 학위 보유자라는 책임감을 가져야하만하고 기업의 기대에 맞는 수준을 갖는 사람이 되어야만 한다고 생각했죠. 말 그대로 학위는 우리를 둘러싼 껍질에 불과한 기본 아이템이라고 표현하면 적절할까요? 그러나, 당시 학사 학위와 빈약한 포트폴리오를 갖고있던 제게 석사 학위를 뛰어넘을만한 Data Scientist 직무로의 취업 준비 방법은 떠오르지 않았죠. 그래서, 대학원 진학을 결정한 것이고요.\n이렇게 졸업 예정자로서 취업 준비를 한 번 해봤던 경험은 제게 통계학 대학원 진학에 대한 필요성을 직접 피부로 느끼게 해주었습니다. 대학원에 진학하여 열심히 공부할 수 있었던 동기부여 또한 마음 속 깊히 채워넣을 수 있었습니다. 그 결정을 한 당시를 돌아보면 통계학 대학원 진학에 대한 후회는 전혀 느껴지지 않습니다. 제 인생에 정말 탁월한 결정이였죠. 오히려 취업 준비를 하지 않고 통계학 대학원 준비에 올인했다면 더 좋은 결과를 가져올 수 있었을까? 하는 무의미한 생각을 하곤 합니다.😂 당시의 저처럼 현재 통계학 대학원 진학에 대한 고민을 품고 있는 학부생들의 선택은 당연히 본인의 몫입니다. 다만, 열심히 공부함과 동시에 자신을 부지런히 브랜딩한다는 가정 하에, 통계학 대학원 진학은 Data Scientist/Analyst로의 취업에 무조건 플러스가 될 것이라고 말씀드리고 싶네요.😀 이 말에도 대학원 진학에 확신이 서질 않는다면, 자신의 수준을 한 번 냉정하게 바라보시고 실무에 나갈 준비가 되었는지 본인에게 질문을 던져보시기 바랍니다. 질문의 답이 “Yes”라면 취업 준비를 해보시는 것 또한 정말 좋은 경험이 되실겁니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#대학원에-들어가며-다짐했던-것",
    "href": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#대학원에-들어가며-다짐했던-것",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "대학원에 들어가며 다짐했던 것",
    "text": "대학원에 들어가며 다짐했던 것\n저는 당시 Data Scientist/Analyst 직무로의 취업을 꿈꿨지만, 어느 기업 또는 어느 업계로 가고 싶다는 구체화는 전혀 되어있지 않던 상태였습니다. 그래서, 수많은 분야에서 의사결정의 도구로 사용되고 있는 Data Science/Analytics의 특성상 어떤 식으로 커리어 방향을 잡아 나갈지, 무엇을 공부해야 할지 참 막막했습니다. 이러한 혼란 속에서 다짐했던 것은 2가지 였습니다. 대학원을 졸업할 무렵에는 누구에게나 자신있다고 말할 수 있는 분석 언어 1가지, 분석 분야 1가지를 만들겠다고 말이죠.4 대학원을 졸업하던 당시 가장 자신있었던 분석 언어와 분석 분야는 R과 시계열 자료분석 이었습니다. 이 두 가지 무기로 첫 번째 직장에 취업을 하고 머릿 속에 그리던 직무를 수행할 수 있었죠. 2가지 다짐의 개인적 근거는 이쪽 업계는 이것저것 두루두루 잘하는 Generalist 보단 하나 혹은 두 가지를 특출나게 잘하는 Specialist를 선호한다고 생각했기 때문입니다. 다양한 백그라운드를 가진 사람들이 일하는 Data Science/Analytics 업계인 만큼, 두리뭉술한 사람 보다는 확실한 아이덴티티가 있는 사람이 채용시장에서 높은 선호도를 보일 것이라고 생각했죠. 이쪽 업계에 있을수록 Generalist가 되기란 참 어렵지 않나 하는 생각을 합니다. Generalist가 되려다 이것저것 얕게 알고있는 특색없는 Generalist가 될 수 있다는 것을 유념하시기 바랍니다.\n반대로, Data Scientist/Analyst 직무로의 취업을 꿈꾸며 어느 기업 또는 어느 업계로 갈지에 대한 구체화가 끝나신 분들도 있을 수 있겠죠. 이 분들은 참 똑똑한 분들이라 생각합니다. 이러면 취업 준비가 꽤 편해지니까요. 가고 싶은 기업의 링크드인, 기술 블로그 등을 팔로우 하고 채용 공고를 미리미리 들여다보며, 자신이 해당 포지션으로 가기 위해 배워야할 것들을 구체화할 수 있습니다. 그럼, 자연스레 해당 기업에서 원하는 Specialist가 되는 길을 걷게 되겠죠. 공부 외에도 적극적인 액션을 취해보시기를 권합니다. Specialist가 되기 위해 공부해야하는 분야에 커뮤니티가 있다면 가입해서 활동도 해보고, 링크드인에 자신이 가고자 하는 업계 또는 기업에서 Data Science/Analytics를 수행하고 있는 분들이 보인다면 콜드메일(메시지)을 보내보기도 하면서요.😀"
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#맺음말",
    "href": "posts/2022-09-12-statistics-playbook-1/statistics-playbook-1.html#맺음말",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "맺음말",
    "text": "맺음말\n2021년 2월에 통계학 석사학위를 마치고, Data Scientist 직무로 현업에 있는 사람으로서, 2018년의 저와 비슷한 고민을 하고 있는 분들께 하고 싶은 몇 마디를 하고 글을 마치려고합니다.\n대학원 졸업을 앞두고 제가 성장한 부분 중 가장 뜻 깊게 생각되는 부분은 특정 알고리즘에 관한 이해가 아닌, 앞으로도 쏟아져 나올 분석 방법론, 그리고 소프트웨어 역량이라 할 수 있는 R의 수많은 패키지 등을 혼자 공부하고 정리할 튼튼한 발판을 마련했다는 점이었습니다. 그리고, 무엇보다 나를 브랜딩하고 PR 할만한 장치들도(e.g. Github, 개인 블로그) 갖출 수 있었죠. 제가 학부를 졸업하고 바로 해당 직무로 취업을 했다고 한들 이렇게 튼튼한 발판과 자신을 브랜딩하는 나만의 방법이 없이는 언젠가 성장의 한계에 마주했을 거라고 생각합니다.\n그래서, 만약 본인이 Data Scientist/Analyst 직무에 대해 열정과 호기심이 있는 통계학 전공자라면 주저하지 마시고 대학원에 진학하시는 것을 추천합니다. 호기심이 이끄는대로 열심히 이것저것 찾아보며 공부하고, 자신을 가꿔나갈 각오가 되어있는 분들께 대학원은 이쪽 업계에서 무조건 플러스라고 생각하니까요. 물론, 꼭 통계학 대학원이 아니여도 상관없습니다. 본인이 원하는 직무와 좀 더 관련성있는 연구실을 운영 중인 다른 학과가 있다면 해당 학과로의 진학을 추천드립니다. 예를 들어, 만약 본인이 특히 머신러닝이나 딥러닝 쪽 연구를 통해 예측 모델링을 전문적으로 수행하는 사람이 되고 싶다면 통계학 대학원이 아닌 컴퓨터 과학(Computer Science, 또는 소프트웨어 학과라 일컫는) 쪽에서 해당 분야를 전문적으로 연구하시는 교수님의 연구실에 들어가는 것을 추천드리고싶습니다.5\n이런저런 이야기들을 하다보니 글이 꽤 길어졌습니다. 제가 느낀 것들을 바탕으로 쓴 글이니 정답이라고 생각하진 않으셨으면 합니다. 취업, 그리고 진로에 대한 길을 만들어 나가는 것에는 수많은 정답이 존재하니까요."
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html",
    "title": "ggplot2 컬러링 가이드",
    "section": "",
    "text": "Photo by David Pisnoy on Unsplash\n오늘은 ggplot2에서 더 적은 수의 컬러로 더 직관적인 시각화를 가능하게끔 해주는 4가지 방식에 대해 소개해보려고 합니다. ggplot2를 바탕으로 진행되는 예제이긴 하나, 본 글에서 소개할 방식들에 담겨있는 아이디어는 언어, 시각화 라이브러리를 막론하고 적용이 가능할거라고 봅니다.물론, ggplot2만큼 짧고 가독성 좋은 코드로 구현이 가능할지는 미지수이지만요.😁 본 글에서 ggplot2의 그래프 문법 1이 갖는 강력한 힘을 확인하실 수 있을 겁니다.\n대중들은 말이 아닌 그림을 기억합니다. 그래서, 잘 만들어진 데이터 시각화는 강력한 힘을 갖습니다. 종종 데이터 시각화를 하시다가 지나치게 많은 색을 사용하게 되어 오히려 전달력이 떨어진다는 느낌을 받은 적이 있지 않으신가요? 그렇다면 이 글이 도움이 되실 수도 있겠습니다.😀"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#준비하기",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#준비하기",
    "title": "ggplot2 컬러링 가이드",
    "section": "준비하기",
    "text": "준비하기\n본격적인 시작 전 몇 가지 준비를 하고자 합니다. ggplot2에서 제공하는 다양한 테마 중 theme_minimal()을 사용할 예정이고, 폰트, 그림 제목과 색 등에 몇 가지 조정을 줄 예정입니다:\n\nshowtext 패키지를 통해 Fira Sans font 설정\n그림 제목은 기본 좌측 정렬, 색맹(color-blind)까지 고려한 Okabe Ito 컬러 팔레트 사용\n\n본 글의 작성에 참고한 원 글의 저자는 Fundamentals of Data Visualization by Claus Wilke를 읽은 뒤, Okabe Ito 컬러 팔레트를 선호하게 되었다고 합니다.\n\n\n\nlibrary(tidyverse)\nlibrary(showtext)\nfont_add_google(\"Fira Sans\", \"firasans\")\nshowtext_auto()\n\ntheme_customs <- theme(\n  text = element_text(family = 'firasans', size = 16),\n  plot.title.position = 'plot',\n  plot.title = element_text(\n    face = 'bold', \n    colour = thematic::okabe_ito(8)[6],\n    margin = margin(t = 2, r = 0, b = 7, l = 0, unit = \"mm\")\n  )\n)\n\ntheme_set(theme_minimal() + theme_customs)"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#음영을-활용하자",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#음영을-활용하자",
    "title": "ggplot2 컬러링 가이드",
    "section": "음영을 활용하자",
    "text": "음영을 활용하자\nggplot2 패키지에서 제공하는 mpg 데이터셋을 이용해 연도별 자동차 종류의 빈도를 시각화해봅시다. 전에 충분히 보셨을만한 데이터라 생각해서, 데이터셋에 관한 설명은 스킵하겠습니다. 이런 방식으로 시각화를 해보신 경험이 있으실겁니다:\n\nmpg |> \n  ggplot(aes(x = factor(year), fill = class)) +\n  geom_bar() +\n  labs(x = \"year\")\n\n\n\n\n\n\n\n\n자동차 종류가 많다보니 무려 7개의 컬러를 시각화에 사용하였습니다. 이 그림이 틀렸다고 할 수는 없습니다. 다만, 좋은 시각화라고 할 수 있는지에 대해 한 번 생각해보자는 겁니다. 제가 보기에 이렇게나 많은 수의 컬러를 사용하는 시각화는 꽤나 정신없어 보인다고 느껴집니다.여러 수준을 갖는 범주형 변수에 관한 컬러링에 있어서 더 적은, 최대 3개 정도의 컬러만 사용하여 시각화를 수행하는 방법은 없을까요? 이제 그 아이디어를 소개하고자 합니다. 자동차 종류를 구분하기 위해 색조(hues) 뿐만이 아닌, 음영(shades)을 활용하는 것이죠. 3가지 컬러만을 사용해 투명도를 줌으로써 7개의 자동차 종류를 구분해보겠습니다. 미리 말씀드리자면, minivan을 단독 하나의 그룹으로 설정해주어 이를 중심으로 투명도가 줄어들고 늘어나게끔 만드는 것이 키 아이디어입니다.\n이를 위해서는 우선 데이터에 자동차의 종류를 3가지로 구분짓는 새로운 그룹 변수를 생성해주어야 합니다:\n\n# Group classes into three groups (to reduce colors to 3)\ndat <- mpg |> \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  )\n\n이를 바탕으로 우선 먼저 시각화를 해보죠:\n\nshades_plt <- dat |> \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar() +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Show shades, not hues'\n  )\nshades_plt\n\n\n\n\n\n\n\n\n색조와 음영까지 활용해 3가지 컬러정도로 줄이긴 했지만, 아직 전달력은 매우 떨어집니다. 우리 눈으로 색조와 음영을 조합해 그림의 자동차 종류를 구분해내는 것은 꽤 어렵죠. 투명도와 색상을 직접 조정해보겠습니다. 투명도는 suv -> minivan까지 점차 줄어들고, minvan 이후부터는 다시 줄어든 양만큼 투명도가 늘어나도록 설정을 해주려고 합니다:\n\n# Color-blind safe colors\ncolors <-  thematic::okabe_ito(3)\n# Possible levels of transparency (one for each class)\nalpha_max <- 1\nalpha_min <- 0.7\nalpha_vals <- c(\n  seq(alpha_max, alpha_min, length.out = 4), \n  seq(alpha_min, alpha_max, length.out = 4)[-1]\n)\nalpha_vals\n\n[1] 1.0 0.9 0.8 0.7 0.8 0.9 1.0\n\n\n\n# Tweak previous plot\nshades_plt <- shades_plt +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals)\nshades_plt\n\n\n\n\n\n\n\n\n아까보다는 좀 낫습니다. 여기서 좀 더 개선을 해보자구요. 우측 범례를 하나로 좀 통합해서 설정하면 좋을 것 같은데요. 7개로 구분되어 있는 투명도에 컬러를 입혀줘서 말이죠. 꽤나 어려운 작업일 것 같지만, ggplot2에서는 아주 손쉬운 작업입니다. guides() 함수를 통해 가능합니다. fill에 관한 범례(class_group)는 삭제를 한 뒤에, guide_legend()를 통해 alpha에 관한 범례에 fill의 색상을 가져와 우리가 사전에 설정한 각각 3개 그룹(class_group)의 컬러를 덮어씌워(override) 줄겁니다. 말이 조금 복잡해보이지만, 코드를 보면 더 쉽게 이해하실 수 있습니다:\n\nshades_plt <- shades_plt +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(\n      override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]\n      )\n    )\n  ) \nshades_plt"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#음영만으로는-부족해",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#음영만으로는-부족해",
    "title": "ggplot2 컬러링 가이드",
    "section": "음영만으로는 부족해",
    "text": "음영만으로는 부족해\n충분히 괜찮은 시각화를 했지만, 아쉬운 부분이 하나 있습니다. 인접한 컬러 블록들에서는 자동차 종류 구분이 쪼~금 불편해 보입니다. 이 문제 또한 손쉽게 해결해줄 수 있어요. 블록마다 선 하나씩만 그어주면 말이죠. 앞선 시각화 코드에 geom_bar() 한 줄이면 해결할 수 있습니다. 이 또한 그래프 문법의 힘이죠.😄\n\ndat |> \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar(col = 'white') + # Add lines for distinction\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]))\n  ) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Group categories together by color, \\nbut keep showing them'\n  )"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#전달하고-싶은-내용만-강조하자",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#전달하고-싶은-내용만-강조하자",
    "title": "ggplot2 컬러링 가이드",
    "section": "전달하고 싶은 내용만 강조하자",
    "text": "전달하고 싶은 내용만 강조하자\n이제 조금 다른 이야기를 해보려고 합니다. 꼭 위와 같이 모든 범주에 대해 컬러를 줄 필요는 없는 상황도 있지 않을까요? 예를 들자면, 우리가 시각화를 통해 꼭 강조해서 전달하고 싶은 내용이 있을 때처럼요. 이번에 사용할 예시 데이터는 흥미로운 다양한 데이터셋을 제공해주는 Our World in Data에서 가져왔습니다. 미국인들을 대상으로 설문조사를 수행한 자료인데요. 본 자료를 통해 우리가 알아보고자 하는 바는 “우리는 과연 일생동안 누구와 시간을 많이 보내는가?”입니다. 여러 나이대의 미국인들을 대상으로 하루에 평균적으로 누구와 얼마나 시간을 보내는지에 대해 조사한 자료라고 할 수 있겠습니다. 위 링크의 차트에 아래 우측 탭을 보시면 Download를 눌러서 데이터를 받으실 수 있습니다:\n이 자료를 바탕으로 다음과 같은 그림을 그려볼 수 있습니다. 과연 나이에 따라 우리가 시간을 함께 보내는 대상은 어떤 식의 패턴을 보이며 변화할까요? 일반적으로는 다음과 같이 시각화를\n\n# Some data wrangling\ntime_data <- read_csv(\"./time-spent-with-relationships-by-age-us.csv\") |> \n  rename_with(\n    ~c('Entitity', 'Code', 'Age', 'alone', 'friends', 'children', 'parents', \n       'partner', 'coworkers')\n  ) |> \n  pivot_longer(\n    cols = alone:coworkers, \n    names_to = 'person',\n    values_to = 'minutes'\n  ) |> \n  janitor::clean_names() |> \n  filter(age <= 80)\n\nRows: 67 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (7): Year, Time spent alone, by age of respondent (United States), Time ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Color-blind safe colors\ncolors <- thematic::okabe_ito(7)[-6]\n\n# Line plot\np <- time_data |> \n  ggplot(aes(x = age, y = minutes, col = person)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = colors) +\n  coord_cartesian(xlim = c(15, 81), expand = F) +\n  scale_y_continuous(minor_breaks = NULL) +\n  labs(x = 'Age (in years)', y = 'Minutes', col = 'Time spent')\np\n\n\n\n\n\n\n\n\n보통 이렇게들 시각화하곤 하죠. 이런 종류의 그림은 스파게티 플롯(spaghetti plot)이라고 표현하기도 합니다. 우리는 또 수많은 컬러에 직면했습니다. 아울러, 이 그림 한 장만 놓고 봤을때는 무슨 말을 전달하고자 하는지 파악하기가 참 힘듭니다. 제 눈엔 우선 2가지 인사이트가 보입니다:\n\n우리는 일생동안 혼자서 가장 많은 시간을 보내게 된다.\n40대 즈음해서 아이와 보내는 시간은 줄어들면서, 혼자 보내는 시간이 많아진다.\n\n이와 같이 만약 우리가 전달하고자 하는 인사이트가 확실한 상태라면, 중요한 부분만 강조함으로써 이 지저분한 스파게티 플롯을 전달력 있는 깔끔한 스파게티 플롯으로 만들어 줄 수 있습니다. ggplothighlight 패키지가 그 해결책이 되어줍니다. 패키지 안의 ggplothighlight() 함수를 이용해 레이어를 하나더 얹어서, 필터링을 해줄 수 있어요. 아주 편리한 패키지죠. 특정 조건을 만족하지 않는 데이터 포인트는 모조리 회색으로 표현이 됩니다. 먼저 첫 번째 인사이트를 그림으로 표현해봅시다. 코드 1줄 정도만 추가해주면 가능합니다.\n\nlibrary(gghighlight)\nalone_plt <- p + \n  gghighlight(person == 'alone', use_direct_label = F) +\n  labs(title = 'Emphasize just one or a few categories')\nalone_plt\n\n\n\n\n\n\n\n\n이 그림에 텍스트를 추가하여 우리가 하고싶은 이야기를 좀 더 강조할 수도 있습니다:\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  )\n\n\n\n\n\n\n\n\n하고 싶은 이야기가 하나가 아니라 여러개라면 어떤 방법이 있을까요? 전혀 문제가 되지 않습니다.😀 gghighlight()를 사용해 그저 여러 조건 넣어주기만 하면 됩니다.\n\nage_40_plt <- p + \n  gghighlight(\n    person %in% c('alone', 'children'), \n    age >= 38, \n    use_direct_label = F\n  ) +\n  geom_segment(x = 38, xend = 38, y = -Inf, yend = 300, linetype = 2, col = 'grey20') +\n  labs(title = 'Emphasize just one or a few categories') \n\nage_40_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 403,\n    label = 'Around the age of 40, we spend \\nless time with children and \\nmore time alone.',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 0.85,\n    size = 5.5\n  )"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#라벨링-활용하기",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#라벨링-활용하기",
    "title": "ggplot2 컬러링 가이드",
    "section": "라벨링 활용하기",
    "text": "라벨링 활용하기\n앞서 본 모든 그림들에서는 그림의 이해를 돕기위한 범례(legend)가 우측에 자리하고 있었습니다. 범례는 그림에서 꽤나 큰 공간을 차지합니다. 아울러, 범례는 그림에 집중도를 떨어뜨릴 수 있죠. 그림의 이해를 위해서는 필연적으로 범례와 그림을 번갈아가며 봐야하니까요. 이 문제를 해결할 방법은 없을까요? 범례를 없애고 그림에 라벨링을 통해 우리가 하고자 하는 이야기를 전달하면 어떨까요? 싱글 레이블에 대해서는 annotate(), 다중 레이블에 대해서는 geom_text()를 이용해 라벨링을 하면 되는데요. 우선 annotate()를 활용해 스파게티 플롯 예제를 개선시켜 보겠습니다:\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  ) +\n  annotate(\n    'text', \n    x = 70, \n    y = 420, \n    label = 'alone',\n    hjust = 0,\n    vjust = 0,\n    size = 7,\n    family = 'firasans',\n    color = colors[1]\n  ) +\n  labs(title = 'Label directly') +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n이러한 방식으로 공간도 세이브하고, 그림에 집중도도 훨씬 높혀줄 수 있죠. 범례와 플롯을 눈으로 왔다갔다 할 필요도 없고요. 여기서 조금 더 개선을 해볼까요? 현재 위 그림에는 alone이라는 단어가 중복으로 들어가있죠. 강조한 선 아래 alone을 없애고 선과 동일한 색상을 좌측 문장의 alone에 넣어주는 것은 어떨까요? 더 매력적인 시각화가 될 것만 같다는 생각이 들지 않나요?\n이를 위해서는 ggtext 패키지를 활용해 HTML 문법을 이용해야합니다. annotation()의 text geom을 richtext geom으로 바꾸고, 우리가 컬러를 반영하고자 하는 텍스트에 대해서는 HTML 코드를 포함하는 문자열을 만들어 주는 과정이 필요합니다. 말이 좀 복잡해보이지만, 코드는 꽤 간단합니다:\n\nlibrary(ggtext)\ncolor_alone <- glue::glue(\n  \"We spend a lot of time <span style = 'color:{colors[1]};'>alone</span>...\"\n)\ncolor_alone\n\nWe spend a lot of time <span style = 'color:#E69F00;'>alone</span>...\n\n\n\nalone_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 400,\n    label = color_alone,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 6,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n멋지지 않습니까? HTML 문법에 익숙하지 않은 분들은 컬러링을 넣어주는 형태를 기억하시기 바랍니다. 이런 식으로 직접적으로 라벨링 하는 방식은 스파게티 플롯 예제의 두 번째 인사이트를 나타내는 그림에 대해서도 손쉽게 적용이 가능합니다.\n\nage_40_text <- glue::glue(\n  \"Around the age of 40, we spent <br> less time with \n  <span style = 'color:{colors[2]};'>children</span> \n  and <br> more time <span style = 'color:{colors[1]};'>alone</span>.\"\n)\n\nage_40_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 350,\n    label = age_40_text,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 1.25,\n    size = 5,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n이런 완성도 있는 시각화는 청중 또는 대중들에게 우리가 전달하고자 하는 이야기를 직관적으로 전달해줍니다. 범례와 플롯을 눈으로 왔다갔다 하는 피로를 덜어주는 것은 덤이고요.\n마지막으로, 우리가 초기에 했던 예제인 막대그래프(bar chart) 예제에도 이를 적용해봅시다. 해당 예제의 경우 다중 레이블에 해당하므로 annotate()이 아닌 geom_text()가 필요로 됩니다. 그런데, 어떤 이유에서인지.. 자동차 종류의 각 레이블 위치가 정렬이 안되는 문제가 있었습니다. 불가피하게 연도별, 자동차 종류별 막대그래프의 높이를 누적빈도(csum)로 계산하고, 레이블이 들어갈 위치(n)를 적당하게 잡아주는 작업을 수행했습니다.🤯 코드가 조금 복잡해 보이긴 하나, 출력된 결과를 확인하시면 어떤 작업을 진행했는지 쉽게 이해하실 수 있을겁니다.\n\nmanual_counts <- mpg |> \n  count(year, class) |> \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  ) \nmanual_counts\n\n# A tibble: 14 × 4\n   year  class          n class_group\n   <fct> <chr>      <int> <chr>      \n 1 1999  2seater        2 grp1       \n 2 1999  compact       25 grp1       \n 3 1999  midsize       20 grp1       \n 4 1999  minivan        6 grp2       \n 5 1999  pickup        16 grp3       \n 6 1999  subcompact    19 grp3       \n 7 1999  suv           29 grp3       \n 8 2008  2seater        3 grp1       \n 9 2008  compact       22 grp1       \n10 2008  midsize       21 grp1       \n11 2008  minivan        5 grp2       \n12 2008  pickup        17 grp3       \n13 2008  subcompact    16 grp3       \n14 2008  suv           33 grp3       \n\nlabels <- manual_counts |> \n  mutate(class = factor(class)) |>  \n  group_by(year) |> \n  arrange(year, desc(class)) |> \n  mutate(\n    csum = cumsum(n), \n    n = (lag(csum, default = 0) + csum) / 2\n  )\nlabels\n\n# A tibble: 14 × 5\n# Groups:   year [2]\n   year  class          n class_group  csum\n   <fct> <fct>      <dbl> <chr>       <int>\n 1 1999  suv         14.5 grp3           29\n 2 1999  subcompact  38.5 grp3           48\n 3 1999  pickup      56   grp3           64\n 4 1999  minivan     67   grp2           70\n 5 1999  midsize     80   grp1           90\n 6 1999  compact    102.  grp1          115\n 7 1999  2seater    116   grp1          117\n 8 2008  suv         16.5 grp3           33\n 9 2008  subcompact  41   grp3           49\n10 2008  pickup      57.5 grp3           66\n11 2008  minivan     68.5 grp2           71\n12 2008  midsize     81.5 grp1           92\n13 2008  compact    103   grp1          114\n14 2008  2seater    116.  grp1          117\n\n\n레이블이 들어갈 자리를 계산하는 키 아이디어는 lag()를 통해서 계산한 누적빈도를 0값을 시작으로해서 한칸씩 당겨주고, 계산해둔 누적 빈도(csum)를 더하여 2로 나눠주는 것입니다. 이 작업을 수행하면 막대그래프를 구성하는 각 칸의 중간 높이를 계산할 수 있는 것이죠.\n아울러, 우리가 본 시각화에서 한 가지 더 극복해야할 난관은 바로 자동차의 종류 중 2seater의 빈도가 매우 작아서 레이블이 들어갈 자리가 없는 점이 었습니다. 그래서, 2seater의 경우 레이블을 막대의 맨 위에 표시되도록 하였습니다. 이러한 모든 난관들을 극복하고 완성한 그림을 공개합니다.\n\nmanual_counts |> \n  ggplot(aes(x = year, y = n, fill = class_group)) +\n  geom_col(aes(alpha = class), col = 'white') +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Label directly'\n  ) +\n  # Add all but one label\n  geom_text(\n    data = labels |> filter(class != '2seater'),\n    aes(label = class), \n    col = 'white',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  # Add 2seater label\n  geom_text(\n    data = labels |> filter(class == '2seater'),\n    aes(y = n + 3, label = class), \n    col = 'black',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#맺음-말",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#맺음-말",
    "title": "ggplot2 컬러링 가이드",
    "section": "맺음 말",
    "text": "맺음 말\n분석을 시작하며 혼자 가볍게 EDA를 하는 단계에서 본 예제와 같이 시각화를 개선해나가는 작업은 필요로 되지 않을겁니다. 오히려 시간 낭비일수도 있구요. 그러나, 내가 얻은 인사이트를 전달하는 자리 또는 데이터를 기반으로 누군가를 설득해야하는 자리에서는 이 글에서 제공하는 몇 가지 방법이 꽤나 도움이 될 것이라고 생각합니다. 물론, 전달하고자 하는 내용이 한 눈에 들어도록 시각화를 수행하는 작업은 의외로 쉬울 때도 있지만, 꽤나 까다로운 과정을 거쳐야하는 상황도 존재합니다. 실무에서는 이와는 또다른 예상치 못한 까다로운 문제들을 겪는 상황들이 있을 수도 있구요. 다만, 본 글에서 그림의 퀄리티를 단계단계 개선해나간 바와 같이 전달하고자 하는 내용을 명확히하고 충분한 시간을 숙고해 그림을 개선해 나간다면, 뭐든 해결할 수 있을 것이라고 봅니다. 하고자 하는 시각화를 구현하지 못해낸다고 하더라도 그 과정 속에서 배우는 것은 분명히 존재할 것입니다. 처음부터 완벽하게 아름다운 시각화를 해낼 수 있는 사람은 없다는 것을 기억하셨으면 합니다.😁\n이번 포스팅을 준비하며 참고했던 글은 올해 봤던 데이터 시각화 관련 아티클 중 제게 가장 큰 임팩트를 주는 글이었습니다. 누구나 하는 평범한 시각화를 비범하게 만들어주는 글이라고 표현하면 적절할까요? 많은 사람들이 알았으면 하는 내용이라, 8월 서울 R 미트업에서 본 내용을 주제로 발표를 하기도 했습니다. 지금 이 글을 읽고 계신 여러분들에게도 좋은 인사이트를 줄 수 있는 글이 되었으면 합니다.\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-20\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.1.251 @ /Users/taemobang/Applications/quarto/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.0.10  2022-09-01 [1] CRAN (R 4.2.0)\n forcats     * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n gghighlight * 0.3.3   2022-06-06 [1] CRAN (R 4.2.0)\n ggplot2     * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n ggtext      * 0.1.2   2022-09-16 [1] CRAN (R 4.2.1)\n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n readr       * 2.1.2   2022-01-30 [1] CRAN (R 4.2.0)\n rmarkdown   * 2.16    2022-08-24 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n showtext    * 0.9-5   2022-02-09 [1] CRAN (R 4.2.0)\n showtextdb  * 3.0     2020-06-04 [1] CRAN (R 4.2.0)\n stringr     * 1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n sysfonts    * 0.8.8   2022-03-13 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr       * 1.2.1   2022-09-08 [1] CRAN (R 4.2.0)\n tidyverse   * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "",
    "text": "Photo by Agê Barros on Unsplash\ntidyverts ecosystem은 시계열 자료에 관한 분석을 tidyverse principle로 수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidyverse priciple이 데이터 전처리에 있어서 얼마나 많은 업무 생산성을 가져다 주는지 우리는 이미 알고있습니다. 시계열 자료를 자주 다루는 사람이라면 꼭 배워둘 만한 패키지죠.😄 tidyverts ecosystem을 이루는 대부분의 패키지들은 {fpp3}으로 불러올 수 있습니다. {tsibbletalk}은 {shiny}와 함께 동작하는 반응형 그래픽을 제공하는 패키지로 본 튜토리얼에서는 생략하겠습니다:\n위 패키지들이 설치되어 있지 않은 분들은 튜토리얼의 본격적인 시작전에, install.packages(\"패키지명\")을 통해 설치해주시기 바랍니다. 개발 버전을 설치하고 싶으신 분이 있다면 다음의 코드를 이용하세요:"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#tsibble",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#tsibble",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "1 tsibble",
    "text": "1 tsibble\n\n1.1 Get Started\n{tsibble}은 일반적인 시계열 자료를 tibble 형태로 표현할 수 있게해줍니다. 우리는 tsibble()을 통해 tidy한 자료에 대해 수행해왔던 {tidyverse}를 이용한 wrangling을 수행할 수 있습니다. 즉, tidyverse ecosystem이 tibble 객체를 기반으로 동작하듯이, tidyverts ecosytem은 tsibble 객체를 기반으로 동작합니다. tsibble 객체가 갖는 기본적인 원칙은 다음과 같습니다:\n\nindex: 과거부터 현재까지 순서화된 자료값의 관측 시간\nkey: 시간에 따른 관측 단위를 정의하는 변수의 집합\n각 관측치는 index와 key를 통해 유일하게(uniquely) 식별되어야만 함\n각 관측치는 등간격으로 관측된 자료여야만 함\n\n즉, 티블(데이터프레임)을 tsibble로 변환하기(coerce) 위해서는 key와 index를 명시해주어야 합니다. 예를 들어, 다음과 같은 {nycflights13} 패키지의 weather 자료를 이용해보겠습니다:\n\nweather_simple <- nycflights13::weather %>% \n    select(origin, time_hour, temp, humid, precip)\nweather_simple\n\n\n\n\n\n  \n\n\n\norigin을 key로 index를 time_hour로 해주면 될 것 같습니다:\n\nweather_tsbl <- as_tsibble(weather_simple, key = origin, index = time_hour)\nweather_tsbl\n\n\n\n\n\n  \n\n\n\n여기서는 자료 자체가 출발지(origin) 별로 기록된 다중(multiple) 시계열에 해당하므로, key를 origin으로 잡아줬지만, 만약 자료가 단일(univariate) 시계열에 해당한다면 해당 key는 설정을 하지 않으면 됩니다(see package?tsibble and vignette(\"intro-tsibble\") for details). 그리고, 사실 tsibble()은 irregular time interval을 갖는 자료에 대해서도 적용이 가능합니다. as_tsibble은 regular = TRUE 옵션이 default로 설정되는데, 이를 FALSE로 바꿔주면 되며, 이러한 irregular time interval을 갖는 tsibble 객체의 경우는 [!] 표시를 통해 확인할 수 있습니다:\n\nnycflights13::flights %>%\n    mutate(\n      sched_dep_datetime = make_datetime(year, month, day, hour, minute, \n                                         tz = \"America/New_York\")) %>%\n    as_tsibble(\n        key = c(carrier, flight), \n        index = sched_dep_datetime, \n        regular = FALSE\n        )\n\n\n\n\n\n  \n\n\n\n\n\n1.2 Turn impicit missing values into explicit missing values\n간혹 시계열 자료에는 암묵적 결측치(implicit missing values)가 존재하는 경우가 있습니다. 암묵적 결측치가 존재하는 시계열 자료가 일정한 시간 간격으로 수집되었을 경우, 우리는 fill_gaps()를 이용해 암묵적 결측을 명시적으로(explicit) 바꿀 수 있어요. 4년간 수집된 연도별 키위, 체리의 수확량(단위: kg)에 관한 자료를 직접 만들어서 fill_gaps()의 쓰임에 대해 알아봅시다. 본 자료에는 암묵적 결측이 존재합니다:\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest\n\n\n\n\n\n  \n\n\n\n암묵적 결측이란, 예를 들어 위 자료처럼 체리 생산량이 2010년에는 기록되지 않았음에도 불구하고 행이 생략되어있는 것을 말합니다. NA로 명시는 다음과 같이 손쉽게 가능합니다:\n\nfill_gaps(harvest, .full = TRUE)\n\n\n\n\n\n  \n\n\n\n다음의 각각 시작점, 끝점에 대해서만 결측치를 명시할 수도 있습니다:\n\n# at the same starting point across units\nfill_gaps(harvest, .full = start())\n# at the same end point across units\nfill_gaps(harvest, .full = end())\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n.full = FALSE를 설정할 경우(fill_gaps()의 default 옵션에 해당), 각 key 내의 period에서 발생한 결측에 대해서만 명시가 이루어집니다.\n\nfill_gaps(harvest, .full = FALSE)\n\n\n\n\n\n  \n\n\n\n특정값으로의 명시도 손쉽게 수행이 가능해요.\n\nharvest %>% \n    fill_gaps(kilo = 0L)\n\n\n\n\n\n  \n\n\n\n변수에 대해 함수를 적용하여 명시도 가능합니다. sum()을 이용하여 합으로 명시해보았습니다:\n\nharvest %>%\n    fill_gaps(kilo = sum(kilo))\n\n\n\n\n\n  \n\n\n\nkey에 대해 group_by를 통해 각 그룹에 대해 함수를 적용할 수도 있죠. 이번에는 median()을 통해 중위수로 명시해보았습니다:\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo))\n\n\n\n\n\n  \n\n\n\n원 자료 자체에 NA가 존재하는 경우, 적용하고자 하는 함수에 na.rm = TRUE을 설정해주면 됩니다:\n\nharvest[2, 3] <- NA\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo, na.rm = TRUE))\n\n\n\n\n\n  \n\n\n\n마지막으로, fill_gaps()아 tidyr::fill()을 함께 이용하면 암묵적 결측치를 이전 시점의 결측치로 대치할 수 있습니다.\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"down\")\n\n\n\n\n\n  \n\n\n\n반대로, 한 시점 미래의 값으로 대치도 가능합니다.\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"up\")\n\n\n\n\n\n  \n\n\n\n\n\n1.3 Aggregate over calendar periods\nindex_by()와 summarise()를 이용하면 관심있는 변수에 대해 특정 시간 주기(e.g. monthly)에 대해 함수(e.g. 합계: sum(), 평균: mean())를 적용할 수 있어요. index_by는 as.Date(), tsibble::yearweek(), tsibble::yearmonth(), tsibble::yearquarter(), 뿐만 아니라 {lubridate} 계열의 함수와 함께 사용됩니다. 예를 들어, weather 자료의 월별 평균 기온, 총 강수량은 다음과 같이 yearmonth()에 index 변수를 .으로 나타내어 계산할 수 있습니다.\n\nweather_tsbl %>% \n    group_by_key() %>% \n    index_by(year_month = ~yearmonth(.)) %>%\n    summarise(\n        avg_temp = mean(temp, na.rm = TRUE),\n        total_precip = sum(precip, na.rm = TRUE)\n    )\n\n\n\n\n\n  \n\n\n\nindex_by()+summarise()는 irregular time interval을 갖는 tsibble에 대해서도 수행이 가능합니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#tsibbledata",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#tsibbledata",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "2 tsibbledata",
    "text": "2 tsibbledata\n{tsibbledata}는 tsibble 형태의 다양한 예제 자료를 제공해줍니다. 어떤 패키지에 대한 튜토리얼을 진행할 때, 적절한 자료들이 필요로 되는데, 이렇게 예제 자료를 직접적으로 제공해준다는 점에서 R 유저들에 대한 배려가 담겨있다는 생각이 드네요. 예를 들어, 다음의 olympic_running은 4년 주기로 수집된 올림픽 달리기 종목의 성별 최고기록에 관한 자료입니다(see ?olympic_running for details).\n\nolympic_running\n\n\n\n\n\n  \n\n\n\n이 자료를 이용하여 달리기 종목별 최고 기록에 대한 시도표를 성별로 나누어서 그려보았습니다. 참고로, 1916, 1940, 1944년의 경우 세계대전으로 인해 결측 처리되었습니다.\n\nggplot(olympic_running, aes(x = Year, y = Time, colour = Sex)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(~ Length, scales = \"free_y\", nrow = 2) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"Dark2\") + \n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  ylab(\"Running time (seconds)\")"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#feasts",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#feasts",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "3 feasts",
    "text": "3 feasts\n{feasts}는 Feature Extraction And Statistics for Time Series의 약자로, 시계열 자료분석에 쓰이는 여러가지 툴을 제공해줍니다. tsibble 객체와 함께 동작하며, 시계열의 분해, feature 추출(e.g. 추세, 계절성), 시각화 등을 수행할 때 쓰입니다. 아울러, {feasts}를 통한 시계열 자료분석은 다음 섹션에서 소개할 tidyverts ecosystem의 예측 모델링 부분을 담당하는 {fable} 패키지와 긴밀하게 결합하여 사용됩니다.\n\n3.1 Graphics\n시각화는 주로 시계열 자료의 패턴을 이해하기 위한 첫 단계에 많이 이루어집니다. {feasts}는 시계열의 패턴을 {ggplot2}를 사용해 자유롭게 커스텀할 수 있는 그래픽을 제공합니다. 첫 번째로는 gg_season을 이용한 계절성(seasonality) 시각화입니다. 시각화에 사용된 자료 tsibbledata::aus_production은 호주의 맥주, 담배 등의 품목에 관한 분기별 생산지표 추정치에 관한 자료입니다. 맥주의 분기별 생산지표에 관한 계절성 시각화를 수행해보았습니다:\n\naus_production %>% \n  gg_season(Beer)\n\n\n\n\n\n\n\n\n다음으로 gg_subseries()를 이용하면 시계열의 각 season별로 시각화가 가능합니다. 예를 들어, aus_production과 같은 분기별 자료의 경우 분기별 패턴에 대한 시각화를 쉽게 수행할 수 있습니다:\n\naus_production %>% \n  gg_subseries(Beer)\n\n\n\n\n\n\n\n\ngg_lag()를 이용하면 원자료와 시차(lag)의 산점도를 season별로 나누어 그릴 수 있습니다:\n\naus_production %>% \n  filter(year(Quarter) > 1991) %>% \n  gg_lag(Beer, geom = \"point\")\n\n\n\n\n\n\n\n\n분기별 자료의 특성상, lag 4와 8 그림을 보면 각 season별로 원자료와의 관계가 \\(y=x\\) 직선에 잘 놓여있는 것을 캐치할 수 있죠. 마지막으로 ACF 그림도 손쉽게 그릴 수 있습니다:\n\naus_production %>% \n  ACF(Beer) %>% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n3.2 Decompositions\n시계열 분해(decomposition)는 시계열 자료분석에서 흔히 수행되는 작업 중 하나이며, 이는 시계열에 대한 패턴을 이해하는데에 큰 도움을 줍니다. 그리고, 추후 예측 모델링을 정교하게 하는 것에도 상당한 도움을 준다. 즉, 시계열 분해는 본인이 분석하고자 하는 시계열의 패턴을 좀 더 정교하게 캐치하고 예측 성능을 향상시키기 위한 목적으로 꼭 필요로 되는 사전 작업이라고 할 수 있습니다. 본 튜토리얼에서는 {feasts}에서 제공하고 있는 2가지 시계열 분해 방법에 대해 소개하려고 합니다.\n\n3.2.1 Classical decompostion\nclassical decompostion은 1920년대에 고안된 방법입니다. 오래된 방법론인 만큼 요즘 쓰이는 시계열 분해 방법들의 초석이 되는 방법이라고 할 수 있으며, 다른 방법들에 비해 상대적으로 간단하다는 장점이 있습니다. classical decompostion은 가법 분해와 승법 분해가 있습니다. 두 방법은 계절성의 반영 방식에 따라 나뉩니다(e.g. 분기별 자료 \\(m = 4\\), 월별 자료 \\(m = 12\\), 일별 자료 \\(m = 7\\)). 보통 가법 classical decompostion의 경우 계절성이 추세에 따라 무관하게 일정한 크기를 유지할 때 사용하며, 반대로 계절성의 크기가 추세의 크기에 따라 변화하는 경우에는 승법 classical decompostion을 사용합니다. 승법 계절성 classical decompostion는 계절 성분이 연도에 따라 상수라고 가정한채로 진행되며, 승법 계절성에서 계절 성분을 형성하는 \\(m\\)은 계절 지수(seasonal indices)라 불리기도 합니다.\nclassical decompostion의 자세한 분해 과정은 여기를 참고해주시기 바랍니다. 여기서는 바로 R을 이용한 튜토리얼을 진행하겠습니다. 앞서 사용했던 자료의 맥주 생산지표를 가법 classical decomposition을 통해 분해해보겠습니다.\n\ndcmp <- aus_production %>%\n    model(classical_decomposition(Beer, type = \"additive\"))\ncomponents(dcmp)\n\n\n\n\n\n  \n\n\n\n먼저, 분해된 시계열의 요소들은 componenets()로 불러올 수 있습니다. 그리고, 이 components()에 대해 autoplot()을 수행해주면 다음과 같이 시각화를 수행할 수 있습니다:\n\ndcmp %>%\n    components() %>% \n    autoplot() +\n    labs(title = \"Classical additive decomposition of Quarterly production of beer in Australia\")\n\n\n\n\n\n\n\n\n\n\n3.2.2 STL decomposition\nSTL은 “Seasonal and Trend decomposition using Loess”의 준말로 다재다능(versatile)하고 로버스트한 시계열 분해 방법에 해당합니다. 그리고, 여기서 loess란 Local regression의 준말로 자료를 비선형으로 추정하는 방법 중 하나에 해당합니다. STL은 앞서 소개한 classical decomposition, 그리고 {feasts}에서 제공하는 또 다른 시계열 분해 방법 SEATS, X-11과 비교하여 몇몇 이점을 갖는다. 자세한 사항은 여기를 참고해주세요. 본 글은 tidyverts ecosystem에 대한 소개 이므로, deep한 이론 정리는 추후에 fpp3 책을 공부하면서 하나하나 정리해나가겠습니다. 일단 바로 실습으로 넘어가겠습니다.😊 다음은 STL decomposition을 이용하여 시계열의 추세 요소는 window = 7을 통해 좀 더 flexible하게 추정하고, 계절 패턴의 경우는 window = \"periodic\"으로 하여 고정(fixed)되도록 하였습니다(see ?STL for details). 여기서. window란, 창을 말하며 자료를 여러 창으로 잘게 쪼갤수록 더 flexible하고 복잡한 함수를 추정하게 됩니다. splines에 지식이 있는 분들은 이해하기 쉬울거라고 생각합니다.\n\naus_production %>%\n  model(\n    STL(Beer ~ trend(window = 7) + season(window = \"periodic\"),\n        robust = TRUE)) %>%\n  components() %>%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Feature extraction and statistics\n{feast}에서 소개할 마지막 기능은 시계열의 feature(e.g. ACF)와 통계량(e.g. 평균)을 뽑아내는 것입니다. {feast}에서는 feature() 함수를 통해 많은 종류의 features들에 대한 정보를 제공합니다만, 본 튜토리얼에서는 시계열의 평균, 분위수, ACF를 뽑아내는 방법에 대해서만 소개하겠습니다(see ?feature for details). 그 외 다른 features들에 관심이 있으시다면, 여기를 참고해주세요.\n\n3.3.1 Some simple statistics\n먼저, 시계열의 평균과 분위수를 뽑는 방법에 대해 소개하겠습니다. 평균, 분위수 등 시계열의 기본적인 통계량은 feature()와 R의 기본 함수(e.g. mean(), median())들을 이용해 간편하게 계산할 수 있습니다. 여기서 이용할 자료 tourism()은 지역, 주, 목적별로 나눠진 1998-2016년 분기별 호주 여행객수에 관한 자료로, 지역, 주, 여행 목적별 여행객 수의 전체 평균과 분위수를 계산해봤습니다:\n\ntourism %>%\n    features(Trips, \n             list(mean = mean, quantile))\n\n\n\n\n\n  \n\n\n\n\n\n3.3.2 ACF features\nACF에 관한 정보는 feat_acf()를 이용하면 됩니다. feat_acf()는 기본적으로 ACF와 관련한 6가지 또는 최대 7가지의 features를 제공해줍니다(see ?feat_acf() for details):\n\n원 계열의 1차 자기상관계수\n원 계열의 1차-10차 자기상관계수의 제곱합\n1차 차분 계열의 1차 자기상관계수\n1차 차분 계열의 1차-10차 자기상관계수의 제곱합\n2차 차분 계열의 1차 자기상관계수\n2차 차분 계열의 1차-10차 자기상관계수의 제곱합\n(계절 시계열에 대해) 첫번째 계절 시차(seasonal lag)에서의 자기상관계수\n\n\ntourism %>% \n  features(Trips, feat_acf)\n\n\n\n\n\n  \n\n\n\n맨 마지막 열이 첫번째 계절 시차에서의 자기상관계수를 나타내는데, 본 자료의 경우 분기별 자료에 해당하므로 계절 주기는 4에 해당합니다. 즉, 본 자료에서 첫번째 계절 시차에서의 자기상관계수는 원 계열의 시차 4에서의 ACF 값을 나타낸다고 할 수 있습니다.\n\ntourism %>% \n  features(Trips, feat_acf) %>% \n  select(Region:Purpose, season_acf1)\n\n\n\n\n\n  \n\n\n\n원자료에 대한 ACF를 구해보면 다음과 같이 시차 4에서의 자기상관계수와 동일한 값을 가짐을 알 수 있죠:\n\ntourism %>% \n    ACF(Trips)\n\n\n\n\n\n  \n\n\n\n본 튜토리얼에서는 소개하지 않았지만, feature()를 이용한 시계열 feature extraction과 연계하여 다양한 시각화도 수행할 수 있습니다. 꼭 참고해보시기 바랍니다: https://otexts.com/fpp3/stlfeatures.html"
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#fable",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#fable",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "4 fable",
    "text": "4 fable\n{fable} 패키지는 tsibble 객체와 함께 tidy한 format으로 시계열 예측 모델링을 수행할 수 있게해줍니다. {tidymodels} 패키지에 대한 이해가 있으신 분들이라면 어렵지 않으실거라 생각합니다. {tidymodels}과 마찬가지로 {fable}은 여러 시계열에 대해 여러 시계열 모형에 대한 추정, 비교, 결합, 예측 등을 가능하게해줍니다.\n본격적인 튜토리얼 시작에 앞서, tourism() 자료를 이용할 것이며, 4가지 여행 목적(“business”, “holiday”, “visiting friends and relatives”, “other reasons”)으로 분해할 수 있는 호주 멜버른(Melbourne)의 일별 여행객 수를 예측하는 것에 관심이 있다고 가정합니다. 각 계열의 첫 번째 관측값은 다음과 같습니다:\n\ntourism_melb <- tourism %>% \n  filter(Region == \"Melbourne\")\ntourism_melb %>% \n    group_by(Purpose) %>% \n    slice(1)\n\n\n\n\n\n  \n\n\n\n우리가 추정하고자 하는 변수는 Trips(일별 여행객 수, 단위: 천)입니다. 해당 계열들의 시도표를 보면, 추세와 약한 계절성이 명확하게 존재함을 알 수 있습니다.\n\ntourism_melb %>% \n  autoplot(Trips)\n\n\n\n\n\n\n\n\n{fable} 패키지에서 폭넓게 쓰이는 시계열 예측 모형은 ETS와 ARIMA 모형입니다. 먼저, ETS 모형은 추세 요소와 계절 요소를 가법, 승법, 감쇠효과 등을 반영하여 시계열을 모델링하는 지수평활법(exponential smoothing)을 통계적 모형으로 확장시킨 것에 해당합니다. 통계적 모형으로의 확장은 오차항 \\(\\epsilon_t\\)에 대해 통계적 분포라 할 수 있는, 평균이 0이고 분산이 \\(\\sigma^2\\)인 가우스 백색잡음 과정(gaussian white noise process)을 가정함으로써 이루어집니다. 즉, ETS 모형의 알파벳 각각은 E(error, 오차), T(trend, 추세), S(seasonal, 계절성)을 나타내며, 각 요소들을 모델링하는 방식(가법, 승법, 가법감쇠(damped), 승법감쇠)에 따라 ETS 모형의 종류가 나뉘어집니다. 아울러, 각 모델은 관측된 자료를 설명하는 측정식(measurement equations)과 시간에 따라 변화하는 관측되지 않은 요소(level, trend, seasonal)들을 설명하는 상태식(state equations)으로 구성되는데, 이러한 이유에서 우리는 ETS 모형을 혁신상태공간모형을 이루는 지수평활법(innovations state space models for exponential smoothing)이라고 표현하기도 합니다(See here for detail). 두 번째로, ARIMA 모형은 시계열의 현재값을 과거값과 과거 예측 오차로 설명하는 대표적인 통계적 시계열 예측모형으로, 자세한 설명은 생략하겠습니다. ARIMA 모형에 대한 개념이 없으신 분들은 여기를 참고해주시기 바랍니다.\n두 모형에 대한 간략한 개념 설명은 이쯤에서 마치기로 하고, 이제 이 모형들을 {fable} 패키지를 이용해 어떻게 적합을 수행하면 되는지 보겠습니다. {fable}을 이용한 모형 적합은 model()을 통해 이루어집니다. model()을 통한 적합 과정은 {tidymodels}와 유사하게 상당히 직관적인 이름의 함수들로 이루어집니다. 먼저, ETS()의 경우는 R에서 일반적으로 사용하는 모형식의 specification를 따라서 각 요소를 반영할 수 있게 해주며, 본 예제에서는 추세 요소만 가법적으로 설정해주고 나머지 요소는 자동으로 선택되도록 하였습니다(AICC를 기준으로, see ?ETS for details). 그리고, ARIMA 모형은 ARIMA() 함수로 적합할 수 있으며, 해당 함수는 {forecast} 패키지의 auto.arima와 유사하게 default 옵션으로 AICC 값을 기준으로 최적의 모형을 선택해 줍니다(see ?ARIMA). model()을 통해 적합이 이루어진 모형 객체는 tidy한 포맥의 모형 테이블로 결과를 반환해줍니다. 이를 이제부터 mable(model table) 객체라 칭하겠습니다:\n\nfit <- tourism_melb %>% \n  model(\n    ets = ETS(Trips ~ trend(\"A\")),\n    arima = ARIMA(Trips)\n  )\nfit\n\n\n\n\n\n  \n\n\n\nmable 객체의 행은 각 시계열로 이루어져있으며, 열은 각 모형의 specification을 나타냅니다. fit이 반환하는 결과를 보면 알 수 있듯이, 적합된 ETS 모형의 추세 요소는 모두 가법적으로 고려되었으며, 나머지 요소들은 각 시계열에 따라서 최적의 성분이 자동으로 선택되었습니다. ARIMA 모형 또한 AICC 값을 기준으로 한 최적의 차수들이 반영되어 모형 적합이 잘 이루어진 것으로 보입니다. 이 mable 객체로 우리는 모델 적합 단계에서 필요한 모든 작업을 tidy한 포맷으로 수행할 수 있습니다.\n먼저, coef() 또는 tidy()를 통해 모형으로부터 추정된 계수들을 추출할 수 있습니다. 아울러, 사전에 select() 함수를 통해 특정 모형에 대한 계수 값만을 뽑을 수도 있습니다:\n\nfit %>%\n  select(Region, State, Purpose, arima) %>%\n  coef()\n\n\n\n\n\n  \n\n\n\ntidy로 수행해도 결과는 같습니다. 다음으로 glance()를 이용하면 모형의 적합 결과를 정보 기준(e.g. AIC, BIC)과 잔차의 분산 등으로 요약해줍니다.\n\nfit %>% \n    glance()\n\n\n\n\n\n  \n\n\n\n만약 하나의 모형으로만 시계열 예측 모델링을 수행하고 있다면, report() 함수를 이용하면 됩니다. 이는 하나의 시계열 예측 모형의 평가를 상당히 만족스러운 포맷으로 제공해줍니다.😊 여행 목적이 “Holiday”일 때 ETS 모형을 적합한 결과 대한 요약을 report()를 통해 진행해봤습니다:\n\nfit %>%\n    filter(Purpose == \"Holiday\") %>%\n    select(ets) %>%\n    report()\n\nSeries: Trips \nModel: ETS(M,A,A) \n  Smoothing parameters:\n    alpha = 0.03084501 \n    beta  = 0.03084499 \n    gamma = 0.0001000967 \n\n  Initial states:\n     l[0]      b[0]     s[0]    s[-1]     s[-2]    s[-3]\n 424.0777 -2.535481 -26.7441 4.256618 -10.10668 32.59417\n\n  sigma^2:  0.011\n\n      AIC      AICc       BIC \n 991.7305  994.3020 1013.1688 \n\n\n아울러, 모형으로부터의 적합값과 잔차는 fitted(), residuals() 각각을 이용해 얻을 수 있습니다:\n\nfit %>%\n    fitted()\nfit %>%\n    residuals()\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n적합값과 잔차를 함께 얻고 싶다면 augment()를 사용하세요:\n\nfit %>% \n    augment()\n\n\n\n\n\n  \n\n\n\n모형간 예측 정확도의 비교는 accuracy()를 이용하면 됩니다. 여러 예측 평가 측도를 제공해줍니다:\n\nfit %>% \n    accuracy() %>% \n    arrange(MASE)\n\n\n\n\n\n  \n\n\n\n참고로, 여기서는 훈련 자료(training data)에 대한 예측 성능에 해당합니다. 본 호주 일별 여행객수에 대한 자료에서는 예측 성능 평가 측도를 MASE로 할 경우, ETS 모형이 여행 목적이 “Other”인 경우를 제외하고는 훨씬 더 좋은 성능을 보이고 있습니다. 향후 시점의 예측은 forecast()로 추가적인 자료에 대한 정보 없이 바로 수행을 할 수 있습니다:\n\nfc <- fit %>% \n    forecast(h = \"5 years\")\nfc\n\n\n\n\n\n  \n\n\n\n향후 시점의 예측 결과는 fable(forecast table)로 요약되며, fable은 예측값의 점 추정치와 예측값의 분포에 대한 정보까지 포함하여 제공해줍니다. 예를 들어, 첫 번째 행의 시계열의 예측값의 분포는 평균이 619, 분산이 3533인 정규분포에 해당합니다. 정규분포를 따르는 이유는, 앞서 ETS의 간략한 소개에서 설명했듯이 오차항에 대해 가우스 백색잡음 과정을 가정했기 때문입니다. 그렇다면, 이러한 예측값의 분포에 따른 구간 추정은 어떤 함수로 수행할 수 있을까요? 예측값의 신뢰구간은 hilo()를 이용하면 됩니다. hilo() 함수는 fable 객체와 함께 동작하며, 원하는 신뢰수준을 반영할 수 있게 해줍니다. 다음은 80%, 95% 각각의 신뢰수준에 대한 구간을 추정한 것입니다:\n\nfc %>%\n    hilo(level = c(80, 95))\n\n\n\n\n\n  \n\n\n\n마지막으로, 예측값에 대한 시각화는 fable 객체에 대해 autoplot()을 적용해주면 됩니다:\n\nfc %>% \n  autoplot(tourism_melb)\n\n\n\n\n\n\n\n\n본 튜토리얼에서 소개한 함수들 외에도 {fable}의 특정 모형 객체들과 함께 동작하는 여러 함수들이 있습니다(e.g. refit(), interpolate(), components(), etc). 튜토리얼에서 소개한 내용외에 자세한 내용이 궁금하시다면 Forecasting: Principles and Practices (3rd Ed.)를 참고해주세요."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#fable.prophet",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#fable.prophet",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "5 fable.prophet",
    "text": "5 fable.prophet\n{fable.prophet}은 facebook에서 제안한 단일 시계열 예측모형에 대한 적합 또한 tidy한 인터페이스로 제공해줍니다. prophet은 시계열의 시간 종속적인 특성을 고려하는 기존의 시계열 모형(e.g. 지수평활법, ARIMA 모형)과 달리 curve-fitting(e.g. splines)으로 모형을 적합하며, 시계열을 다음과 같이 세 가지 요소로 분해하고 각 요소를 시간의 함수로 가법적으로 모형화합니다.\n\\[\ny(t) = g(t) + s(t) + h(t) + \\epsilon_t\n\\]\n여기서 \\(g(t)\\)는 비주기적 변화를 모형화하는 추세 함수, \\(s(t)\\)는 주별 또는 연별 계절성과 같은 주기적 변화를 반영하며, \\(h(t)\\)는 불규칙하게 발생할 가능성이 있는 휴일효과(holidays and events effects)를 모형화합니다. 세 요소 중에서도 휴일효과에 대한 반영이 prophet의 상당히 특징적인 부분이라 할 수 있겠으며, 모형에서 조절할 수 있는 모수들이 상당히 많아서 아주 유연하고 디테일하게 모델링이 가능합니다. 도메인 지식이 풍부할수록 prophet을 통한 성능 개선의 가능성은 무궁무진합니다. 본 튜토리얼에서 prophet에 대한 개념 설명은 이쯤에서 간략하게 마치겠습니다. prophet을 이번에 처음 접하시는 분들은 여기를 참고해주시기 바랍니다. 개념 정리와 R을 이용한 튜토리얼 과정을 정리해놓았는데, 여기서 소개할 tidy한 인터페이스의 이해를 위해서 꼭 필요로 될겁니다.\n본 튜토리얼에서 prophet을 이용한 예측 모델링에 이용할 자료는 호주의 카페, 레스토랑 및 케이터링 서비스에 관한 월 매출액 자료(단위: milions $AUD)입니다:\n\ncafe <- tsibbledata::aus_retail %>%\n    filter(Industry == \"Cafes, restaurants and catering services\")\nautoplot(cafe)\n\nPlot variable not specified, automatically selected `.vars = Turnover`\n\n\n\n\n\n\n\n\n\n주별로 나뉜 해당 자료의 각 계열은 증가하는 추세와 그에 따른 연별 계절 패턴이 눈에 보입니다. 또한, 계절 패턴의 경우 계열의 수준(level)에 비례하는 형태를 보이고 있으므로, 계절성을 승법적으로 고려해야할 것입니다. 아울러, 월별 자료의 경우는 휴일 효과의 경우 계절 요소를 통해 모형화가 가능합니다. 휴일효과에 대한 반영은 이번에 진행하지 않을 예정입니다(기존의 prophet 인터페이스에서 수행했던 것과 같이 간단하게 반영, see here for details). 본 자료에 대해 추세 요소는 선형으로 하여(default), 연별 계절성을 승법으로 고려하여 prophet을 적합해보았습니다:\n\nfit <- cafe %>%\n  model(\n    prophet = prophet(Turnover ~ season(\"year\", 4, type = \"multiplicative\"))\n  )\nfit\n\n\n\n\n\n  \n\n\n\n각 계열에 대해 prophet이 잘 적합된 것을 확인할 수 있습니다. 적합된 모형의 각 요소들은 components()로 추출할수 있습니다:\n\ncomponents(fit)\n\n\n\n\n\n  \n\n\n\ncomponents()를 통해 주어지는 객체 자체에 autoplot()을 수행하면 모든 요소에 대한 시각화가 한꺼번에 가능하지만, 추세와 월별 계절 패턴에 대해서만 시각화해보겠습니다.\n\ncomponents(fit) %>%\n  ggplot(aes(x = Month, y = trend, colour = State)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\ncomponents(fit) %>%\n  ggplot(aes(x = month(Month), y = year, \n             colour = State, group = interaction(year(Month), State))) + \n  geom_line() + \n  scale_x_continuous(breaks = 1:12, labels = month.abb) + \n  xlab(\"Month\")\n\n\n\n\n\n\n\n\n연별 계절패턴의 경우 주별로 대개 비슷하나, 북방 지역(the Northern Territory)의 경우 다른 주들과는 크게 다른 계쩔 패턴을 보여주고 있습니다. 마지막으로, prophet의 예측도 forecast()를 이용해 쉽게 수행할 수 있습니다. 향후 2년에 대해 예측해보았습니다:\n\nfc <- fit %>% \n  forecast(h = 24)\ncafe %>% \n  ggplot(aes(x = Month, y = Turnover, colour = State)) + \n  geom_line() + \n  autolayer(fc)\n\n\n\n\n\n\n\n\nForecasting: Principles and Practices (3rd Ed.)에서는 prophet외에도, 벡터 자기회귀모형, 인공신경망 기반의 시계열 예측모형, 붓스트랩 및 배깅 기법을 활용한 시계열 예측 모형 등의 고급 시계열 예측 모형도 제공해줍니다. 관심있으신 분들은 fpp3을 참고해보시기 바랍니다."
  },
  {
    "objectID": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#맺음말",
    "href": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/index.html#맺음말",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "section": "맺음말",
    "text": "맺음말\ntidyverts ecosystem이 전반적으로 작동하는 과정을 소개해 보았습니다. 그러나, 시계열 자료의 예측 모델링 대한 이해와 더불어 tidyverts를 좀 더 디테일하게 활용하기 위해서는, Forecasting: Principles and Practices (3rd Ed.)을 참고하시는게 좋을 것이라 생각합니다. tidyverse와 tidymodels를 통해 데이터를 전처리, 예측모형 개발, 개선 등의 과정에 걸리는 시간을 크게 단축시켰듯이, fpp3을 잘 익혀두면 시계열 예측 모델링에 전반적인 과정에 드는 시간을 상당히 단축시킬 수 있을 겁니다.😊\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version date (UTC) lib source\n dplyr         * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n fable         * 0.3.1   2021-05-16 [1] CRAN (R 4.2.0)\n fable.prophet * 0.1.0   2020-08-20 [1] CRAN (R 4.2.0)\n fabletools    * 0.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n feasts        * 0.2.2   2021-06-03 [1] CRAN (R 4.2.0)\n fpp3          * 0.4.0   2021-02-06 [1] CRAN (R 4.2.0)\n ggplot2       * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n lubridate     * 1.8.0   2021-10-07 [1] CRAN (R 4.2.0)\n nycflights13  * 1.0.2   2021-04-12 [1] CRAN (R 4.2.0)\n purrr         * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n Rcpp          * 1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rmarkdown     * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n sessioninfo   * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble        * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr         * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tsibble       * 1.1.1   2021-12-03 [1] CRAN (R 4.2.0)\n tsibbledata   * 0.4.0   2022-01-07 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "",
    "text": "Photo by Makcus Wincler on Unsplash\ntidymodels ecosystem은 R에서 머신러닝을 tidyverse principle로 수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidymodels은 {caret}1을 완벽하게 대체하며, 더 빠르게 그리고 더 직관적인 코드로 모델링을 수행할 수 있습니다. {tidymodels}는 모델링에 필요한 패키지들의 묶음이라고 보면 됩니다. {tidyverse}처럼 {tidymodels}를 로딩하면 모델링에 쓰이는 여러 패키지의 묶음을 불러와줍니다. 그중에는 {ggplot2}와 {dplyr} 같은 {tidyverse}에 포함되는 패키지들도 있습니다. 본격적으로 튜토리얼을 시작하기 전에 필요한 패키지와 데이터를 먼저 불러오겠습니다.\n본 튜토리얼에서 이용할 toy data는 `diamonds{ggplo2}`💎입니다. 해당 데이터는 다이아몬드의 등급과 크기 및 가격에 관한 정보를 갖습니다:\n다음은 우리가 모델링에 사용할 features(\\(X\\))들의 상관계수 행렬을 시각화 한 것이며, 상관계수 행렬을 다이아몬드의 가격(price, \\(y\\)) 열의 상관계수의 절댓값을 기준으로 내림차순 정렬하여 그린 것입니다.\ntoy data를 이용해 {tidymodels}의 전반적인 진행 과정을 보여주는 예제이기 때문에, 상관계수 행렬 그림은 전체 데이터가 아닌 2,000개만을 샘플링하여 그렸습니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#데이터-분할-rsample",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#데이터-분할-rsample",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "1 데이터 분할: {rsample}",
    "text": "1 데이터 분할: {rsample}\ntidymodels ecosystem을 구성하는 패키지들 중 가장 먼저 소개할 친구는 데이터 분할에 쓰이는 {rsample}입니다. 본 예제의 마지막 단계에서 시험 자료(test data)를 기반으로 모형의 예측 성능을 평가할 것이기 때문에, 먼저 데이터를 훈련 자료(training data), 시험 자료로 분할해야 합니다. 이번에도 모형 적합 및 교차 검증을 이용한 모수 튜닝 단계에서의 계산 비용 절감을 위해, 훈련 자료의 비율을 10%로 낮게 잡아 데이터를 나눌 것입니다. 다음의 모든 과정은 {rsample} 패키지의 함수들로 진행됩니다. 패키지 또는 함수의 이름이 직관적이고 인간 친화적이면 그 역할을 기억하기 쉬운데, 앞으로 소개할 {tidymodels}를 구성하는 패키지와 패키지를 이루는 함수들의 이름은 대부분 이러한 점을 고려하여 네이밍이 되어있습니다.😊\n\nset.seed(1)\ndia_split <- initial_split(diamonds, prop = .1, strata = price)\ndia_train <- training(dia_split)\ndia_test <- testing(dia_split)\ncat(\"the number of observations in the training set is \", \n    nrow(dia_train), \n    \".\\n\",\n     \"the number of observations in the test set is \", \n    nrow(dia_test), \".\", \n    sep = \"\")\n\nthe number of observations in the training set is 5393.\nthe number of observations in the test set is 48547."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#데이터-전처리-및-feature-engineering-recipes",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#데이터-전처리-및-feature-engineering-recipes",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "2 데이터 전처리 및 Feature Engineering: {recipes}",
    "text": "2 데이터 전처리 및 Feature Engineering: {recipes}\n다음으로는 {recipes}를 이용하여, 데이터 전처리 및 Feature Engineering을 수행한다. recipe는 요리법이라는 뜻뿐만 아니라 특정 결과를 가져올 듯한 방안의 뜻2도 갖습니다. 이럴 때마다 영어권의 R 유저들이 부럽습니다. 패키지나 함수 이름을 통해 그 역할을 기억하고 필요할 때 꺼내쓰기가 좀 더 편하지 않을까 하는 생각이 드네요. {recipes}의 step_*() 함수들을 이용해 모델링에 사용할 자료를 준비3할 수 있습니다. 다음의 산점도는 다이아몬드의 가격(price)과 carat 사이에 비선형적인 관계가 있음을 암시하며, 이러한 관계는 carat의 다항함수를 변수로 도입하여 모델링에 반영할 수 있습니다.\n\nqplot(carat, price, data = dia_train) +\n  scale_y_continuous(trans = log_trans(), labels = function(x) round(x, -2)) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x, 4)\") +\n  labs(title = \"The degree of the polynomial is a potential tuning parameter\")\n\n\n\n\n\n\n\n\nrecipe()는 자료와 모형식을 인수로 하며, step_*() 함수들을 이용하여 step by step👞으로 다양한 전처리를 수행할 수 있게끔 해줍니다.4 여기서는 \\(y\\)에 로그 변환(step_log())을 수행하고, 연속형 예측변수5에 표준화(중심화 및 척도화, step_normalize()), 범주형 예측변수는 더미 변수화(step_dummy())를 수행합니다. 그리고, step_poly()를 이용해 carat의 2차 효과를 반영해주었습니다. 준비가 끝난 recipe 객체는 prep() 함수를 통해 자료에 수행된 전처리들을 확인할 수 있다.\n\ndia_rec <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = 2)\nprep(dia_rec)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nTraining data contained 5393 data points and no missing data.\n\nOperations:\n\nLog transformation on price [trained]\nCentering and scaling for carat, depth, table, x, y, z [trained]\nDummy variables from cut, color, clarity [trained]\nOrthogonal polynomials on carat [trained]\n\n\nrecipe 객체에 prep()를 적용한 것에 juice()를 수행하면 전처리가 수행된 자료를 추출할 수 있죠.\n\ndia_juiced <- juice(prep(dia_rec))\nglimpse(dia_juiced)\n\nRows: 5,393\nColumns: 25\n$ depth        <dbl> 0.52494063, -0.86779062, -0.51960781, 0.59457719, 0.24639…\n$ table        <dbl> -0.2037831, 1.5902131, 0.6932150, -0.2037831, -0.6522821,…\n$ x            <dbl> -1.5716610, -1.5805830, -1.2772351, -1.3218451, -1.277235…\n$ y            <dbl> -1.6114895, -1.5845446, -1.2612061, -1.3061143, -1.261206…\n$ z            <dbl> -1.5470415, -1.6483712, -1.3154309, -1.2575282, -1.243052…\n$ price        <dbl> 5.872118, 5.877736, 6.003887, 6.003887, 6.313548, 6.31716…\n$ cut_1        <dbl> 3.162278e-01, -1.481950e-18, 6.324555e-01, -1.481950e-18,…\n$ cut_2        <dbl> -0.2672612, -0.5345225, 0.5345225, -0.5345225, 0.5345225,…\n$ cut_3        <dbl> -6.324555e-01, -3.893692e-16, 3.162278e-01, -3.893692e-16…\n$ cut_4        <dbl> -0.4780914, 0.7171372, 0.1195229, 0.7171372, 0.1195229, 0…\n$ color_1      <dbl> 3.779645e-01, -5.669467e-01, 3.779645e-01, 3.779645e-01, …\n$ color_2      <dbl> -5.621884e-17, 5.455447e-01, -5.621884e-17, -5.621884e-17…\n$ color_3      <dbl> -4.082483e-01, -4.082483e-01, -4.082483e-01, -4.082483e-0…\n$ color_4      <dbl> -0.5640761, 0.2417469, -0.5640761, -0.5640761, 0.2417469,…\n$ color_5      <dbl> -4.364358e-01, -1.091089e-01, -4.364358e-01, -4.364358e-0…\n$ color_6      <dbl> -0.19738551, 0.03289758, -0.19738551, -0.19738551, 0.0328…\n$ clarity_1    <dbl> 0.07715167, -0.07715167, -0.38575837, -0.23145502, -0.231…\n$ clarity_2    <dbl> -0.38575837, -0.38575837, 0.07715167, -0.23145502, -0.231…\n$ clarity_3    <dbl> -0.1846372, 0.1846372, 0.3077287, 0.4308202, 0.4308202, -…\n$ clarity_4    <dbl> 0.3626203, 0.3626203, -0.5237849, -0.1208734, -0.1208734,…\n$ clarity_5    <dbl> 0.3209704, -0.3209704, 0.4921546, -0.3637664, -0.3637664,…\n$ clarity_6    <dbl> -0.30772873, -0.30772873, -0.30772873, 0.55391171, 0.5539…\n$ clarity_7    <dbl> -0.59744015, 0.59744015, 0.11948803, -0.35846409, -0.3584…\n$ carat_poly_1 <dbl> -0.01605633, -0.01634440, -0.01432792, -0.01432792, -0.01…\n$ carat_poly_2 <dbl> 0.017042209, 0.017792818, 0.012731517, 0.012731517, 0.012…\n\n\n또한, recipe 객체에 prep()를 적용한 것에 juice()가 아닌 bake()를 수행하면 새로운 자료에 recipe 객체에 수행했던 것과 같은 전처리를 수행할 수 있습니다. 예를 들어, 다음은 시험 자료에 대해 훈련 자료에 수행한 전처리를 수행한 뒤에 해당 자료를 추출하라는 것과 같죠. 시험 자료의 예측을 통한 모형의 성능평가에는 사전에 훈련자료와 동일한 전처리가 필요로되는데, bake()는 이러한 시간을 크게 단축시켜줍니다.\n\nglimpse(\n  bake(prep(dia_rec), dia_test)\n)\n\nRows: 48,547\nColumns: 25\n$ depth        <dbl> -0.1714250, -1.3552466, -3.3747069, 0.4553041, 1.0820331,…\n$ table        <dbl> -1.1007812, 1.5902131, 3.3842094, 0.2447160, 0.2447160, -…\n$ x            <dbl> -1.589505, -1.643037, -1.500285, -1.366455, -1.241547, -1…\n$ y            <dbl> -1.575563, -1.701306, -1.494728, -1.351022, -1.243243, -1…\n$ z            <dbl> -1.604944, -1.778652, -1.778652, -1.315431, -1.141723, -1…\n$ price        <dbl> 5.786897, 5.786897, 5.789960, 5.811141, 5.814131, 5.81711…\n$ cut_1        <dbl> 6.324555e-01, 3.162278e-01, -3.162278e-01, 3.162278e-01, …\n$ cut_2        <dbl> 0.5345225, -0.2672612, -0.2672612, -0.2672612, -0.2672612…\n$ cut_3        <dbl> 3.162278e-01, -6.324555e-01, 6.324555e-01, -6.324555e-01,…\n$ cut_4        <dbl> 0.1195229, -0.4780914, -0.4780914, -0.4780914, -0.4780914…\n$ color_1      <dbl> -3.779645e-01, -3.779645e-01, -3.779645e-01, 3.779645e-01…\n$ color_2      <dbl> 8.914347e-17, 8.914347e-17, 8.914347e-17, -5.621884e-17, …\n$ color_3      <dbl> 4.082483e-01, 4.082483e-01, 4.082483e-01, -4.082483e-01, …\n$ color_4      <dbl> -0.5640761, -0.5640761, -0.5640761, -0.5640761, 0.2417469…\n$ color_5      <dbl> 4.364358e-01, 4.364358e-01, 4.364358e-01, -4.364358e-01, …\n$ color_6      <dbl> -0.19738551, -0.19738551, -0.19738551, -0.19738551, 0.032…\n$ clarity_1    <dbl> -0.38575837, -0.23145502, 0.07715167, -0.07715167, -0.385…\n$ clarity_2    <dbl> 0.07715167, -0.23145502, -0.38575837, -0.38575837, 0.0771…\n$ clarity_3    <dbl> 0.3077287, 0.4308202, -0.1846372, 0.1846372, 0.3077287, -…\n$ clarity_4    <dbl> -0.5237849, -0.1208734, 0.3626203, 0.3626203, -0.5237849,…\n$ clarity_5    <dbl> 0.4921546, -0.3637664, 0.3209704, -0.3209704, 0.4921546, …\n$ clarity_6    <dbl> -0.30772873, 0.55391171, -0.30772873, -0.30772873, -0.307…\n$ clarity_7    <dbl> 0.11948803, -0.35846409, -0.59744015, 0.59744015, 0.11948…\n$ carat_poly_1 <dbl> -0.01634440, -0.01692054, -0.01634440, -0.01461599, -0.01…\n$ carat_poly_2 <dbl> 0.01779282, 0.01932160, 0.01779282, 0.01342699, 0.0120452…"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#모형-정의-및-적합-parsnip",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#모형-정의-및-적합-parsnip",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "3 모형 정의 및 적합: {parsnip}",
    "text": "3 모형 정의 및 적합: {parsnip}\n이제 훈련 자료에 대한 기본적인 전처리가 끝났으므로, {parsnip}을 이용하여 모형을 정의하고 적합하려고 합니다. {parsnip}은 우리나라 말로 연노란색의 긴 뿌리채소를 뜻하는데, 왜 이렇게 네이밍이 된 지는 아직 잘 모르겠습니다. 영어권의 원어민들은 어떻게 생각할지 궁금하네요. {parsnip}은 인기 있는 수많은 머신러닝 알고리즘6을 제공해줍니다. 그리고, 최대 장점은 단일화된 인터페이스로 여러 모형을 적합할 수 있다는 점이죠. 예를 들어, 랜덤포레스트를 제공하는 두 패키지 {ranger}와 {randomForest}에는 고려할 트리의 개수를 지정하는 모수가 존재하는데 해당 옵션의 이름이 각각 ntree, num.trees로 다릅니다. 이는 사용자들에게 꽤 불편한 점일 수 있는데, {parsnip}은 이러한 문제를 해결해줌으로써 두 인터페이스를 모두 기억할 필요가 없게끔 해줍니다.\n{parsnip}에서는 먼저 특정 함수를 통해 모형을 정의하고7, set_mode()로 어떤 문제8를 해결할 것인지 설정한 뒤에, 마지막으로 어떤 시스템 또는 패키지를 이용하여 해당 모형을 적합할지를 set_engine()으로 설정합니다. 여기서는 먼저 stats::lm() 엔진을 이용하여 기본적인 회귀모형으로 적합을 시작해 보겠습니다.\n\nlm_model <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\n본격적인 모형 적합 전에, 앞서 언급했던 {parsnip}의 장점을 확인해보기 위해 랜덤포레스트를 예로 들어보겠습니다. 랜덤포레스트 모형의 적합에는 {ranger} 또는 {randomForest}를 이용할 수 있는데, 서로 조금 다른 인터페이스를 지닌다고 했었습니다. {parsnip}은 다음과 같이 엔진 설정 전에 {parsnip}만의 함수로 먼저 모형을 정의하고 해당 함수에서 모수를 설정함으로써 서로 다른 인터페이스를 통합하여줍니다.\n\nrand_forest(mtry = 3, trees = 500, min_n = 5) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\", importance = \"impurity_corrected\")\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 500\n  min_n = 5\n\nEngine-Specific Arguments:\n  importance = impurity_corrected\n\nComputational engine: ranger \n\n\n이제 다시 회귀모형으로 돌아오겠습니다. 설정했던 기본적인 회귀모형을 전처리를 완료한 훈련 자료에 적합해 줍니다.\n\nlm_fit1 <- fit(lm_model, price ~ ., dia_juiced)\nlm_fit1\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = price ~ ., data = data)\n\nCoefficients:\n (Intercept)         depth         table             x             y  \n   7.7110881     0.0582418     0.0138979     0.8357574     0.2337963  \n           z         cut_1         cut_2         cut_3         cut_4  \n   0.0532288     0.1134826    -0.0282144     0.0315527    -0.0020513  \n     color_1       color_2       color_3       color_4       color_5  \n  -0.4452258    -0.0887138    -0.0090620     0.0071217    -0.0059503  \n     color_6     clarity_1     clarity_2     clarity_3     clarity_4  \n  -0.0001745     0.9025208    -0.2480065     0.1424917    -0.0664178  \n   clarity_5     clarity_6     clarity_7  carat_poly_1  carat_poly_2  \n   0.0265924     0.0031308     0.0245773    -3.1129423    -6.9995161  \n\n\n예제에서 사용되진 않았지만, step_rm()을 이용하여 사전에 모델링에 필요 없는 변수는 제거할 수도 있습니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#적합된-모형-요약-broom",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#적합된-모형-요약-broom",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "4 적합된 모형 요약: {broom}",
    "text": "4 적합된 모형 요약: {broom}\nR에서 여러 모형 객체들의 요약은 summary() 또는 coef()와 같은 함수로 이루어집니다. 그러나, 이러한 함수들의 출력물은 타이디한 포맷9으로 주어지지 않습니다. {broom} 패키지는 적합 된 모형의 요약을 타이디한 포맷으로 제공해줍니다. broom은 빗자루와 같은 브러쉬를 의미하는 명사인데, 적합한 모형을 깨끗하게 쓸어 담는 패키지라고 생각하면 기억하기 쉽지 않을까 싶습니다. 이와 같이 패키지 이름, 함수 이름 하나하나를 신중하게 네이밍하는 일관성은 {tidyverse}, {tidymodels}에 포함되는 패키지들의 공통된 좋은 특징이라 할 수 있다. 실제로 R4DS10 책에서도 Hadley Wickham은 객체의 이름이나 함수의 이름을 설정하는 것에 있어서 어느정도의 시간을 투자하는 것은 전혀 아깝지 않다고 말하기도 했습니다.\n{broom} 패키지를 구성하는 첫 번째 함수로 glance()를 소개합니다. glance는 힐끗 본다는 뜻을 갖는다는 점에서 추측할 수 있듯이, 적합된 모형의 전체적인 정보를 간략히 제공해줍니다.\n\nglance(lm_fit1$fit)\n\n\n\n\n\n  \n\n\n\n적합된 모형의 수정된 \\(R^2\\) 값(adj.r.squared)은 약 98.27%로 상당히 높은 설명력을 보여줍니다. RMSE는 sigma 열에서 확인할 수 있습니다. 다음으로 tidy()는 추정된 모수에 대한 정보를 제공합니다. 다음의 결과에서 우리는 carat의 2차 효과가 유의하게 존재함을 알 수 있습니다. 통계량의 크기를 기준으로 내림차순으로 정렬하여 표시하였습니다.\n\ntidy(lm_fit1) %>% \n    arrange(desc(abs(statistic)))\n\n\n\n\n\n  \n\n\n\n마지막으로 augment()는 모형의 예측값, 적합값 등을 반환해줍니다. augment는 우리나라 말로 어떤 것의 양 또는 값, 크기 등을 늘리는 것11을 뜻하는 동사로, 해당 함수도 이름을 통해 어느정도 그 역할을 가늠할 수 있죠.\n\nlm_predicted <- augment(lm_fit1$fit, data = dia_juiced) %>% \n  rowid_to_column()\nselect(lm_predicted, rowid, price, .fitted:.std.resid)\n\n\n\n\n\n  \n\n\n\n앞서 생성한 lm_predicted 객체를 이용해 적합값과 관측값 간의 산점도를 그려보았습니다. 잔차의 크기가 2 이상인 관측치에 대해서는 해당 관측치의 행 번호를 붙여주었으며, 겹치는 점이 있는 경우를 고려하여 점에 투명도를 주었습니다.\n\nggplot(lm_predicted, aes(.fitted, price)) +\n  geom_point(alpha = .2) +\n  ggrepel::geom_label_repel(aes(label = rowid),\n                            data = lm_predicted %>% filter(abs(.resid) > 2)) +\n  labs(x = \"fitted values\",\n       y = \"observed values\")\n\n\n\n\n\n\n\n\n원자료의 각 행을 의미하는 두 단어 관측값(observed values)과 실제값(actual values)은 서로 통용되니 어떤 용어를 써도 문제가 없습니다. 특히, 머신러닝에서는 이를 데이터포인트(data point)라고 표현하기도 합니다. 3가지 용어 모두 통용되는 말이니 몰랐다면 알아둡시다. 모든 학문에서 그렇겠지만 통계학에서는 특히 정확한 용어 정의가 중요하므로, 비슷한 용어 또는 비슷한 듯 다른 용어들이 있다면 틈틈이 정리하는 습관을 갖는 것이 좋다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#모형-성능-평가-yardstick",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#모형-성능-평가-yardstick",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "5 모형 성능 평가: {yardstick}",
    "text": "5 모형 성능 평가: {yardstick}\n위에서 glance()를 통해 적합된 모형의 성능을 RMSE, \\(R^2\\)를 통해 힐끗 확인할 수 있었습니다. {yardstick}은 모형의 성능에 대한 여러 측도를 계산하기 위한 패키지입니다. 물론, \\(y\\)가 연속형이든 범주형이든 문제없으며 교차 검증(Cross Validation, CV)에서 생산되는 그룹화된 예측값들과도 매끄럽게 잘 작동한다. yardstick은 기준, 척도를 뜻하는 명사에 해당하므로, 기억하기도 쉬울 것이라 생각합니다. 이제는 {rsample}, {parsnip}, {yardstick}으로 교차 검증을 수행하여 좀 더 정확한 RMSE를 추정해봅시다.\n다음 코드 블럭들에서 나타날 긴 파이프라인(pipeline, %>%)들을 정리해서 간략히 나타내면 다음과 같습니다. 천천히 음미해보시기 바랍니다:\n\nrsample::vfold_cv()를 훈련용 자료를 3-fold CV를 수행할 수 있도록 분할\nrsample::analysis()와 rsample::assessment()를 이용해 각 분할에서 모형 훈련용, 평가용 자료를 불러옴\n앞서 만든 모형 적합 전 전처리가 완료된 recipe 객체 dia_rec을 각 fold의 모형 훈련용 자료에 prepped 시킴\npreped한 훈련용 자료를 recipes::juice()로 불러오고, recipes::bake()를 이용해 훈련용 자료에 처리한 것과 같은 처리를 평가용 자료에 수행\nparsnip::fit()으로 3개의 모형 적합용(analysis) 자료 각각에 모형을 적합(훈련)\npredicted()로 훈련시킨 각 모형으로 평가용(assessment) 자료를 예측\n\n\nset.seed(1)\ndia_vfold <- vfold_cv(dia_train, v = 3, strata = price)\ndia_vfold\n\n\n\n\n\n  \n\n\n\n\nlm_fit2 <- mutate(dia_vfold,\n                  df_ana = map(splits, analysis),\n                  df_ass = map(splits, assessment))\nlm_fit2\n\n\n\n\n\n  \n\n\n\n\nlm_fit3 <- lm_fit2 %>% \n  mutate(\n    recipe = map(df_ana, ~prep(dia_rec, training = .x)),\n    df_ana = map(recipe, juice),\n    df_ass = map2(recipe,\n                  df_ass, ~bake(.x, new_data = .y))) %>% \n  mutate(\n    model_fit = map(df_ana, ~fit(lm_model, price ~ ., data = .x))) %>% \n  mutate(\n    model_pred = map2(model_fit, df_ass, ~predict(.x, new_data = .y)))\nselect(lm_fit3, id, recipe:model_pred)\n\n\n\n\n\n  \n\n\n\n여기서 tidymodels ecosystem의 마법을 확인할 수 있습니다. 위 과정에서 확인했다시피, 꽤 복잡한 과정들이 단 하나의 티블 객체 lm_fit2에서 이루어졌습니다. 이렇게 복잡한 작업이 단 하나의 티블 객체만으로 이루어질 수 있었던 이유는, 티블은 리스트-열(list-column)을 가질 수 있기 때문이죠. 덕분에 우리는 R에서 연산이 느린 반복문(e.g. for(), while())을 사용하지 않고, purrr::map()을 loop로 이용하여 반복문을 통한 지루하고 느린 모델링 작업을 완벽한 함수형 프로그래밍으로 수행할 수 있게 되었습니다. R 사용자라면 어디서 한번 쯤은 반복문의 사용은 지양하고, 함수형 프로그래밍을 해야 한다고 들어봤을 것입니다. {tidymodels}이 모델링 과정을 {tidyverse}와 함께 작동할 수 있게 해줌으로써, 한 자료에 대해서 여러 가지 모형의 적합, 교차검증을 통한 모수 튜닝, 예측 성능평가 등의 작업을 통해 경험적으로(empirically) 최적의 모형을 선택하는 수고가 필요한 머신러닝에 드는 시간을 상당히 줄여줬다고 할 수 있습니다.\n이쯤 되면 제가 왜 {tidyverse}를 좋아하고, {tidymodels}의 튜토리얼을 이렇게 상세하게 기술하는지 이해하실 거라고 생각합니다. 이제 평가용 자료로부터 실제 관측값(price)을 추출하여 예측값(.pred)과 비교한 뒤, yardstick::metrics()를 이용해 여러 평가 측도를 계산해보려고 합니다.\n\nlm_preds <- lm_fit3 %>% \n  mutate(res = map2(df_ass, model_pred, ~data.frame(price = .x$price, \n                                                    .pred = .y$.pred))) %>% \n  select(id, res) %>% \n  tidyr::unnest(res) %>% \n  group_by(id)\nlm_preds\n\n\n\n\n\n  \n\n\n\n\nmetrics(lm_preds, truth = price, estimate = .pred)\n\n\n\n\n\n  \n\n\n\n여기서 계산한 평가 측도의 값은 out-of-sample에 대한 성능이므로 모형 적합값에 대해 평가 측도를 계산한 glance(lm_fit1$fit)의 결과와 비교하여 보면 당연히 조금은 떨어지는 성능을 보입니다. metrics()는 연속형 outcome(\\(y\\))에는 위와 같이 RMSE, \\(R^2\\), MAE를 기본적인 측도로 제공해줍니다. 물론, 범주형 outcome에 대해서도 다른 기본적인 측도를 제공해주죠. 또한, 하나의 측도만으로 비교하길 원한다면 rmse()와 같이 RMSE 값만을 제공해주는 함수도 이용할 수 있으며, metric_set()을 이용하면 원하는 metrics들을 직접 커스텀하여 정의할 수도 있습니다.\n3-fold CV를 통해 훈련 자료를 분할 및 전처리하고 예측값을 구하여 RMSE를 계산하는 과정을 담은 앞선 코드블럭들은 {tidyverse}, {tidymodels}에 익숙한 사람이라면 편하게 읽어나가실 수 있을겁니다. 그러나, 코드가 매우 긴 것도 사실입니다. 사실, 위 코드블럭은 다음 섹션에서 소개할 {tune} 패키지를 이용하면 다음과 같이 단 몇 줄로 간결하게 코딩할 수 있습니다.\n\ncontrol <- control_resamples(save_pred = TRUE)\nset.seed(1)\nlm_fit4 <- fit_resamples(lm_model, dia_rec, dia_vfold, control = control)\nlm_fit4 %>% \n    pull(.metrics)\n\n\n\n[[1]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.143 Preprocessor1_Model1\n2 rsq     standard       0.980 Preprocessor1_Model1\n\n[[2]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.127 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1\n\n[[3]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.130 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#모형의-모수-튜닝-tune-dials",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#모형의-모수-튜닝-tune-dials",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "6 모형의 모수 튜닝: {tune}, {dials}",
    "text": "6 모형의 모수 튜닝: {tune}, {dials}\ntune은 조정하다12 라는 뜻을 갖는 동사이며, 말 그대로 {tune} 패키지는 모수를 튜닝(조율)하는(e.g. via grid search) 함수들을 제공합니다. 그리고, 어떤 것을 조정하는 다이얼13을 의미하는 이름을 갖는 {dials} 패키지는 {tune}을 통해 튜닝할 모수들을 정하는 역할을 합니다. 즉, {tune}과 {dials}는 대개 함께 쓰이는 패키지라고 보면 됩니다. 본 예제에서는 랜덤포레스트 모형을 튜닝하는 과정을 보여줄 것입니다.\n\n6.1 튜닝을 위한 {parsnip} 모형 객체 준비\n첫 번째로, 랜덤포레스트 모형을 형성할 때 매 트리 적합시 고려할 변수들의 개수를 조정하는 mtry 모수를 조율해줍니다. tune()을 placeholder로 하여 후에 교차검증을 통해 최적의 mtry를 선정할 입니다.\n다음 코드블럭의 출력물은 mtry의 기본 최솟값은 1이고 최댓값은 자료에 의존함을 의미합니다. 어떤 자료를 다루느냐에 따라 feature의 수는 다르므로, 따로 지정하지 않는한 mtry의 최댓값은 자료에 의존하게 됩니다.\n\nrf_model <- rand_forest(mtry = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\nparameters(rf_model)\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[?]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\nmtry()\n\n# Randomly Selected Predictors (quantitative)\nRange: [1, ?]\n\n\n아직 랜덤포레스트 모형의 적합에 쓰이는 모수 값을 결정하지 않았으므로 모형을 훈련 자료에 적합할 준비가 된 상태가 아니라고 할 수 있습니다. 그리고, mtry의 최댓값은 update()를 사용해 원하는 값을 명시할 수도 있고, 또는 finalize()를 사용해 해당 자료가 갖는 예측변수의 수로 지정할 수도 있죠.\n\nrf_model %>% \n  parameters() %>% \n  update(mtry = mtry(c(1L, 5L)))\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[+]\n\n\n\nrf_model %>% \n  parameters() %>% \n  finalize(x = juice(prep(dia_rec)) %>% select(-price)) %>% \n  pull(\"object\")\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, 24]\n\n\n\n\n6.2 튜닝을 위한 자료 준비: {recipes}\n두 번째로 튜닝하고 싶은 것은 carat의 다항식 차수입니다. 2 데이터 전처리 및 Feature Engineering: {recipes}의 그림에서 확인했듯이, 최대 4차까지의 다항식이 자료에 잘 적합 될 수 있음을 알 수 있습니다. 그러나, 우리는 모수 절약의 원칙(priciplt of parsimony)14을 생각할 필요가 있고, 그에 따라 더 간단한 모형도 자료에 잘 적합 될 수 있다는 가능성을 배제해서는 안됩니다. 그래서, carat의 다항식 차수 또한 교차 검증을 통해 최대한 간단하면서 좋은 성능을 내는 carat의 차수를 찾을 것입니다.\n모형의 적합에서 각 모형이 갖는 고유한 초모수15와 달리 예측변수 carat의 차수는 {recipe}를 통해 새로운 레시피 객체를 만들어 튜닝이 진행됩니다. 그 과정은 초모수를 튜닝했던 과정과 유사합니다. 다음과 같이 step_poly()에 tune()을 사용하여 훈련 자료(dia_train())에 대한 2번째 레시피 객체를 만들어 줍니다.\n\ndia_rec2 <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = tune())\n\ndia_rec2 %>% \n  parameters() %>% \n  pull(\"object\")\n\nWarning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\n\n[[1]]\nPolynomial Degree (quantitative)\nRange: [1, 3]\n\n\n고려하는 다항식의 차수 범위가 기본값으로 설정하여 [1, 3]으로 되어있는데, 이 부분은 다음 섹션에서 {workflows} 패키지를 소개하며 개선할 부분이니 신경 쓰지 않으셔도 됩니다.\n\n\n6.3 모든 것을 결합하기: {workflows}\nworkflow를 직역하면 어떤 작업의 흐름을 뜻하듯이, {workflows} 패키지는 recipe나 model 객체와 같은 머신러닝 파이프라인의 다른 부분이라 할 수 있는 것들을 한 번에 묶어주는 역할을 합니다.\n이를 위해서는 먼저 workflow()를 선언하여 객체를 만들고, 6.2 튜닝을 위한 자료 준비: {recipes}에서 만든 recipe 객체와 6.1 튜닝을 위한 {parsnip} 모형 객체 준비에서 만든 랜덤포레스트 모형 객체를 add_*()로 결합해줍니다.\n\nrf_wflow <- workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(dia_rec2)\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_dummy()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger \n\n\n아직 mtry의 최댓값이 알려져있지 않고 degree의 최댓값이 기본 설정인 3으로 설정되어 있으므로, 두 번째로는 rf_wflow 객체의 모수 설정을 update()로 갱신할 것입니다.\n\nrf_param <- rf_wflow %>% \n  parameters() %>% \n  update(mtry = mtry(range = c(3L, 5L)),\n         degree = degree_int(range = c(2L, 4L)))\n\nWarning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\nPlease use `hardhat::extract_parameter_set_dials()` instead.\n\nrf_param %>% pull(\"object\")\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [3, 5]\n\n[[2]]\nPolynomial Degree (quantitative)\nRange: [2, 4]\n\n\n앞서 말했듯이 교차검증을 통해 튜닝을 수행할 것이기 때문에, 세 번째로는 설정한 모수들의 조합을 만들어야 합니다. 복잡한 튜닝 문제에는 tune_bayes()를 통한 베이지안 최적화(Bayesian optimization)(Silge and Julia, n.d.)가 추천되지만, 해당 예제에서 고려하는 초모수들의 조합 정도는 grid search로도 충분해 보입니다. 다음과 같이 필요로 되는 모든 모수 조합의 grid를 만듭니다.\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid\n\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid %>% \n    paged_table()\n\n\n\n  \n\n\n\n여기서 levels는 grid를 만드는 데 사용되는 각 모수의 수에 대한 정숫값을 조정하는 옵션입니다. default 값이 levels = 3이므로 해당 옵션은 생략해도 문제없을 것입니다. 교차 검증을 통한 모수 튜닝에는 수많은 모형을 적합해야 하는데, 이 예제에서는 9개의 모수 집합과 3개의 folds를 사용하므로 총 \\(3 \\times 9 = 27\\)개의 모형을 적합해야 한다. 27개의 모형을 빠르게 적합하기 위해 병렬처리를 수행하려고 합니다. 이는 {tune} 패키지에서 직접적으로 지원받을 수 있습니다.\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: future\n\n\n\nAttaching package: 'future'\n\n\nThe following object is masked from 'package:rmarkdown':\n\n    run\n\nall_cores <- parallel::detectCores(logical = FALSE) - 1\n\nregisterDoFuture()\ncl <- parallel::makeCluster(all_cores)\nplan(future::cluster, workers = cl)\n\n이제 튜닝을 시작합니다.\n\noptions(future.rng.onMisue = \"ignore\")\nrf_search <- tune_grid(rf_wflow, grid = rf_grid, resamples = dia_vfold,\n                       param_info = rf_param)\n\n튜닝 결과는 autoplot()과 show_best()로 검토할 수 있습니다:\n\nautoplot(rf_search, metric = \"rmse\")\n\n\n\n\n\n\n\n\n\\(x\\) 축은 mtry를 나타내며, 각 선의 색상은 고려한 다항식 차수를 나타냅니다. mtry는 5와 carat의 2차항까지 고려한 초모수 조합이 최적임을 알 수 있습니다. show_best()로도 확인할 수 있습니다:\n\nshow_best(rf_search, \"rmse\", n = 9)\n\n\n\n\n\n  \n\n\n\n\nselect_best(rf_search, metric = \"rmse\")\n\n\n\n\n\n  \n\n\n\n그리고, select_by_one_std_err()을 이용하면 원하는 metric 값의 \\(\\pm 1SE\\)를 고려한 최적의 초모수 조합을 얻을 수도 있죠.\n\nselect_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\n\n\n\n\n\n  \n\n\n\n\n\n6.4 선택한 최적의 모형으로 예측 수행\n6.3 모든 것을 결합하기: {workflows}에서 carat 변수는 2차항으로도 충분히 설명되고, 매 트리 적합 시 고려할 변수의 수는 5개임을 확인할 수 있었습니다. 이제는 해당 초모수 조합을 이용해 훈련 자료에 모형을 적합하고 최종 예측을 수행하려고 합니다. 이번 예제에서는 설정값이 똑같긴 하지만, \\(\\pm 1SE\\)를 고려한 초모수 조합을 모형 적합에 사용하였습니다.\n\nrf_param_final <- select_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\nrf_wflow_final <- finalize_workflow(rf_wflow, rf_param_final)\nrf_wflow_final_fit <- fit(rf_wflow_final, data = dia_train)\n\n이제 적합된 모형객체 rf_wflow_final_fit으로 원하는 unobserved 자료16를 predict()로 예측할 수 있다. 우리에게는 미리 나눠둔 시험 자료 dia_test가 있습니다. 다만, dia_test의 \\(y\\)는 로그변환이 취해지지 않았으므로, predict(rf_wflow_final_fit, new_data = dia_test)가 아닌 {recipe}로 step_log()를 취해주어야 합니다. 여기서는 workflow로부터 추출한 prepped된 recipe 객체를 이용해 시험 자료에 대하여 bake()를 취할 것입니다. 그리고, baked된 시험 자료를 적합한 최종 모형을 통해 예측할하면 되죠. bake()가 이렇게나 편합니다:\n\ndia_rec3 <- pull_workflow_prepped_recipe(rf_wflow_final_fit)\nrf_final_fit <- pull_workflow_fit(rf_wflow_final_fit)\n\ndia_test$.pred <- predict(rf_final_fit,\n                          new_data = bake(dia_rec3, dia_test)) %>% pull(.pred)\ndia_test$logprice <- log(dia_test$price)\n\nmetrics(dia_test, truth = logprice, estimate = .pred)\n\n\n\nWarning: `pull_workflow_prepped_recipe()` was deprecated in workflows 0.2.3.\nPlease use `extract_recipe()` instead.\n\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\n\n\n\n  \n\n\n\n시험 자료에 대한 RMSE는 약 0.11로 교차 검증에서 계산된 RMSE보다는 조금 더 나은 성능을 보여줍니다."
  },
  {
    "objectID": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#맺음말",
    "href": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/index.html#맺음말",
    "title": "tidyvese principle로 머신러닝 하기",
    "section": "맺음말",
    "text": "맺음말\n{tidymodels}의 ecosystem은 머신러닝 문제를 풀기 위해 필요한 첫 단계부터 끝까지 함께 작동하는 패키지들의 집합을 한대 묶어 제공해줍니다. 또한, {tidyverse}를 통한 data-wrangling 기능과 훌륭한 시각화 패키지 {ggplot2}와도 함께 작동하는 {tidymodels}은 R을 사용하는 데이터 사이언티스트들에게는 더없이 풍부한 toolbox라 할 수 있을 것 같습니다. 아울러, 해당 튜토리얼에서는 예측 모형들을 결합해주는17 기능을 갖는 패키지 {stacks}에 대한 내용을 다루지 않았는데18, {tidymodels}을 불러올 때 로딩이 되는 패키지는 아니지만, {stacks} 또한 {tidymodels}의 한 부분으로 소개되는 패키지에 해당합니다. 그리고, tidymodels ecosystem을 “머신러닝”에만 국한시키기에는 너무나도 많은 기능들이 업데이트되고 있습니다. 최근엔 반복측정자료분석에 자주 쓰이는 모형 중 하나인 혼합효과모형(linear mixed model)까지 지원하기 시작했습니다:\n\n\nLots of new #rstats package versions! Here’s a summary for the parsnip packages, including the new {multilevelmod} package!https://t.co/rv5Z9izpho— Max Kuhn (@topepos) March 24, 2022\n\n\n\ntidyverse 블로그를 꼭 팔로우업하세요. 본 튜토리얼은 20년 2월에 작성된 글을 기반으로 쓰여졌기 때문에 최신이라고 하긴 어렵습니다.😂 그러나, tidymodels ecosystem의 기본기를 익히기에는 충분할 겁니다. 이 튜토리얼이 {tidymodels}을 배우길 원하는, R로 머신러닝을 수행하길 원하는 우리나라 R 유저들에게 조금이나마 도움이 됐으면 좋겠습니다.\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n corrplot     * 0.92    2021-11-18 [1] CRAN (R 4.2.0)\n dials        * 1.0.0   2022-06-14 [1] CRAN (R 4.2.0)\n doFuture     * 0.12.2  2022-04-26 [1] CRAN (R 4.2.0)\n dplyr        * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n foreach      * 1.5.2   2022-02-02 [1] CRAN (R 4.2.0)\n future       * 1.27.0  2022-07-22 [1] CRAN (R 4.2.0)\n ggplot2      * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n ggrepel      * 0.9.1   2021-01-15 [1] CRAN (R 4.2.0)\n infer        * 1.0.2   2022-06-10 [1] CRAN (R 4.2.0)\n modeldata    * 1.0.0   2022-07-01 [1] CRAN (R 4.2.0)\n parsnip      * 1.0.0   2022-06-16 [1] CRAN (R 4.2.0)\n purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n recipes      * 1.0.1   2022-07-07 [1] CRAN (R 4.2.0)\n rmarkdown    * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n rmdformats   * 1.0.4   2022-05-17 [1] CRAN (R 4.2.0)\n rsample      * 1.0.0   2022-06-24 [1] CRAN (R 4.2.0)\n scales       * 1.2.0   2022-04-13 [1] CRAN (R 4.2.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble       * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidymodels   * 1.0.0   2022-07-13 [1] CRAN (R 4.2.0)\n tidyr        * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tune         * 1.0.0   2022-07-07 [1] CRAN (R 4.2.0)\n tweetrmd     * 0.0.9   2022-09-13 [1] Github (gadenbuie/tweetrmd@075102b)\n workflows    * 1.0.0   2022-07-05 [1] CRAN (R 4.2.0)\n workflowsets * 1.0.0   2022-07-12 [1] CRAN (R 4.2.0)\n yardstick    * 1.0.0   2022-06-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "",
    "text": "Photo by Caspar Camille Rubin on Unsplash\n실무에서는 Data analyst, Data scientist를 가리지 않고 SQL에 관한 능력을 요구합니다. 우리나라의 채용공고를 둘러보면 Data analyst의 경우 특히 SQL 스킬을 중요하게 요구하는 듯 합니다. 방대한 양의 데이터를 저장하고 관리하기 위해 실무에서는 데이터베이스를 사용합니다. 데이터베이스는 종종 관계형 데이터베이스 시스템1(이하 RDBMS)이라 불리기도 하죠. 그리고, 우리는 SQL2 언어 또는 SQL을 조금 변형한(variant) 언어를 통해 이 데이터베이스에 질의(query)를 합니다. 여기서 변형이라는 말을 사용한 이유는, RDBMS를 제공하는 업체에서 표준화된 SQL을 제공하는 경우도 있지만, 표준화된 SQL을 조금 변형시켜 사용하는 경우도 있기 때문입니다.\n만약 이렇게 특정 업체로부터 제공되는 변형된 RDBMS를 사용해야한다면, 해당 업체에서 사용하는 특정 SQL dialect3를 사용해 쿼리를 작성하는 방법을 이해해야 하실겁니다. 변형된 RDBMS를 예로 들어보자면, PostgreSQL, PrestoDB(AWS의 Athena를 위한) 등이 있습니다. PostgreSQL DB의 JSON 필드는 AWS에서 구조화된 중첩 배열로(array) 수집되므로, 동일한 필드를 쿼리하고자 할 때 다른 쿼리문을 사용합니다.\nR을 사용하는 여러분 모두 잘 아시다시피, R에서는 {dplyr}4 패키지를 통해 이러한 작업을 데이터에 수행할 수 있습니다. {dplyr}이 select(), group_by(), left_join() 등 SQL 문법을 잘 모방하긴 했지만, SQL 문법과 R 문법 사이를 완벽하게 왔다갔다 하기는 어렵습니다. 예를 들자면, {dplyr}의 filter()를 이용해 특정 행을 뽑아올 때, 우리는 R 문법을 따라야하므로 조건문에 =이 아닌 ==을 사용하죠. 이는 SQL 문법과는 완벽히 다른 부분입니다.\n자, 여기서 이러한 상황을 타개할 방법은 무엇일까요. 엄청난 용량의 데이터베이스를 R로 가져올 수는 없습니다. 메모리 베이스인 R에 이 짓을 햇다가는요? 생각도 하기 싫습니다.😰 그럼, RDBMS 환경에서 이러한 무거운 작업을(e.g. computation) 수행하고 필요로 될 때에만 R에다가 가져오면 되지 않을까요? 예를 들면, 집계된 데이터를 가져와서 보고서용 그림을 그린다든지. 이를 가능하게끔 해주는 패키지에 대해 배워보려고 합니다.\n본 튜토리얼에서는 {dplyr}의 데이터베이스 백엔드 버전이라 할 수 있는 {dbplyr} 패키지에 대해 배울거에요. {dbplyr}은 당신의 RDBMS에 R의 tidyverse 문법을 사용한 쿼리문을 직접적으로 사용할 수 있게끔 해줄겁니다.😀"
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html#db-연결하기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html#db-연결하기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "1 DB 연결하기",
    "text": "1 DB 연결하기\n먼저 필요한 패키지를 불러오죠. install.packages(\"패키지명\")을 통해 설치할 수 있습니다.\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RSQLite)\nlibrary(odbc)\n\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인 패키지입니다.\n{dbplyr}: {dplyr} 문법을 사용하여 데이터베이스에 질의를 할 수 있게끔 해줍니다.\n{dplyr}: 데이터베이스에 질의할 때 사용할 패키지입니다.\n{RSQLite}: 가벼운 단일 유저용 데이터베이스 SQLite DB에 연결할 수 있게끔 해주는 DBI5 호환 패키지입니다. R-SQLite로 이해하시면 편합니다.\n다른 DBI 용 호환 패키지가 필요할 수도 있습니다. 예를 들어, {RPostgres}는 PostgreSQL RDBMS와 연결을 해주는 패키지입니다.6\n{odbc}: odbc 드라이버를 사용해 RDBMS 인터페이스에 인터페이스할 수 있도록 해주는 DBI 호환 인터페이스입니다.7\n\n\n예제용 토이 DB\nAlison Hill이 The Great British Bake off에서 만든 데이터를 사용하려고 합니다. 본 예제에서 다룰 데이터베이스는 여기서 내려받으세요. {bakeoff} 패키지의 데이터를 이용해 연습에 사용할 SQLite DB를 만들었습니다. 이 튜토리얼의 원 저자 Vebash Naidoo님께 감사의 말을 전합니다.\n\n\nSQLite DB 연결하기\n이제 DB를 SQLite DB에 연결해봅시다. DB와 대화를 나누기 위해서, 우선 연결(connection)을 해줘야합니다. 다음의 작업을 해줄겁니다.\n\nDBI 패키지 로딩: library(DBI)\n연결하기: con <- dbConnect(RSQLite::SQLite(), \"내려받은 db 경로\")\n\n\nlibrary(DBI) # main DB interface\nlibrary(dplyr) \nlibrary(dbplyr) # dplyr back-end for DBs\n\ncon <- dbConnect(drv = RSQLite::SQLite(), # give me a SQLite connection\n        dbname = \"data/great_brit_bakeoff.db\")\nsummary(con) # What do we have?\n\n          Length            Class             Mode \n               1 SQLiteConnection               S4 \n\n\n위와 같은 명령어가 출력되면 DB에 성공적으로 연결된 것입니다."
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html#db-둘러보고-다뤄보기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html#db-둘러보고-다뤄보기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "2 DB 둘러보고 다뤄보기",
    "text": "2 DB 둘러보고 다뤄보기\n자, DB 연결도 했으니 이제 몇 가지 DBI 함수를 이용해 연결한 DB를 둘러보고 다뤄봅시다.\n\nDBI 함수\nDBI 함수들의 이름은 꽤 직관적입니다.\n\ndbListTables(con) # 연결된 테이블 리스트를 보여줘!\n\n [1] \"baker_results\"     \"bakers\"            \"bakes\"            \n [4] \"challenge_results\" \"challenges\"        \"episode_results\"  \n [7] \"episodes\"          \"ratings\"           \"ratings_seasons\"  \n[10] \"results\"           \"seasons\"           \"series\"           \n\n\n\ndbListFields(con, # 연결한 DB로 가서\n      \"bakers\")   # bakes 테이블에 어떤 필드가 있는지 알려줘!\n\n[1] \"series\"     \"baker_full\" \"age\"        \"occupation\" \"hometown\"  \n\n\nDB에 질의는 다음과 같이 수행할 수 있어요.\n\nres <- dbSendQuery(con, \"SELECT * FROM bakers LIMIT 3\") # 쿼리문 실행\n# bakers 테이블에 모든 필드를 가져오는데, 관측치 3개까지만 가져와봐!\ndbFetch(res) # 결과 출력해줘\n\n  series          baker_full age                        occupation\n1      1       Annetha Mills  30                           Midwife\n2      1      David Chambers  31                      Entrepreneur\n3      1 Edward \"Edd\" Kimber  24 Debt collector for Yorkshire Bank\n       hometown\n1         Essex\n2 Milton Keynes\n3      Bradford\n\n\n\ndbClearResult(res) # 결과 지우기\n\n\n\ndplyr 함수\n이제, 우리가 잘하는 {dplyr}의 함수들을 이용해 마음껏 DB와 이야기해보죠.\n\ndplyr::tbl(con, \"테이블명\"): 연결한 DB(con)으로 가서 SELECT * FROM 테이블명 실행해줘.\n\n\ntbl(con, \"bakers\")\n\n# Source:   table<bakers> [?? x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker_full                age occupation                       homet…¹\n    <dbl> <chr>                   <dbl> <chr>                            <chr>  \n 1      1 \"Annetha Mills\"            30 Midwife                          Essex  \n 2      1 \"David Chambers\"           31 Entrepreneur                     Milton…\n 3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yorkshire Ba… Bradfo…\n 4      1 \"Jasminder Randhawa\"       45 Assistant Credit Control Manager Birmin…\n 5      1 \"Jonathan Shepherd\"        25 Research Analyst                 St Alb…\n 6      1 \"Lea Harris\"               51 Retired                          Midlot…\n 7      1 \"Louise Brimelow\"          44 Police Officer                   Manche…\n 8      1 \"Mark Whithers\"            48 Bus Driver                       South …\n 9      1 \"Miranda Gore Browne\"      37 Food buyer for Marks & Spencer   Midhur…\n10      1 \"Ruth Clemens\"             31 Retail manager/Housewife         Poynto…\n# … with more rows, and abbreviated variable name ¹​hometown\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\ntbl(con, \"bakers\") %>% \n    head(3) # \"SELECT * FROM bakers LIMIT 3\"와 동일\n\n# Source:   SQL [3 x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n  series baker_full                age occupation                        homet…¹\n   <dbl> <chr>                   <dbl> <chr>                             <chr>  \n1      1 \"Annetha Mills\"            30 Midwife                           Essex  \n2      1 \"David Chambers\"           31 Entrepreneur                      Milton…\n3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yorkshire Bank Bradfo…\n# … with abbreviated variable name ¹​hometown\n\n\n데이터베이스와 대화를 나눌 때 마다 초기에 연결해둔 con을 사용한다는 점을 유념해주세요. 초기에 불러왔던 con은 아까처럼 일반적인 SQL 쿼리문을 이용해 질의를 할 때 뿐만이 아닌 {dplyr}을 통해 타이디한 파이프라인으로 원하는 테이블을 가져올 때도 사용됩니다.\n자 이제 예시 상황을 하나 들어서 {dplyr}로 원하는 테이블을 가져와보겠습니다. baker_results 테이블에는 각 제빵 대회에 참가한 제빵사(baker)의 세부 정보 필드가 담겨있습니다:\n\ndbListFields(con, \"baker_results\")\n\n [1] \"series\"                    \"baker_full\"               \n [3] \"baker\"                     \"age\"                      \n [5] \"occupation\"                \"hometown\"                 \n [7] \"baker_last\"                \"baker_first\"              \n [9] \"star_baker\"                \"technical_winner\"         \n[11] \"technical_top3\"            \"technical_bottom\"         \n[13] \"technical_highest\"         \"technical_lowest\"         \n[15] \"technical_median\"          \"series_winner\"            \n[17] \"series_runner_up\"          \"total_episodes_appeared\"  \n[19] \"first_date_appeared\"       \"last_date_appeared\"       \n[21] \"first_date_us\"             \"last_date_us\"             \n[23] \"percent_episodes_appeared\" \"percent_technical_top3\"   \n\n\n각 제빵대회 우승자의 출신이 영국의 일부 지역에서 나왔는지, 아니면 다양한 지역으로부터 우상자가 배출되었는지 알고싶은 상황이라고 해봅시다. 우선 다음과 같이 관심있는 필드만 불러와주겠습니다.\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner)\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker     hometown              series_winner\n    <dbl> <chr>     <chr>                         <int>\n 1      1 Annetha   Essex                             0\n 2      1 David     Milton Keynes                     0\n 3      1 Edd       Bradford                          1\n 4      1 Jasminder Birmingham                        0\n 5      1 Jonathan  St Albans                         0\n 6      1 Lea       Midlothian, Scotland              0\n 7      1 Louise    Manchester                        0\n 8      1 Mark      South Wales                       0\n 9      1 Miranda   Midhurst, West Sussex             0\n10      1 Ruth      Poynton, Cheshire                 0\n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n24개 열 중 관심있는 4개 열만 불러왔습니다. 이제 제빵대회에 우승한 사람만 골라낸 뒤(filter()) 우승자들이 영국의 어떤 지역으로 부터 왔는지 지역별로 인원을 구하고(count()) 내림차순 정렬(sort())을 해보죠.\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>%\n  count(hometown, sort = TRUE)\n\n# Source:     SQL [8 x 2]\n# Database:   sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Blog/posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n# Ordered by: desc(n)\n  hometown                              n\n  <chr>                             <int>\n1 Wigan                                 1\n2 West Molesey, Surrey                  1\n3 Ongar, Essex                          1\n4 Market Harborough, Leicestershire     1\n5 Leeds / Luton                         1\n6 Bradford                              1\n7 Barton-Upon-Humber, Lincolnshire      1\n8 Barton-Le-Clay, Bedfordshire          1\n\n\n이 결과에 따르면, 제빵대회 우승자들의 출신 지역은 각기 다르다고 결론을 내릴 수 있겠네요.\n\n\ndplyr 문법을 SQL 쿼리문으로\n앞서 {dplyr}을 이용해 수행한 질의를 SQL 쿼리문으로는 어떻게 작성할까요? 코드 한 줄이면 손쉽게 알 수 있습니다.😀\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>% \n  count(hometown, sort = TRUE) %>% \n  show_query()\n\n<SQL>\nSELECT `hometown`, COUNT(*) AS `n`\nFROM (\n  SELECT `series`, `baker`, `hometown`, `series_winner`\n  FROM `baker_results`\n)\nWHERE (`series_winner` = 1.0)\nGROUP BY `hometown`\nORDER BY `n` DESC\n\n\n멋지지 않습니까? 이제 제가 왜 이 글의 맨 위 요약을 “R을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!”라고 적은 지 아시겠나요? {dplyr}로 작업을 수행하고, SQL 쿼리문으로 변환을 수행해보는 작업은 SQL을 배우는 과정에 꽤 큰 도움이 될겁니다. 직장 또는 기관에서 DB를 관리할 때 모두 같은 업체의 SQL DB를 사용하는 건 아니므로, 이렇게 광범위한 업체들로부터 공급되는 SQL을 알고, 읽는 것은 언제나 중요하기 때문입니다.\n\n\n출력문의 lazy query / ??의 의미\n앞서 테이블, 쿼리를 작성하며 출력물에서 Source: table [?? x 5] 또는 Source: lazy query [?? x 4]와 같은 문장을 확인하실 수 있었을 겁니다.\n\n이런 문장이 출력물에 포함되는 이유\n\n먼저, 우리가 직접적인 RDBMS 상에서가 아닌 R이라는 공간을 빌려 뒤에서(behind the scenes) 작성한 dplyr코드는 우리가 연결하려는 DB의 SQL에 해당하는 dialect로 변환됩니다.\n즉, SQL은 DB에 직접적으로 실행됩니다. 즉, 데이터를 먼저 R로 가져와서 조작하는 것이 아닌 쿼리 자체를 DB에 보내고 DB에서 계산(computation)이 수행됩니다.\n정리하면, dplyr 파이프라인을 사용해 DB에서 쿼리를 실행하면, DB에서 계산을 수행하고 실행된 최종 결과의 전체가 아닌 일부를 R에서 보여주는 식입니다.\n이러한 이유들을 들여다보면 우리는 ??를 이해할 수 있습니다.\n??는 “연결 DB con에서 이 쿼리(파이프라인을 SQL로 변환시킨 것)를 실행했고, 여기 R에서 출력물을 스니펫(snippet)으로만 가져왔는데, 얼마나 많은 수의 행이 있는지에 관한 메타 정보까진 캐치하진 못했어. 그저 출력물에 몇 개의 열이 있다는 것 정도만 캐치했어”라고 이해할 수 있습니다.\n이 튜토리얼은 파트 1 입니다. 다음 파트에서 가져온 테이블에 얼마나 많은 행들이 존재하는 지와 같은 메타 정보들을 R로 어떻게 가져오는지에 대해 알아볼 예정입니다."
  },
  {
    "objectID": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html#db-연결-해제하기",
    "href": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/index.html#db-연결-해제하기",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "section": "3 DB 연결 해제하기",
    "text": "3 DB 연결 해제하기\n작업이 끝나면 연결을 해제하는 것을 잊지마세요!\n\ndbDisconnect(con) # db 연결 닫기\n\n연결 해제가 체크는 dbListTable(con)을 실행해보시면 됩니다. 연결해제가 잘 되었다면 에러문이 출력될겁니다.\n\n다음 파트에서 배울 내용\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인 패키지입니다.\n데이터 R로 가져오기\n\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n dbplyr      * 2.2.1   2022-06-27 [1] CRAN (R 4.2.0)\n dplyr       * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n odbc        * 1.3.3   2021-11-30 [1] CRAN (R 4.2.0)\n RSQLite     * 2.2.15  2022-07-17 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/index.html",
    "href": "posts/2022-05-13-how-to-review-a-paper/index.html",
    "title": "관심 논문 읽고 요약하기",
    "section": "",
    "text": "Photo by Aaron Burden on Unsplash\n논문 읽기가 초심자에게는 만만치않은 작업인 만큼, 논문을 정리하는 자기만의 방식을 만들어 놓는 것은 참 중요합니다. 저 또한 아직 초심자라고 생각하고 있는데요, 오늘은 제가 관심있는 논문을 읽고 정리하는 방식에 대해 얘기해보려고 합니다. 제가 스스로 터득한 방법을 소개드리는 것은 아닙니다. 논문을 읽고 정리하는 좋은 방식이 있나 싶어 검색을 하던 도중 좋은 글(An 2022)을 발견하게 됐고, 이 글을 바탕으로 저만의 방식을 정립해봤습니다. 좋은 글을 써주신 안수빈님께 감사의 마음을 전합니다.\n논문 요약은 1st read(첫 번째 읽기), 2nd read(두 번째 읽기) 2가지 섹션으로 진행할 것입니다. 1st read의 결과에 따라 2nd read는 진행되지 않을 수도 있습니다."
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/index.html#st-read",
    "href": "posts/2022-05-13-how-to-review-a-paper/index.html#st-read",
    "title": "관심 논문 읽고 요약하기",
    "section": "1st read",
    "text": "1st read\n첫 번째 읽기의 핵심은 빠르게 읽으며 논문의 큰 그림을 파악하는 것이라고 합니다. 5분에서 10분 정도 다음 순서에 따라 읽으라고 권합니다.\n\n제목(title), 초록(abstract), 소개(introduction)를 집중해서 읽으세요.\n섹션(section)과 하위 섹션(subsection)의 세부내용은 무시하고 제목만 읽으세요.\n결론(conclusion)을 읽으세요.\n참고문헌(reference)을 보며 저자가 인용한 논문, 이전에 읽은 논문에 대해 가볍게 체크하세요.\n\n먼저 1st read에서는 논문을 빠르게 읽으면서 파악한 전반적인 그림에 관해 기술합니다. 첫 번째 읽기를 하고나서는 다음의 여섯 가지(5C + 1M)를 답할 줄 알아야 하며, 본 블로그에서 요약할 논문들 또한 다음과 같은 섹션으로 요약하려고 합니다.\n\nCategory: 논문의 종류\nMain Topic: 논문 제목 및 주제\nContext: 다른 페이퍼들과의 관계, 문제를 풀기 위해 사용한 이론적 바탕\nCorrectness: 논문에 필요한 가정의 명확성\nContributions: 논문의 핵심 기여\nClarity: 논문의 가독성, 명료함\n\n논문에 필요한 가정의 명확성(Correctness)은 제 경우 보통 방법론 부분에서 모델에서 요구하는 가정이나 모델링 과정의 각 단계가 합리적인 근거로 진행 되었는지에 관심이 있으므로, 첫 번째 읽기에서 Method 부분을 빠르게 검토해보는 과정이 필요로 될 것 같습니다. 아울러, 논문의 가독성과 명확성(Clarity)에 관한 부분은 잘 아는 분야가 아니라면 감히 기술하기 어려울 것 같습니다. 때때로 생략할 수도 있는 부분입니다.😅 그리고, 논문의 종류(Category)는 이 글(Hong 2012)을 참고하세요. 5C + 1M을 바탕으로 논문을 더 읽을지 말지 선택할 것입니다. 더 읽지 않는 결정을 한다면, 대부분은 다음의 이유일 겁니다.\n\n관심이 없는 내용\n해당 논문을 읽기엔 사전 지식이 부족\n저자의 가정이 모호 또는 불명확\n\n만약, 꼭 읽어야만 하는 논문임에도 해당 논문을 읽기에 사전 지식이 부족하다면, 참고문헌(reference)들을 다시 검토해보면서 관심있는 연구 분야의 핵심 연구라고 생각 되는 것을 찾아내 읽어보는 과정을 가져야 할겁니다. 또는, 논문에 쓰인 방법론에 관한 이해가 안되어 있는 상태라면 해당 방법론의 Method paper나 Review paper를 찾아보는 것도 큰 도움이 될 겁니다."
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/index.html#nd-read",
    "href": "posts/2022-05-13-how-to-review-a-paper/index.html#nd-read",
    "title": "관심 논문 읽고 요약하기",
    "section": "2nd read",
    "text": "2nd read\n2nd read에서는 좀 더 세부적인 내용에 집중하라고 합니다. 단, 증명같은 디테일은 무시한채 말이죠. 핵심 사항을 노트에 적거나 테두리에 본인의 의견을 써놓는 것을 권장합니다. 두 번째 읽기는 약 1시간 정도가 소모됩니다. 처음 접하는 분야의 논문이나 개인의 논문 독해 실력에 따라 훨씬 더 많은 시간이 소요될 수도 있습니다. 다음과 같은 사항에 주목하여 읽으세요.\n\nFigure, Diagram, Table 등 논문 내 다양한 도표와 일러스트레이션을 주의깊게 보세요. 특히, Data Science에 관심이 있는 분들이라면 그래프를 잘 봐야합니다. 그래프의 \\(x\\)축, \\(y\\)축, 테이블의 행과 열이 의미하는 바 등을 확인하고 이를 통해 저자가 주장하고자 하는 바가 무엇인지 한마디로 정리할 줄 알아야합니다. 물론, 이 부분은 저자가 확실하게 주장하고자 하는 바를 가지고 시각화, 테이블 작성를 수행했다는 전제 하에 있습니다.\n아직 읽지 않은 연관 논문을 체크하세요. 이 과정은 논문의 배경 지식 또는 특정 방법론에 관한 Method paper인 경우 해당 방법론의 모티베이션을 공부하는데 도움이 됩니다.\n\n두 번째 읽기가 끝난 상태에서 우리가 바라는 희망사항은 다음과 같습니다:\n\n논문의 핵심 내용 이해\n논문의 핵심 주장에 대해 근거와 함께 요약할 수 있어야 함\n\n그래서, 두 번째 읽기를 끝낸 논문은 다음과 같은 섹션으로 상세한 추가 요약을 수행할 예정입니다.\n\nMain Findings: 논문의 핵심 주장과 뒷받침 근거\nMethods: Main Findings에 사용된 핵심 방법론에 관한 내용\nResults: Main Findings외 다른 연구 결과\nLimitations: 연구의 한계점\n\nMain Findings외 다른 연구 결과에 해당하는 Results와 연구의 한계점(Limitations)는 때때로 생략될 수 있습니다.\n두 번째 읽기는 당신이 관심있어 하지만, 당신의 전문 연구 분야는 아닌 논문에 적합하다고 합니다. 하지만, 저는 제 전문 연구 분야도 위와 같은 두 번째 읽기를 통해 추가적으로 세부적인 요약을 수행할 예정입니다. 전문 연구 분야라면 훨씬 더 빠르게 두 번째 읽기를 할 수 있겠죠. 그러나, 여러 이유로 두 번째 읽기에도 이해가 안될 수도 있습니다:\n\n이 주제나 내용이 새로워서 전문 용어나 약어에 익숙하지 않음\n저자가 사용한 방법론이나 연구 결과를 낼 때 사용된 테크닉이 이해가 안됨\n합리적 근거가 부족한 주장 또는 너무 많은 레퍼런스\n피곤해서!\n\n이럴 때 3가지 선택지를 제안합니다.\n\n논문을 치우세요. 그리고, 해당 논문의 내용이 커리어에 무관하기를 바라세요.\n배경 지식을 공부하고 다시 읽으세요.\n노력해보고 세 번째 읽기를 해보세요.\n\n거인의 어깨 위에 올라서서 세상을 바라보라고 하는데, 거인에 어깨 위에 올라서는 것 조차 참 어렵습니다..😭"
  },
  {
    "objectID": "posts/2022-05-13-how-to-review-a-paper/index.html#맺음말",
    "href": "posts/2022-05-13-how-to-review-a-paper/index.html#맺음말",
    "title": "관심 논문 읽고 요약하기",
    "section": "맺음말",
    "text": "맺음말\n앞으로 제 블로그에 읽은 논문들을 요약하는 글을 작성하기에 앞서, 논문 요약 방식에 대한 설명이 필요할 것 같아서 쓰게 된 글입니다. 논문 요약 방식에 정답은 없습니다. 각자의 논문 요약 방식에 대해 나눠보는 것도 참 흥미로운 대화 거리가 될 것 같네요. 참고한 글(An 2022)에 더 좋은 내용이 많습니다. 그리고, 해당 글의 세 번째 읽기, 문헌 조사 등 “논문 쓰기”에 도움이 될 만한 내용들 또한 기술이 되어있습니다. 다시 한 번 좋은 글 작성해주신 안수빈님께 감사의 말씀을 전합니다. 저도 아직 많이 부족하지만, 이 글이 첫 논문을 접하는 분들께 조금이나마 도움이 됐으면 합니다.🙏"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/index.html",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/index.html",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "",
    "text": "Prerequisite: 논문 요약 방식"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/index.html#st-read",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/index.html#st-read",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "1st read",
    "text": "1st read\n\nCategory\n\nResearch paper\n\n\n\nMain Topic\n\n제목\n\nCardiac dyspnea risk zones in the South of France identified by geo-pollution trends study - (Simões et al. 2022)\n\n\n\n주제\n\n프랑스 남부 지역의 Cardiac dyspnea(CD, 이하 심호흡곤란) 발생에 미치는 대기오염원(\\(\\rm{PM}_{10}\\), \\(\\rm{NO}_{2}\\), \\(\\rm{O}_{3}\\)) 영향 평가\n\n\n\n\nContext\n\n선행 연구들에서 대기오염원에 관한 단기 노출이 심근경색(myocardial infarction), 울혈성심부전(congestive heart failure)과 같은 몇몇 심혈관 병리(cardiovascular pathologies)들에 미치는 영향을 평가하긴 했으나, 심호흡곤란의 경우 이러한 관계를 아직 완전히 입증하지 못함\n따라서, 본 연구의 목적은 대기오염원, 기상요인, 심호흡곤란 입원 데이터를 활용해 심호흡곤란 입원 발생 원인에 관한 메커니즘을 알아보고, 이를 예방하기 위한 정책을 개발하는 것에 있음\n본 연구의 주요 방법론은 Distributed lag non linear model(이하, DLNM)과 메타분석(Meta analysis)에 해당함\n\n\n\nCorrectness\n\n기상요인(meteorological factors)들을 공변량(coviariates)으로 활용하는데, 다중공선성(multicollinearity)을 피하기 위해 상관이 존재할만한 두 변수 중 하나의 변수만 모형에 포함시킴\n최대 지연 효과(maximum lag days)는 14일까지 고려하였으나, 이에 관한 합리적 근거는 없다고 보여짐\n\n\n\nContributions\n\n프랑스 남부 전체 지역의 심호흡곤란 입원 발생에 관한 대기오염원의 영향을 평가한 첫 번째 연구\n\\(\\rm{NO}_2\\), \\(\\rm{O}_3\\), \\(\\rm{PM}_{10}\\)에 단기 노출이 심호흡곤란으로 인한 응급실 방문을 증가시킨다는 것에 관한 유의한 증거 제시\n본 논문의 접근 방식은 공중 보건 정책에 관한 예측 도구로서 대기오염원 모니터링을 효과적으로 제안함\n\n\n\nClarity\n\n지금까지 읽어본 바로는 명료하게 잘 쓰인 논문이라 생각됨"
  },
  {
    "objectID": "posts/2022-05-24-paper-review-simes-et-al-2022/index.html#맺음말",
    "href": "posts/2022-05-24-paper-review-simes-et-al-2022/index.html#맺음말",
    "title": "논문 요약 - Simões et al (2022)",
    "section": "맺음말",
    "text": "맺음말\n본 논문을 통해 실제 각 도시별 DLNM을 이용한 대기오염원 건강영향평가 수행 후, 메타분석으로 오버롤한 결과를 제시할 수 있음을 확인했습니다. 메타분석을 어떻게 진행하였는지에 관한 이론적 부분은 자세하게 기술되어 있지 않아서 두 번째 읽기는 진행하지 않았으나, 도시별 분석 결과를 메타분석을 통해 종합할 수 있다는 것을 확인하는 것으로는 첫 번째 읽기로도 충분했습니다.\n본 논문에 쓰인 메타분석은 일반적으로 임상연구에서 수행하는 메타분석을 다양한 상황에 쓸 수 있도록 일반화하여 확장시킨 형태의 메타분석 방법론이라고 보시면 됩니다. 해당 방법론을 깊이있게 이해하기 위해서는 (Sera et al. 2019)을 참고하시면 됩니다. 해당 논문의 예제 R 소스코드는 여기를 참고하시면 됩니다. 다양한 형태의 분석을 수행한 뒤에 library(mixmeta)를 통해 메타분석을 수행하여 결과를 종합하는 과정을 보여준다는 점에서 큰 의미가 있습니다. 그러나, 정작 제가 필요로하는 DLNM으로 건강영향평가를 도시별로 수행한 뒤에 메타분석을 하는 소스코드는 없다는 점이 조금 아쉬웠습니다.😂 그래서, 추가적으로 (Gasparrini, Armstrong, and Kenward 2012)에서 제공하는 R 예제 소스코드를 함께 참고했습니다. 확장된 형태의 메타분석인 (Sera et al. 2019)가 나오기 전이라 library(mvmeta)를 통해 분석이 진행되긴 합니다만, library(mixmeta)와 똑같은 로직으로 분석이 진행되기 때문에 해당 소스코드를 함께 참고하시면 도시별 DLNM 분석 결과를 메타분석하는 것을 어렵지 않게 구현하실 수 있을겁니다."
  },
  {
    "objectID": "posts/2022-06-08-monthly-memory-202204/index.html",
    "href": "posts/2022-06-08-monthly-memory-202204/index.html",
    "title": "월간 회고록: 2022년 4월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash"
  },
  {
    "objectID": "posts/2022-06-08-monthly-memory-202204/index.html#새로운-스터디를-시작하다",
    "href": "posts/2022-06-08-monthly-memory-202204/index.html#새로운-스터디를-시작하다",
    "title": "월간 회고록: 2022년 4월",
    "section": "새로운 스터디를 시작하다",
    "text": "새로운 스터디를 시작하다\n올해 3월부터 SQL 스터디, Python 코딩테스트, Tensorflow 스터디를 시작했습니다. 올 초부터 다양한 기업의 Data Scientist 채용 공고를 둘러봤고, 아무래도 이 세 가지는 꼭 필요로 된다고 느꼈습니다. “왜 이제 와서 시작하냐?” 하는 생각을 가지시는 분들이 많으실 것 같습니다. 작년에 대학원을 졸업했고 실무에서 1년차를 넘긴 지금에서야 말이죠. 지금부터 그 이야기를 풀어보려고 합니다. 사실, 지금 생각해보면 대학원 때 시작했어야할 것을 이제서야 시작한다는게.. 참 많이 늦은 감있습니다. 하지만, 늦었을 때가 가장 빠른?..뭐 이런 말로 위로를 삼아봅니다..\n사실 코딩테스트는 학부생 시절 대학원에 진학하기 전에 잠깐 취업 준비를 해보면서, 대학원을 졸업하고 취업 준비를 하면서 몇 번 치뤘던 적이 있습니다. R이 주 언어인 사람에게 다행스러웠던 것은 이때 치뤘던 코딩테스트들에서는 다행히 R을 지원해줬었다는 점이죠. 두시간 세시간 붙잡고 알고리즘 한두문제를 겨우 풀어서 제출했던 기억이 있습니다. 함수를 다 짜서 제출하면 뭐하나요, 뭣도 모르고 입력을 받아야하는 input()도 안해서 테스트케이스는 다 틀리는데 말이죠.😅 네, 당연히 항상 결과는 불합격이었습니다.\n\n그래서, 이제서야 시작한 이유는?..\n참 부끄럽지만 “내가 이걸 왜 준비해야하지?”라는 고집같은 생각을 했습니다. 내가 개발자도 아니고, Data science를 하고 싶은 사람인데 굳이 알고리즘 문제를 왜 잘 풀어내야하지? 왜 이런 것을 요구하는 걸까? 하는 생각을 했었죠. 지금 생각하면 참 바보같습니다. 아시다시피 요즘 나오는 여러분들이 이름만 대면 알만한 대기업, 빅 플랫폼 기업, 금융 기관의 Data Scientist 나 Data Analyst 채용 공고를 보시면 면접 전형 전에 꼭 코딩테스트가 포함되어 있습니다.1 극 소수의 대기업에서는 면접 전형 전 코딩테스트 대신 사전 과제 또는 Data Analyst의 경우 SQL 쿼리 테스트를 진행하는 경우도 있긴 합니다만, 코딩테스트가 포함된 형태의 채용 전형은 앞으로 기업들 사이에서 더더욱 확대될 것이라고 봅니다.\n과거에는 코딩테스트 공부는 거들떠보지 않았던 제가 지금에서야 공부를 시작한 이유는 “내가 이걸 왜 준비해야하지?”와 같이 어리석은 고집같은 생각을 버리고 그간 여러 생각을 해왔기 때문입니다. 먼저 “과연 내가 Data Science를 수행하기 위해 가고싶은 마음 속 업계 또는 기업만을 위해서 한 노력이 있는가?”에 대해 생각했고, 수많은 지원자를 평가해야만하는 기업과 실무자의 입장을 생각하기 시작하면서 제 관점은 많이 바뀌기 시작했습니다. 대기업, 우리가 이름만 대면 알만한 핫한 기업에는 수많은 지원자가 몰립니다. 그들의 입장에서 생각해보면, 다른 전형 없이 서류전형에서 각 지원자들의 서류를 세세하게 평가하여 바로 면접 전형을 진행하는 것은 결코 불가능합니다. 그래서, 코딩테스트와 같이 객관적인 평가 기준으로 지원자들을 한 번 걸러내는 작업이 필요로 되는 것이라 생각합니다. 공기업 채용 전형에서의 NCS, 사기업 채용 전형에서의 적성 평가2와 같은 것과 같은 맥락으로, 개발 직군에게는 코딩테스트라는 것이 존재하는 것이죠. 과거에는 이러한 형태의 채용 전형을 이해하고 싶지 않았습니다. 기업의 Culture fit과 얼마나 맞는지에 관한 인성 검사와 같은 것들은 꼭 필요로 된다고 생각했지만, NCS, 직무적성검사, 코딩테스트 같은 것들은 실질적인 직무 수행 능력과 직결이 되는 것도 아닌데, 왜 치뤄야 하는지에 대해 이해가 안됐었죠. 지금은 백 번 이해합니다. 오하려 과거에 되도 않는 고집을 피우며 코딩테스트 공부를 거들떠보지 않았던 저를 참 한심하게 생각하고있습니다.🤬\n아무튼 이러한 모티베이션에서 코딩테스트를 시작했고, SQL을 현업에서 다루고 있지만 SQL 쿼리테스트 스터디도 시작을 했습니다. 아울러, Tensorflow의 경우는 수많은 Data Scientist 채용공고를 둘러본 결과, torch나 tensorflow 등과 같은 머신러닝 프레임워크 하나 정도는 다룰 줄 알아야 될 것 같음을 느껴 시작하게 됐습니다. 여러 프레임워크 중 Tensorflow를 선택한 이유는, 현재 M1 GPU를 지원해주는 유일한 프레임워크이기 때문입니다. 그마저도 싱글코어긴 합니다..(사실 torch를 배워보고 싶었는데,,) 그리고, R의 {tidymodels}을 통해 머신러닝을 수행할 수 있긴 합니다만, 우리나라 업계의 Data Scientist 채용 공고에서 아직 R의 {tidymodels}를 기재해놓은 공고는 본 적이 없습니다. Python의 scikit-learn을 요구하는 경우는 종종 봤지만 말이죠. 참 씁쓸하네요..😭 개인적으로 {tidymodels}은 scikit-learn과 비교하기 미안할 정도로 더 좋은 패키지인데 말이죠. 아무튼, Tensorflow 스터디는 4월에 아카이브를 만들어 놓고, 업무와 다른 일을 핑계로 아직도 제대로 시작하지 않고 있네요.. 마침 Deep Learning with R, Second Edition이 곧 출판을 앞두고 있다는 소식을 들었는데, 이 책으로 스터디를 진행할까 합니다. 아니, 해야죠!\n\n🔗SQL 스터디\n🔗Python 코딩테스트 스터디\n🔗Tensorflow 스터디"
  },
  {
    "objectID": "posts/2022-06-08-monthly-memory-202204/index.html#이력서-포트폴리오-제작기",
    "href": "posts/2022-06-08-monthly-memory-202204/index.html#이력서-포트폴리오-제작기",
    "title": "월간 회고록: 2022년 4월",
    "section": "이력서, 포트폴리오 제작기",
    "text": "이력서, 포트폴리오 제작기\n기존에는 canva로 이력서와 경력기술서를 관리하고, 포트폴리오는 애플 키노트로 관리하고 있었는데 하나의 툴로 관리하고 싶었어요. R 마크다운과 노션 중에 고민하다가 노션으로 택했습니다. R Markdown에 비해 웹 공유도 편하고, PDF 변환, 그리고 무엇보다 디자인적인 요소가 훨씬 낫다고 생각했습니다. 그리고, R Markdown으로 관리했을 때 얻을 수 있는 베네핏도 딱히 없다고 생각했고, 이력서뿐만이 아니라 포트폴리오까지 함께 관리하기엔 노션이 확실히 편합니다. 이번에 노션으로 이력서와 포트폴리오를 다시 쭉 작성하며 참고해봤던 자료들입니다:\n\n🔗개발자 이력서 작성하기\n🔗eo - 최고의 직장에서 깨달은 내 몸값을 높이는 스킬 | 커리어 액셀러레이터 김나이\n🔗Data Scientist 김단아님 노션 Resume\n\n참고할만한 노션 Resume의 99%는 개발자 이력서이고 나머지는 통계학과 외에 다른 백그라운드로 Data Science를 하시는 분들의 이력서 뿐인데, 김단아님은 저와 같은 통계학 백그라운드로 Data Science를 하시는 분이라 참 많은 도움이 됐습니다. 이력서, 포트폴리오를 만들고 다듬는데에 대략 4일정도 걸린 것 같습니다. 이미 작성된 이력서, 경력기술서, 포트폴리오가 있었음에도 불구하고, 지겹고 힘들더군요.😪 참고했던 글, 영상 들에서 공통적으로 주장하는 이력서와 포트폴리오의 주요 포인트는 다음과 같습니다:\n\n내가 “어떤 것을 했다.”와 같이 팩트만 펼처 놓는 것이 아닌, 나의 강점을 펼치고 상대방을 설득할 수 있도록 기술하자\n이력서는 영화 예고편과 같다. 짧고 간결하게 꼭 보여주고 싶은 것들만 컴팩트하게 담자\n경험과 직무를 연결하자\n가능하다면 숫자로 성과를 드러내라\n\n숫자로 표현할 수 없다면, 그 일을 왜 했는지, 타겟이 누구였는지 디테일하게 담아보자\n\n개발 직군의 경우 다룰줄 아는 Tool의 수준을 나타내는 것은 지양하자\n\nTool의 수준에는 주관이 개입하기 마련이고, 객관적인 기준이 없기 때문\n개인적으로 주 언어정도를 표기하는 것은 나쁘지않다고 봄\n나머지 본인이 다루는 각 Tool의 수준은 포트폴리오에서 자연스럽게 드러나야함\n\n경력 기술, 포트폴리오 작성 시 Data Privacy, Research Privacy, 업무 상 비밀은 꼭 지켜야 함\n\n이를 지키지 않으면 이력서를 평가하는 사람 입장에서도 큰 (-)가 될 수 있음\nPrivacy를 지키기 위해 마스킹이 필요한 부분은 꼭 마스킹하여 기술하자\n\n\n버려야 하는 내용은 과감하게 버려야하는데, 이게 참 어려웠던 것 같습니다. Privacy를 지키는 일도 매우 중요한데, 꽤 귀찮았고요.😅\n이직을 계획하고 계신 분들이 아니여도 이력서, 포트폴리오를 틈틈히 정리해두는 습관은 꼭 필요합니다. 이력서와 포트폴리오가 꼭 필요한 상황에 닥쳐서 한꺼번에 지금까지 해온 것들을 정리하는 작업은 정말 힘든 일입니다. 정말 많은 시간이 소요될 것이고, 사람의 기억력에는 한계가 있기 때문에 틈틈히 주기적으로 이력서와 포트폴리오를 관리해온 사람에 비해 좋은 퀄리티를 갖기도 힘들 것입니다. 더군다나, 요새는 “평생직장”이 아닌 “평생직업”을 바라보고 살아가야하는 세상이기에 본인 PR을 할 줄 알아야합니다. 과장 좀 보태서 이야기 해보면, 본인이 한 것은 100인데 50으로 밖에 포장을 못하는 사람이 있는 반면, 본인이 한 것은 70인데 100만큼 포장할 줄 아는 사람이 있습니다. 본인이 어디쯤 위치하는 사람인지 곰곰이 생각해보시기 바랍니다. 그래서, 커리어를 쌓아가는 데에 있어서 본인이 이루어 낸 것들을 주기적으로 잘 정리하고 포장하는 것은 기본 중의 기본이라 생각합니다. 이직 계획과는 무관하게 적어도 분기에 1번 정도는 이력서와 포트폴리오의 유지보수에 시간을 투자하는 것을 적극 권장합니다. 나라는 상품을 취업 시장에 내놓는데, 다른 상품들과의 차별점을 꾀하기 위해 이정도 노력은 꼭 필요하지 않겠습니까? 이런 노력 없이도 남들보다 훨씬 더 뛰어난 무언가를 갖고 있는 인재가 아닌 이상 말이죠.\n마지막으로 4일 간의 끈질긴 작업 끝에 완성한 제 이력서와 포트폴리오 링크를 첨부하면서 회고를 마칩니다. 앞서 말씀드렸던 사항들을 최대한 지키려고 노력했지만, 잘 지켜졌는지.. 틈틈히 들여다 보고 유지보수 해나가려고 합니다.\n\n🔗방태모의 이력서"
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/index.html",
    "href": "posts/2022-09-12-statistics-playbook-1/index.html",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "",
    "text": "Photo by Christian Erfurt on Unsplash\n유튜브 채널 슬기로운 통계생활에서 운영하는 블로그에 기고했던 칼럼들을 최신화하여 다시 적어보려고 합니다. 첫 번째 칼럼 주제는 대학원에 대한 고민입니다. 저는 늘 고민과 생각이 많은 사람인데요.😂 때는 제가 통계학과 학부 4학년이던 2018년으로 거슬러 올라갑니다. 4학년 1학기 때는 학내 교환학생 프로그램에 신청하여 한 학기를 영국의 쉐필드대학(The University of Sheffield)에서 보내게 됩니다.\n아쉽게도 이 곳에서 통계학 전공 과목을 들을 기회는 없었습니다. 영어와 여러 가지 교양 과목을 수강했고, 시간이 많았던 때라 실컷 놀면서 자연스레 진로에 대한 고민을 다시 한 번 깊게 해보게 되었습니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/index.html#진로에-대한-고민",
    "href": "posts/2022-09-12-statistics-playbook-1/index.html#진로에-대한-고민",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "진로에 대한 고민",
    "text": "진로에 대한 고민\n당시 저는 통계학 전공을 살려 Data Scientist라 표현되는 직업을 갖고 싶었습니다. 그래서, 다음과 같은 두 가지 선택지에서 고민하기 시작했습니다.\n\n취업 준비\n통계학 대학원 진학\n\n어중이떠중이 기질이 있었던 저는 깊은 고민 끝에 2년이라는 시간을 통계학 대학원에 투자할 용기가 없어, 4학년 1학기를 쉐필드에서 마치고 한국으로 돌아가 취업 준비를 해보기로 결심했습니다. 학부 졸업 요건과 취업에 필요한 기본 요건1은 준비가 되어있었고, 무엇보다 마음 속에 지금 상태로 취업 준비를 해봐도 되겠다는 알 수 없는 자신감이 있었습니다. 그 이유는 지금 생각해보면 정말 별것 아닌 것들 때문이었죠. 기본적인 소양에 불과한 평균 평점(3.93/4.5)과 전공 평균 평점(4.1/4.5), 그리고 지금 다시 돌아보면 정말 형편없었던 R 숙련도에 대한 자부심은 제게 “이정도면 취업 준비를 해봐도 되지 않을까?” 하는 근거없는 자신감을 갖게 했죠. 이렇게 취업 준비를 결심하고 채용 공고를 들여다보면서 시간을 보내는 와중에, 계속해서 눈에 밟히던 두 가지 키워드가 있었습니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/index.html#호기심을-불러일으킨-두-가지-키워드",
    "href": "posts/2022-09-12-statistics-playbook-1/index.html#호기심을-불러일으킨-두-가지-키워드",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "호기심을 불러일으킨 두 가지 키워드",
    "text": "호기심을 불러일으킨 두 가지 키워드\n2016년 구글 딥마인드 팀이 개발한 바둑 AI 알파고가 이세돌과의 바둑 대국에서 압도적으로 승리를 거두며, 수십 년에 걸쳐 발전해온 딥러닝이라는 기술은 마치 최근 개발된 혁신적인 신기술인냥 세상의 주목을 받게 되었습니다. 매스컴의 주목에 따라 뉴스에서 종종 등장하던 두 단어 “머신러닝”과 “딥러닝”은 제게 또다른 호기심을 심어주었습니다.\n\n\n\n알파고와 대결한 이세돌 9단\n\n\n당시 학부 전공 과목으로 데이터마이닝을 수강한 상태였던터라 이러한 호기심 매우 자연스러운 현상이었던 것 같습니다. 그러나, 당시 통계학 학부 4학년에 불과하던 제게 머신러닝, 딥러닝과 같은 키워드는 머릿속에 큰 그림은 커녕 기존에 배웠던 전공 과목2들과 자연스러운 비교를 하면서 혼란을 가중시킬 뿐이였죠. 내가 배웠던 것들과 두 키워드는 어떤 관련이 있는지 알고 싶었고, 심지어는 “전자와 후자 중 어떤 것이 더 나은 방법론인가?” 와 같이 지금 생각해보면 참 바보 같은 생각을 했었습니다. 이러한 생각들은 Data Scientist의 꿈이 있었던 사람에게 왠지 모를 두려움과 불안감을 심어주었습니다. 그래서, 이것 저것 찾아보며 두 기술에 대해 이해해보려고 노력했습니다. 이 과정에서 문득 “과연 내가 이 상태로 실무에 나가서 호기심이 있는 기술, 또는 직무에 꼭 필요로 되는 기술이 있을 때 이러한 기술들을 독학하여 실무에 적용할 수 있을까?” 하는 생각을 했습니다. 4학년 2학기 졸업예정자 신분으로서 Data Scientist 직무로의 취업을 성공한다고 한들, 직무를 잘 수행해내며 스스로 발전할 수 있을지에 대한 의구심이 생겼죠. 그래서, 마음속에서는 대학원 진학에 대한 열망이 다시 한 번 피어오르고 있었습니다.\n대학원 진학이라는 길이 머릿속을 떠나지 않았습니다. 그래서, 4학년 2학기 딱 한 학기만 학부 졸업 예정자로서 취업 준비를 하며 제가 다니던 본교의 통계학 대학원 진학 준비를 병행해서 해보기로 했습니다. 당연히 취업 준비 결과는 참담했습니다:\n\n\n\n(학부 4학년 2학기) 2018년 하반기 채용 지원 결과\n\n\n그때 제 수준을 지금 생각해보면 이러한 결과는 당연했다는 생각이 드네요.😂"
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/index.html#참담한-취업-실패에서-배운-것들",
    "href": "posts/2022-09-12-statistics-playbook-1/index.html#참담한-취업-실패에서-배운-것들",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "참담한 취업 실패에서 배운 것들",
    "text": "참담한 취업 실패에서 배운 것들\n취업 준비 결과 실질적으로 손에 쥔 것은 없었지만, 수많은 채용공고를 보고 자기소개서를 쓰며 얻은 것과 배운 것들은 많았습니다:\n\n1 2018년까지 내가 해온 활동에 대한 정리\n대학원 졸업을 앞두고 내가 해온 활동에 대한 정리를 시작했다면, 졸업 논문 작업과 겹쳐 취업 준비에 매우 어려움을 겪었을거라 생각합니다.\n\n\n2 자기소개서를 쓰는 방식\n당시 썼던 자기소개서들을 올해 이직 준비를하며 썼던 자기소개서들과 비교해보면, 과거의 제가 썼던 자기소개서는 정말 형편없었습니다. 그러나, 첫 자기소개서를 대학원을 졸업하던 시기에 쓰기 시작했다면 그야말로 아찔하네요.\n\n\n3 우리나라 기업에서 Data Scientist/Analyst 채용시 원하는 구체적인 역량\n당시 완벽하게 깨우치지는 못했지만 수많은 채용공고를 들여다보니 준비해야할 방향이 조금이나마 보였던 것 같습니다. 취업이나 이직을 준비하시는 분들이 아니더라도 업계의 인재 영입 동향 파악을 위해 틈틈히 채용공고를 들여다보시는 것을 추천드립니다. 이번에 이직 준비를 하서면서도 우리나라 기업에서 낸 수많은 Data Scientist/Analyst 채용 공고를 들여다보았는데, 우리나라의 분석 직군들의 직무들도 점차 세분화 되어 잘 정립되어 가고 있다는 느낌을 받을 수 있었습니다. 물론, 여전히 채용 공고를 아무리 읽어 봐도 무슨 일을 하게 될 지 알 수 없는 그런 공고들도 종종 보였으나, 이건 어느 직무에서든 종종 보이는 성의없게 쓰여진 채용공고이므로 별 의미를 두지 않았습니다. 세분화되어 잘 정립되어 가고 있는 우리나라 분석 직군의 세부 직무들을 자세하게 알아보고 싶은 분들께는 변성윤님이 올려주신 🔗유튜브 영상을 추천드립니다.\n\n\n4 석사학위에 대한 필요성\n제 머릿 속에 대학원 진학이라는 키워드가 계속해서 맴돌았기 때문일지도 모르겠습니다. 2018년 당시 석박사 채용을 통해서만 Data Scientist 직무를 뽑는 경우도 종종있어 지원조차 못하는 기업들이 있었고3, 4년제 대졸 신입사원 채용으로 뽑더라도 우대사항에는 늘 석사학위 보유자 키워드가 함께 자리하고 있었습니다. Data Scientist/Analyst 채용 시 통계학 학사와 석사가 경쟁하면 기업 입장에서는 기본적으로 어떤 지원자가 더 매력적이겠습니까? 학위를 뛰어넘을만한 좋은 경력이나 포트폴리오를 갖고 있지 않는 이상 석사 학위 보유자를 선호할 것입니다. 단, 학위 자체가 어떤 특정한 어드벤티지를 준다고는 생각하지 않았습니다. 그만큼 석사 학위 보유자라는 책임감을 가져야하만하고 기업의 기대에 맞는 수준을 갖는 사람이 되어야만 한다고 생각했죠. 말 그대로 학위는 우리를 둘러싼 껍질에 불과한 기본 아이템이라고 표현하면 적절할까요? 그러나, 당시 학사 학위와 빈약한 포트폴리오를 갖고있던 제게 석사 학위를 뛰어넘을만한 Data Scientist 직무로의 취업 준비 방법은 떠오르지 않았죠. 그래서, 대학원 진학을 결정한 것이고요.\n이렇게 졸업 예정자로서 취업 준비를 한 번 해봤던 경험은 제게 통계학 대학원 진학에 대한 필요성을 직접 피부로 느끼게 해주었습니다. 대학원에 진학하여 열심히 공부할 수 있었던 동기부여 또한 마음 속 깊히 채워넣을 수 있었습니다. 그 결정을 한 당시를 돌아보면 통계학 대학원 진학에 대한 후회는 전혀 느껴지지 않습니다. 제 인생에 정말 탁월한 결정이였죠. 오히려 취업 준비를 하지 않고 통계학 대학원 준비에 올인했다면 더 좋은 결과를 가져올 수 있었을까? 하는 무의미한 생각을 하곤 합니다.😂 당시의 저처럼 현재 통계학 대학원 진학에 대한 고민을 품고 있는 학부생들의 선택은 당연히 본인의 몫입니다. 다만, 열심히 공부함과 동시에 자신을 부지런히 브랜딩한다는 가정 하에, 통계학 대학원 진학은 Data Scientist/Analyst로의 취업에 무조건 플러스가 될 것이라고 말씀드리고 싶네요.😀 이 말에도 대학원 진학에 확신이 서질 않는다면, 자신의 수준을 한 번 냉정하게 바라보시고 실무에 나갈 준비가 되었는지 본인에게 질문을 던져보시기 바랍니다. 질문의 답이 “Yes”라면 취업 준비를 해보시는 것 또한 정말 좋은 경험이 되실겁니다."
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/index.html#대학원에-들어가며-다짐했던-것",
    "href": "posts/2022-09-12-statistics-playbook-1/index.html#대학원에-들어가며-다짐했던-것",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "대학원에 들어가며 다짐했던 것",
    "text": "대학원에 들어가며 다짐했던 것\n저는 당시 Data Scientist/Analyst 직무로의 취업을 꿈꿨지만, 어느 기업 또는 어느 업계로 가고 싶다는 구체화는 전혀 되어있지 않던 상태였습니다. 그래서, 수많은 분야에서 의사결정의 도구로 사용되고 있는 Data Science/Analytics의 특성상 어떤 식으로 커리어 방향을 잡아 나갈지, 무엇을 공부해야 할지 참 막막했습니다. 이러한 혼란 속에서 다짐했던 것은 2가지 였습니다. 대학원을 졸업할 무렵에는 누구에게나 자신있다고 말할 수 있는 분석 언어 1가지, 분석 분야 1가지를 만들겠다고 말이죠.4 대학원을 졸업하던 당시 가장 자신있었던 분석 언어와 분석 분야는 R과 시계열 자료분석 이었습니다. 이 두 가지 무기로 첫 번째 직장에 취업을 하고 머릿 속에 그리던 직무를 수행할 수 있었죠. 2가지 다짐의 개인적 근거는 이쪽 업계는 이것저것 두루두루 잘하는 Generalist 보단 하나 혹은 두 가지를 특출나게 잘하는 Specialist를 선호한다고 생각했기 때문입니다. 다양한 백그라운드를 가진 사람들이 일하는 Data Science/Analytics 업계인 만큼, 두리뭉술한 사람 보다는 확실한 아이덴티티가 있는 사람이 채용시장에서 높은 선호도를 보일 것이라고 생각했죠. 이쪽 업계에 있을수록 Generalist가 되기란 참 어렵지 않나 하는 생각을 합니다. Generalist가 되려다 이것저것 얕게 알고있는 특색없는 Generalist가 될 수 있다는 것을 유념하시기 바랍니다.\n반대로, Data Scientist/Analyst 직무로의 취업을 꿈꾸며 어느 기업 또는 어느 업계로 갈지에 대한 구체화가 끝나신 분들도 있을 수 있겠죠. 이 분들은 참 똑똑한 분들이라 생각합니다. 이러면 취업 준비가 꽤 편해지니까요. 가고 싶은 기업의 링크드인, 기술 블로그 등을 팔로우 하고 채용 공고를 미리미리 들여다보며, 자신이 해당 포지션으로 가기 위해 배워야할 것들을 구체화할 수 있습니다. 그럼, 자연스레 해당 기업에서 원하는 Specialist가 되는 길을 걷게 되겠죠. 공부 외에도 적극적인 액션을 취해보시기를 권합니다. Specialist가 되기 위해 공부해야하는 분야에 커뮤니티가 있다면 가입해서 활동도 해보고, 링크드인에 자신이 가고자 하는 업계 또는 기업에서 Data Science/Analytics를 수행하고 있는 분들이 보인다면 콜드메일(메시지)을 보내보기도 하면서요.😀"
  },
  {
    "objectID": "posts/2022-09-12-statistics-playbook-1/index.html#맺음말",
    "href": "posts/2022-09-12-statistics-playbook-1/index.html#맺음말",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "section": "맺음말",
    "text": "맺음말\n2021년 2월에 통계학 석사학위를 마치고, Data Scientist 직무로 현업에 있는 사람으로서, 2018년의 저와 비슷한 고민을 하고 있는 분들께 하고 싶은 몇 마디를 하고 글을 마치려고합니다.\n대학원 졸업을 앞두고 제가 성장한 부분 중 가장 뜻 깊게 생각되는 부분은 특정 알고리즘에 관한 이해가 아닌, 앞으로도 쏟아져 나올 분석 방법론, 그리고 소프트웨어 역량이라 할 수 있는 R의 수많은 패키지 등을 혼자 공부하고 정리할 튼튼한 발판을 마련했다는 점이었습니다. 그리고, 무엇보다 나를 브랜딩하고 PR 할만한 장치들도(e.g. Github, 개인 블로그) 갖출 수 있었죠. 제가 학부를 졸업하고 바로 해당 직무로 취업을 했다고 한들 이렇게 튼튼한 발판과 자신을 브랜딩하는 나만의 방법이 없이는 언젠가 성장의 한계에 마주했을 거라고 생각합니다.\n그래서, 만약 본인이 Data Scientist/Analyst 직무에 대해 열정과 호기심이 있는 통계학 전공자라면 주저하지 마시고 대학원에 진학하시는 것을 추천합니다. 호기심이 이끄는대로 열심히 이것저것 찾아보며 공부하고, 자신을 가꿔나갈 각오가 되어있는 분들께 대학원은 이쪽 업계에서 무조건 플러스라고 생각하니까요. 물론, 꼭 통계학 대학원이 아니여도 상관없습니다. 본인이 원하는 직무와 좀 더 관련성있는 연구실을 운영 중인 다른 학과가 있다면 해당 학과로의 진학을 추천드립니다. 예를 들어, 만약 본인이 특히 머신러닝이나 딥러닝 쪽 연구를 통해 예측 모델링을 전문적으로 수행하는 사람이 되고 싶다면 통계학 대학원이 아닌 컴퓨터 과학(Computer Science, 또는 소프트웨어 학과라 일컫는) 쪽에서 해당 분야를 전문적으로 연구하시는 교수님의 연구실에 들어가는 것을 추천드리고싶습니다.5\n이런저런 이야기들을 하다보니 글이 꽤 길어졌습니다. 제가 느낀 것들을 바탕으로 쓴 글이니 정답이라고 생각하진 않으셨으면 합니다. 취업, 그리고 진로에 대한 길을 만들어 나가는 것에는 수많은 정답이 존재하니까요."
  },
  {
    "objectID": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#맺음말",
    "href": "posts/2022-08-07-coloring-guide-for-ggplot2/index.html#맺음말",
    "title": "ggplot2 컬러링 가이드",
    "section": "맺음말",
    "text": "맺음말\n분석을 시작하며 혼자 가볍게 EDA를 하는 단계에서 본 예제와 같이 시각화를 개선해나가는 작업은 필요로 되지 않을겁니다. 오히려 시간 낭비일수도 있구요. 그러나, 내가 얻은 인사이트를 전달하는 자리 또는 데이터를 기반으로 누군가를 설득해야하는 자리에서는 이 글에서 제공하는 몇 가지 방법이 꽤나 도움이 될 것이라고 생각합니다. 물론, 전달하고자 하는 내용이 한 눈에 들어도록 시각화를 수행하는 작업은 의외로 쉬울 때도 있지만, 꽤나 까다로운 과정을 거쳐야하는 상황도 존재합니다. 실무에서는 이와는 또다른 예상치 못한 까다로운 문제들을 겪는 상황들이 있을 수도 있구요. 다만, 본 글에서 그림의 퀄리티를 단계단계 개선해나간 바와 같이 전달하고자 하는 내용을 명확히하고 충분한 시간을 숙고해 그림을 개선해 나간다면, 뭐든 해결할 수 있을 것이라고 봅니다. 하고자 하는 시각화를 구현하지 못해낸다고 하더라도 그 과정 속에서 배우는 것은 분명히 존재할 것입니다. 처음부터 완벽하게 아름다운 시각화를 해낼 수 있는 사람은 없다는 것을 기억하셨으면 합니다.😁\n이번 포스팅을 준비하며 참고했던 글은 올해 봤던 데이터 시각화 관련 아티클 중 제게 가장 큰 임팩트를 주는 글이었습니다. 누구나 하는 평범한 시각화를 비범하게 만들어주는 글이라고 표현하면 적절할까요? 많은 사람들이 알았으면 하는 내용이라, 8월 서울 R 미트업에서 본 내용을 주제로 발표를 하기도 했습니다. 지금 이 글을 읽고 계신 여러분들에게도 좋은 인사이트를 줄 수 있는 글이 되었으면 합니다.\n\n\n\n\n\n\n세션정보를 보려면 누르세요\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-20\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.38 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.0.9   2022-04-28 [1] CRAN (R 4.2.0)\n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.2.0)\n gghighlight * 0.3.3   2022-06-06 [1] CRAN (R 4.2.0)\n ggplot2     * 3.3.6   2022-05-03 [1] CRAN (R 4.2.0)\n ggtext      * 0.1.1   2020-12-17 [1] CRAN (R 4.2.0)\n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.2.0)\n readr       * 2.1.2   2022-01-30 [1] CRAN (R 4.2.0)\n rmarkdown   * 2.14    2022-04-25 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n showtext    * 0.9-5   2022-02-09 [1] CRAN (R 4.2.0)\n showtextdb  * 3.0     2020-06-04 [1] CRAN (R 4.2.0)\n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.2.0)\n sysfonts    * 0.8.8   2022-03-13 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr       * 1.2.0   2022-02-01 [1] CRAN (R 4.2.0)\n tidyverse   * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-19-monthly-memory-202205/index.html",
    "href": "posts/2022-09-19-monthly-memory-202205/index.html",
    "title": "월간 회고록: 2022년 5월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash\n이직 준비하랴, 논문 작업하랴 미루고 미뤘던 월간 회고록들을 이제야 다시 적습니다. 5월 회고록을 작성하고 있는 이 시점은 어느덧 10월이군요. 부끄럽네요. 다행히 글감은 다 정해놨습니다. 하하. 지난 5월은 당시 재직 중인 연구센터에서 맡고 있는 학술연구과제 <빅데이터를 이용한 미세먼지 건강영향평가>에서 다변량 메타분석을 수행하는데에 가장 많은 시간을 할애했습니다. 1년반을 재직했던 직장에서 가장 많은 시간을 할애한 연구과제이기도 합니다. 분석해야할 질환들의 양이 정말 방대했거든요.🤯 대학원을 졸업하고 실무에 나와 처음으로 맡은 프로젝트인데, 데이터 수집, 데이터 마트 구축 부터 분석, 보고서 작성까지.. 정말 많은 양의 일을 혼자 처리했어야 했죠. 중간중간에 여러 난관에 봉착했고, 이 난관들을 하나하나 해결하면서 쾌감을 느끼기도 했습니다. 이번 월간 회고에서는 제가 겪은 크고 작은 난관들에 대해 이야기해보려고 합니다."
  },
  {
    "objectID": "posts/2022-09-19-monthly-memory-202205/index.html#sas를-다시-손에-잡다",
    "href": "posts/2022-09-19-monthly-memory-202205/index.html#sas를-다시-손에-잡다",
    "title": "월간 회고록: 2022년 5월",
    "section": "SAS를 다시 손에 잡다",
    "text": "SAS를 다시 손에 잡다\n본 연구과제는 건강보험공단 빅데이터 맞춤형 연구DB를 기반으로 진행되는데요. 자료의 특성 상 데이터 프라이버시때문에 건강보험공단에서 운영 중인 분석센터에 직접 가야지만 데이터를 열람하고 처리할 수 있습니다. 연구DB 신청 당시 분석 툴에 SAS + R을 선택할 수 있길래 당연히 데이터를 확인하고 처리하는 것도 R로 가능할 것이라 생각 했는데, 구축된 데이터는 SAS 서버 상에 놓여져있더군요. 보통 수십기가가 넘는 데이터를 처리해야하기 때문에, 분석센터의 데스크탑 로컬에 구축된 R로 원 자료를 핸들링하는 것은 어려웠습니다. 그래서, SAS로 원 자료를 랭글링\u001c1해야만 하는 상황이였습니다.\n좀 당황스러웠습니다. 하하.. SAS는 학부생 때 2년정도 배우고 손을 뗀 상태였기 때문이죠. 제가 원하는 데이터 마트를 원활하게 구축하기 위해서는 SAS에서 SQL과 Macro2의 적절한 활용이 필요했습니다. 다행히, 학부생때 공부했던 기억이 남아있었죠. 그러나, 약 3년의 시간이 흘렀기에 복습이 필요했고 위키독스의 SAS로 하는 기초 데이터 전처리, 핸들링(Data handling) 를 활용해 기본기를 다시 다진 후에 본격적인 랭글링 작업에 들어갔습니다. 분석센터로 출장을 가기 전 데이터 구조를 떠올리며 미리 코드를 작성하고, 현장에서 발생하는 오류가 있다면 디버깅만을 해주는 식으로 효율적으로 작업을 진행해 나갔습니다.\n이러한 작업을 반복하다보니 SAS SQL, Macro가 손에 익었고 제가 필요로 하는 데이터 마트를 수월하게 구축하고 추출해낼 수 있었습니다."
  },
  {
    "objectID": "posts/2022-09-19-monthly-memory-202205/index.html#풀고자-했던-문제",
    "href": "posts/2022-09-19-monthly-memory-202205/index.html#풀고자-했던-문제",
    "title": "월간 회고록: 2022년 5월",
    "section": "풀고자 했던 문제",
    "text": "풀고자 했던 문제\n본 과제 이름은 <빅데이터를 이용한 미세먼지 건강영향평가>이긴 하나, 6종의 대기오염원(\\({\\rm{PM}}_{10}\\), \\({\\rm{PM}}_{2.5}\\), \\({\\rm{NO}}_{2}\\), \\({\\rm{SO}}_{2}\\), \\({\\rm{O}}_{3}\\), \\({\\rm{CO}}\\))에 관해 모두 평가를 실시했어야 했습니다. 말이 거창해서 건강영향평가이지만, 기본적인 로직은 간단합니다. 질병의 일 발생/입원 건수를 \\(Y\\)로 잡고, 일 대기오염원 농도를 \\(X\\)로 잡아서 시계열 회귀 모델링을 하는 것이죠. 이를 통해 대기오염원들이 특정 질병의 발생/입원에 미치는 건강영향을 평가하는 것입니다. 여기서, 대기오염원이 미치는 효과는 비선형(non-linear)의 형태를 띤다는 점과 지연 효과(lag effect)3를 갖는다는 점까지 모형에서 고려해주어야 했죠. Distributed lag non-linear model(Antonio Gasparrini, Armstrong, 와/과 Kenward 2010) (이하, DLNM)은 이 둘을 동시에(simultaneously) 모델링 하게끔 해줍니다. 크게 봤을 때는 시계열 회귀모형의 일종이죠. 환경적 요인들로 Outcome을 모델링하는 생태학적 연구분야에서는 DLNM이 최신 시계열 회귀모형이라고 보시면 될 것 같습니다. 본 분야의 모델링 쪽 연구는 Gasparrini 교수가 주도하고 있구요.\n대학원에서 가장 많이 시간을 할애했던 연구 분야는 시계열 자료분석이었고, 이때 시계열을 회귀적으로 모델링하는 접근과 Distributed lag model에 대해 공부했던 경험이 있어 DLNM으로의 확장은 크게 어렵지 않았습니다."
  },
  {
    "objectID": "posts/2022-09-19-monthly-memory-202205/index.html#모형-최적화",
    "href": "posts/2022-09-19-monthly-memory-202205/index.html#모형-최적화",
    "title": "월간 회고록: 2022년 5월",
    "section": "모형 최적화",
    "text": "모형 최적화\nDLNM은 \\(X\\) 의 비선형 효과와 지연 효과까지 모델링하기 때문에, 튜닝해야하는 초모수(Hyperparameter)가 참 많은데요. 문제는 앞서 말씀드렸다시피 봐야할 질환들이 굉장히 많았다는 것이죠. 신경과 질환 6개, 정신과 질환 3개, 호흡기 질환 8개, 안과 질환 21개 총 38개 질환을 봐야했습니다. 여기서 문제는 2가지가 더 있었어요.\n\n6종의 대기오염원 각각에 대해 모델링\n\n여러개 대기오염원을 고려하는 경우, 다중공선성 이슈로 인해 회귀계수 추정량의 표준오차가 굉장히 커지는 문제가 있었습니다. 이 문제를 겪고 다시 조사했던 선행연구들을 들여다보니 모두 대기오염원 1개씩만 모형에 포함시켜 모델링했더군요.\n\n우리나라 6개 대도시 지역 각각에 대해 모델링\n\n지역마다 질환 발생 패턴은 비슷하나, 대기오염원의 농도가 지리적 환경에 따라 다르기 때문에 좀 더 정교한 모델링을 위해서는 각 지역마다 모델링이 필요했습니다.\n\n\n그럼 제가 최적화해야 했던 모델은 총 몇개였을까요? \\(38\\times6\\times6=1,368\\)개의 모형을 최적화해야 했습니다. 정녕 이것이 혼자 할 수 있는 양이였을까요?.. 참고로, 튜닝해야할 초모수가 하나가 아니라 3가지정도 됐었습니다. 이 사실을 깨닫고 한동안 꽤나 막막했습니다. 선행 연구 사례들을 살펴봐도, 논문 하나에 실리는 대부분의 연구는 일반적으로 특정 하나의 지역 및 질환에 국한하여 수행된 대기오염원 건강영향평가였죠.\n아시다시피, \\(Y\\)와 \\(X\\)간의 relationship에 관한 모델링이 주 목적인 회귀 모델링에서는 그 과정에서 분석자의 세심한 검토(e.g. 잔차분석)가 필요합니다. 그러나, 1,368개의 모형을 최적화해야하는 제게, 모형 하나하나를 세심하게 살펴서 모델링하기란 불가능했습니다. 그래서, 모형에 들어갈 공변량4에 대한 variable selection과 모형 최적화를 위해 몇 가지 초모수들을 튜닝하는, 추후에 논문을 평가할 리뷰어들이 납득이 가능한 하나의 획일화된 알고리즘을 만들 필요성을 느꼈습니다. 그 속에는 계산양이 적지않기 때문에 병렬처리 또한 구현을 해야 했고요. 선행연구들을 살펴봤으나 대개 이쪽 연구는 환경역학 쪽 학술지에 게재되서인지, 모델링 과정에 있어서 어떤 식으로 초모수 튜닝을 수행했고, 변수 선택은 어떻게 했고에 대한 것들이 매우 러프하게 기술되어 있었습니다. 그래서, 제가 알고리즘을 직접 짜봐야했죠.\n선형회귀분석에서 변수 선택을 할 때 쓰는 일반적인 알고리즘 중 하나인 best subset selection과 초모수 탐색 기법 중 가장 기본이라 할 수 있는 grid search를 결합하여 알고리즘을 만들어냈습니다. 이 알고리즘을 R로 구현하고 병렬처리까지 도입하여 1,368개의 모형에 대한 최적화를 해낼 수 있었습니다."
  },
  {
    "objectID": "posts/2022-09-19-monthly-memory-202205/index.html#지역별-분석-결과를-하나의-분석-결과로",
    "href": "posts/2022-09-19-monthly-memory-202205/index.html#지역별-분석-결과를-하나의-분석-결과로",
    "title": "월간 회고록: 2022년 5월",
    "section": "지역별 분석 결과를 하나의 분석 결과로",
    "text": "지역별 분석 결과를 하나의 분석 결과로\n분석을 완료하고 분석 결과를 요약해서 각 과 교수님들께 송부했으나, 문제가 하나 있었습니다. 6개 대도시 별 결과가 조금 상이한 질환들이 있었는데, 이것들을 어떤 합리적인 근거로 설명을 하느냐 였죠. 그래서, 나왔던 한 가지 해결책이 6개 지역별 분석 결과를 하나의 분석 결과로 요약하는 것이었습니다. 이를 위해서는, Gasparrini 교수가 제안한 다변량 메타분석(mutivariate meta-analysis, (A. Gasparrini, Armstrong, 와/과 Kenward 2012)) 방법론을 이용해야했습니다. 몇 가지 상황에 사용할 수 있는 여러 다변량 메타분석 방법론을 제시하고 있으니, 관심있으신 분들은 한번 쯤 들여다보셔도 좋을 것 같습니다. 최적화 해둔 모형들에 대해 다변량 메타분석을 수행하며, Reference로 참고할만한 R 코드들이 없어서 Github를 뒤지는데에 꽤 시간을 썼습니다. 그렇게 찾아낸 괜찮은 Reference를 바탕으로 다변량 메타분석을 R 상에서 수행하는 로직을 이해하고, 확장하여 적용할 수 있었습니다. 다변량 메타분석으로 지역별 분석 결과를 요약하는 것을 끝으로, 더이상의 추가적인 모델링은 없었습니다. 분석 결과를 요약하는 시각화를 꾸준하게 개선해나갔고, 동시에 본격적인 논문화 작업을 시작했습니다."
  },
  {
    "objectID": "posts/2022-09-19-monthly-memory-202205/index.html#맺음말",
    "href": "posts/2022-09-19-monthly-memory-202205/index.html#맺음말",
    "title": "월간 회고록: 2022년 5월",
    "section": "맺음말",
    "text": "맺음말\n이번 5월 회고록에서는 제 첫 직장이였던 곳에서 연구과제를 수행하며 겪었던 난관들에 대해서 이야기해봤는데요. 회고를 하며 그때 겪었던 감정들이 다시 새록새록 떠올랐습니다. 막막해보이는 것들도 충분한 시간을 할애하고 노력하면 결국 해결할 수 있는 것이라는 큰 교훈도 다시 한 번 얻을 수 있었고요. 회고라고 했지만, 어찌보면 나 이만큼 힘들었다고 푸념을 적어 놓은 것 같기도하네요.😂 실무에 계신 분들이든, 학업에 계신 분들이든 다 각자가 겪고 있는 난관이 있을텐데요. 쉽게 포기하는 마음보다는, “이 문제를 어떻게 해결할 수 있을까?”에 충분한 시간을 할애해보시기를 권해봅니다. 오늘도 이러한 난관을 헤쳐나가기 위해 고민하고, 노력하고 계신 분들께 응원의 말씀을 올리며 글을 마칩니다."
  },
  {
    "objectID": "posts/2022-10-08-monthly-memory-202205/index.html",
    "href": "posts/2022-10-08-monthly-memory-202205/index.html",
    "title": "월간 회고록: 2022년 5월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash\n이직 준비하랴, 논문 작업하랴 미루고 미뤘던 월간 회고록들을 이제야 다시 적습니다. 5월 회고록을 작성하고 있는 이 시점은 어느덧 10월이군요. 부끄럽네요. 다행히 글감은 다 정해놨습니다. 하하. 지난 5월은 당시 재직 중인 연구센터에서 맡고 있는 학술연구과제 <빅데이터를 이용한 미세먼지 건강영향평가>에서 다변량 메타분석을 수행하는데에 가장 많은 시간을 할애했습니다. 1년반을 재직했던 직장에서 가장 많은 시간을 할애한 연구과제이기도 합니다. 분석해야할 질환들의 양이 정말 방대했거든요.🤯 대학원을 졸업하고 실무에 나와 처음으로 맡은 프로젝트인데, 데이터 수집, 데이터 마트 구축 부터 분석, 보고서 작성까지.. 정말 많은 양의 일을 혼자 처리했어야 했죠. 중간중간에 여러 난관에 봉착했고, 이 난관들을 하나하나 해결하면서 쾌감을 느끼기도 했습니다. 이번 월간 회고에서는 제가 겪은 크고 작은 난관들에 대해 이야기해보려고 합니다."
  },
  {
    "objectID": "posts/2022-10-08-monthly-memory-202205/index.html#sas를-다시-손에-잡다",
    "href": "posts/2022-10-08-monthly-memory-202205/index.html#sas를-다시-손에-잡다",
    "title": "월간 회고록: 2022년 5월",
    "section": "SAS를 다시 손에 잡다",
    "text": "SAS를 다시 손에 잡다\n본 연구과제는 건강보험공단 빅데이터 맞춤형 연구DB를 기반으로 진행되는데요. 자료의 특성 상 데이터 프라이버시때문에 건강보험공단에서 운영 중인 분석센터에 직접 가야지만 데이터를 열람하고 처리할 수 있습니다. 연구DB 신청 당시 분석 툴에 SAS + R을 선택할 수 있길래 당연히 데이터를 확인하고 처리하는 것도 R로 가능할 것이라 생각 했는데, 구축된 데이터는 SAS 서버 상에 놓여져있더군요. 보통 수십기가가 넘는 데이터를 처리해야하기 때문에, 분석센터의 데스크탑 로컬에 구축된 R로 원 자료를 핸들링하는 것은 어려웠습니다. 그래서, SAS로 원 자료를 랭글링1해야만 하는 상황이였습니다.\n좀 당황스러웠습니다. 하하.. SAS는 학부생 때 2년정도 배우고 손을 뗀 상태였기 때문이죠. 제가 원하는 데이터 마트를 원활하게 구축하기 위해서는 SAS에서 SQL과 Macro2의 적절한 활용이 필요했습니다. 다행히, 학부생때 공부했던 기억이 남아있었죠. 그러나, 약 3년의 시간이 흘렀기에 복습이 필요했고 위키독스의 SAS로 하는 기초 데이터 전처리, 핸들링(Data handling) 를 활용해 기본기를 다시 다진 후에 본격적인 랭글링 작업에 들어갔습니다. 분석센터로 출장을 가기 전 데이터 구조를 떠올리며 미리 코드를 작성하고, 현장에서 발생하는 오류가 있다면 디버깅만을 해주는 식으로 효율적으로 작업을 진행해 나갔습니다.\n이러한 작업을 반복하다보니 SAS SQL, Macro가 손에 익었고 제가 필요로 하는 데이터 마트를 수월하게 구축하고 추출해낼 수 있었습니다."
  },
  {
    "objectID": "posts/2022-10-08-monthly-memory-202205/index.html#풀고자-했던-문제",
    "href": "posts/2022-10-08-monthly-memory-202205/index.html#풀고자-했던-문제",
    "title": "월간 회고록: 2022년 5월",
    "section": "풀고자 했던 문제",
    "text": "풀고자 했던 문제\n본 과제 이름은 <빅데이터를 이용한 미세먼지 건강영향평가>이긴 하나, 6종의 대기오염원(\\({\\rm{PM}}_{10}\\), \\({\\rm{PM}}_{2.5}\\), \\({\\rm{NO}}_{2}\\), \\({\\rm{SO}}_{2}\\), \\({\\rm{O}}_{3}\\), \\({\\rm{CO}}\\))에 관해 모두 평가를 실시했어야 했습니다. 말이 거창해서 건강영향평가이지만, 기본적인 로직은 간단합니다. 질병의 일 발생/입원 건수를 \\(Y\\)로 잡고, 일 대기오염원 농도를 \\(X\\)로 잡아서 시계열 회귀 모델링을 하는 것이죠. 이를 통해 대기오염원들이 특정 질병의 발생/입원에 미치는 건강영향을 평가하는 것입니다. 여기서, 대기오염원이 미치는 효과는 비선형(non-linear)의 형태를 띤다는 점과 지연 효과(lag effect)3를 갖는다는 점까지 모형에서 고려해주어야 했죠. Distributed lag non-linear model(Antonio Gasparrini, Armstrong, and Kenward 2010) (이하, DLNM)은 이 둘을 동시에(simultaneously) 모델링 하게끔 해줍니다. 크게 봤을 때는 시계열 회귀모형의 일종이죠. 환경적 요인들로 Outcome을 모델링하는 생태학적 연구분야에서는 DLNM이 최신 시계열 회귀모형이라고 보시면 될 것 같습니다. 본 분야의 모델링 쪽 연구는 Gasparrini 교수가 주도하고 있구요.\n대학원에서 가장 많이 시간을 할애했던 연구 분야는 시계열 자료분석이었고, 이때 시계열을 회귀적으로 모델링하는 접근과 Distributed lag model에 대해 공부했던 경험이 있어 DLNM으로의 확장은 크게 어렵지 않았습니다."
  },
  {
    "objectID": "posts/2022-10-08-monthly-memory-202205/index.html#모형-최적화",
    "href": "posts/2022-10-08-monthly-memory-202205/index.html#모형-최적화",
    "title": "월간 회고록: 2022년 5월",
    "section": "모형 최적화",
    "text": "모형 최적화\nDLNM은 \\(X\\) 의 비선형 효과와 지연 효과까지 모델링하기 때문에, 튜닝해야하는 초모수(Hyperparameter)가 참 많은데요. 문제는 앞서 말씀드렸다시피 봐야할 질환들이 굉장히 많았다는 것이죠. 신경과 질환 6개, 정신과 질환 3개, 호흡기 질환 8개, 안과 질환 21개 총 38개 질환을 봐야했습니다. 여기서 문제는 2가지가 더 있었어요.\n\n6종의 대기오염원 각각에 대해 모델링\n\n여러개 대기오염원을 고려하는 경우, 다중공선성 이슈로 인해 회귀계수 추정량의 표준오차가 굉장히 커지는 문제가 있었습니다. 이 문제를 겪고 다시 조사했던 선행연구들을 들여다보니 모두 대기오염원 1개씩만 모형에 포함시켜 모델링했더군요.\n\n우리나라 6개 대도시 지역 각각에 대해 모델링\n\n지역마다 질환 발생 패턴은 비슷하나, 대기오염원의 농도가 지리적 환경에 따라 다르기 때문에 좀 더 정교한 모델링을 위해서는 각 지역마다 모델링이 필요했습니다.\n\n\n그럼 제가 최적화해야 했던 모델은 총 몇개였을까요? \\(38\\times6\\times6=1,368\\)개의 모형을 최적화해야 했습니다. 정녕 이것이 혼자 할 수 있는 양이였을까요?.. 참고로, 튜닝해야할 초모수가 하나가 아니라 3가지정도 됐었습니다. 이 사실을 깨닫고 한동안 꽤나 막막했습니다. 선행 연구 사례들을 살펴봐도, 논문 하나에 실리는 대부분의 연구는 일반적으로 특정 하나의 지역 및 질환에 국한하여 수행된 대기오염원 건강영향평가였죠.\n아시다시피, \\(Y\\)와 \\(X\\)간의 relationship에 관한 모델링이 주 목적인 회귀 모델링에서는 그 과정에서 분석자의 세심한 검토(e.g. 잔차분석)가 필요합니다. 그러나, 1,368개의 모형을 최적화해야하는 제게, 모형 하나하나를 세심하게 살펴서 모델링하기란 불가능했습니다. 그래서, 모형에 들어갈 공변량4에 대한 variable selection과 모형 최적화를 위해 몇 가지 초모수들을 튜닝하는, 추후에 논문을 평가할 리뷰어들이 납득이 가능한 하나의 획일화된 알고리즘을 만들 필요성을 느꼈습니다. 그 속에는 계산양이 적지않기 때문에 병렬처리 또한 구현을 해야 했고요. 선행연구들을 살펴봤으나 대개 이쪽 연구는 환경역학 쪽 학술지에 게재되서인지, 모델링 과정에 있어서 어떤 식으로 초모수 튜닝을 수행했고, 변수 선택은 어떻게 했고에 대한 것들이 매우 러프하게 기술되어 있었습니다. 그래서, 제가 알고리즘을 직접 짜봐야했죠.\n선형회귀분석에서 변수 선택을 할 때 쓰는 일반적인 알고리즘 중 하나인 best subset selection과 초모수 탐색 기법 중 가장 기본이라 할 수 있는 grid search를 결합하여 알고리즘을 만들어냈습니다. 이 알고리즘을 R로 구현하고 병렬처리까지 도입하여 1,368개의 모형에 대한 최적화를 해낼 수 있었습니다."
  },
  {
    "objectID": "posts/2022-10-08-monthly-memory-202205/index.html#지역별-분석-결과를-하나의-분석-결과로",
    "href": "posts/2022-10-08-monthly-memory-202205/index.html#지역별-분석-결과를-하나의-분석-결과로",
    "title": "월간 회고록: 2022년 5월",
    "section": "지역별 분석 결과를 하나의 분석 결과로",
    "text": "지역별 분석 결과를 하나의 분석 결과로\n분석을 완료하고 분석 결과를 요약해서 각 과 교수님들께 송부했으나, 문제가 하나 있었습니다. 6개 대도시 별 결과가 조금 상이한 질환들이 있었는데, 이것들을 어떤 합리적인 근거로 설명을 하느냐 였죠. 그래서, 나왔던 한 가지 해결책이 6개 지역별 분석 결과를 하나의 분석 결과로 요약하는 것이었습니다. 이를 위해서는, Gasparrini 교수가 제안한 다변량 메타분석(mutivariate meta-analysis, (A. Gasparrini, Armstrong, and Kenward 2012)) 방법론을 이용해야했습니다. 몇 가지 상황에 사용할 수 있는 여러 다변량 메타분석 방법론을 제시하고 있으니, 관심있으신 분들은 한번 쯤 들여다보셔도 좋을 것 같습니다. 최적화 해둔 모형들에 대해 다변량 메타분석을 수행하며, Reference로 참고할만한 R 코드들이 없어서 Github를 뒤지는데에 꽤 시간을 썼습니다. 그렇게 찾아낸 괜찮은 Reference를 바탕으로 다변량 메타분석을 R 상에서 수행하는 로직을 이해하고, 확장하여 적용할 수 있었습니다. 다변량 메타분석으로 지역별 분석 결과를 요약하는 것을 끝으로, 더이상의 추가적인 모델링은 없었습니다. 분석 결과를 요약하는 시각화를 꾸준하게 개선해나갔고, 동시에 본격적인 논문화 작업을 시작했습니다."
  },
  {
    "objectID": "posts/2022-10-08-monthly-memory-202205/index.html#맺음말",
    "href": "posts/2022-10-08-monthly-memory-202205/index.html#맺음말",
    "title": "월간 회고록: 2022년 5월",
    "section": "맺음말",
    "text": "맺음말\n이번 5월 회고록에서는 제 첫 직장이였던 곳에서 연구과제를 수행하며 겪었던 난관들에 대해서 이야기해봤는데요. 회고를 하며 그때 겪었던 감정들이 다시 새록새록 떠올랐습니다. 막막해보이는 것들도 충분한 시간을 할애하고 노력하면 결국 해결할 수 있는 것이라는 큰 교훈도 다시 한 번 얻을 수 있었고요. 회고라고 했지만, 어찌보면 나 이만큼 힘들었다고 푸념을 적어 놓은 것 같기도하네요.😂 실무에 계신 분들이든, 학업에 계신 분들이든 다 각자가 겪고 있는 난관이 있을텐데요. 쉽게 포기하는 마음보다는, “이 문제를 어떻게 해결할 수 있을까?”에 충분한 시간을 할애해보시기를 권해봅니다. 오늘도 이러한 난관을 헤쳐나가기 위해 고민하고, 노력하고 계신 분들께 응원의 말씀을 올리며 글을 마칩니다."
  },
  {
    "objectID": "posts/2022-10-09-monthly-memory-202206/index.html",
    "href": "posts/2022-10-09-monthly-memory-202206/index.html",
    "title": "월간 회고록: 2022년 6월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash\n지난 6월에는 당시 재직 중이던 가천대 길병원 내 교수님들을 대상으로 하는 R 핸즈온 고급통계교육 강의를 맡게되었습니다. 매주 화요일 2시간씩 3주간 총 3회 강의를 진행했어요. 강의 주제는 정해져있었습니다. 첫 번째는 회귀분석과 ARIMA 모형, 두 번째는 생존분석, 세 번째는 매칭이었습니다. 강의 경험이라곤 대학원생 시절 실습 조교를 맡았던 것, 대전 통계교육원에 외부 출강을 하시던 지도교수님을 따라 보조강사를 맡았던 것 뿐이었습니다. 그래고, 스스로 자료를 준비하고 강의를 해보는 경험은 꽤 특별할 것다는 생각을 해서, 설레는 마음으로 준비를 했던 기억이 납니다. 그래서, 이번 6월 회고록에서는 강의 소회에 대해 써보려고 합니다."
  },
  {
    "objectID": "posts/2022-10-09-monthly-memory-202206/index.html#깨달은-점",
    "href": "posts/2022-10-09-monthly-memory-202206/index.html#깨달은-점",
    "title": "월간 회고록: 2022년 6월",
    "section": "깨달은 점",
    "text": "깨달은 점\n\n누군가를 가르치는 것은 정말 어려운 일임을 다시 한 번 느꼈다.\n짧은 강의 시간에 각 주제에 어떤 내용을 담아야할 지 결정하는 것 또한, 매우 어려운 일이다.\nR 초심자들을 대상으로 R 강의를 할 때는 간략한 R 코드 튜토리얼보다는, R과 Rstudio 설치 및 Rstudio 설정 팁을 먼저 전달해보자.\n\n일종의 아이스브레이킹 장치가 될 수도 있을 듯 하다. 나 같은 경우 강의 시간이 짧아서 미리 설치를 부탁하였고, Rstudio의 메인 화면 구성과 메인 화면의 각 탭이 의미하는 바, 간략한 설정 팁들을 공유해주었다.\n특히, SPSS와 같은 툴을 대부분 다뤄왔던 청중들의 특징을 고려해서, Rstudio에서 클릭을 통해 외부파일을 읽는 방법과 패키지를 설치하는 방법을 소개했더니 반응이 꽤 좋았다. 이게 R을 딥하게 배우고자 하는 사람에게 장기적으로 도움이 될 지는 모르겠지만..😅 어차피 자연스레 R이 익숙해지면 손으로 코딩하는 것이 더 편하다고 느낄 것이라 생각했다. 코딩을 하는 사람들이면 알다시피, 키보드에 올려진 두 손 중 하나를 마우스로 옮기는 것은 매우 귀찮은 일이기 때문이다.\n\n물고기를 잡아서 가져다주는 것 보다는, 물고기 잡는 법을 알려주자.\n\nR에서 help()를 활용하는 방법에 대해서는 꼭 전달하자.\n또 하나 초심자들에게 유용한 정보는 패키지에 관한 help()도 제공된다는 점이다. help(package = \"패키지명\")을 해주면 해당 패키지의 일종의 Full name도 알 수 있고, 패키지는 어떤 함수들로 구성되어 있는지, 각 함수들의 설명 또한 확인할 수 있다.\nR 코드에 오류가 났을 경우에 대처 방법1과 나만의 디버깅 방법2을 소개해주었다. 어차피 제한된 시간에 R 실력을 비약적으로 상승시킬 순 없다. 책에서 배운 것들 또는 강의에서 배운 것들을 실무에 적용하다보면, 분명히 예상치 못한 오류에 마주하게 될 것이다. 이때마다 이 오류를 잡기 위해 삽질을 하게될텐데, 이러한 삽질이 비로소 프로그래밍 실력을 크게 향상시켜준다고 생각한다. 그래서, 내 생각에 짧은 R 강의에서는 R 실력을 키울 수 있는 자신만의 노하우, 실무에서 문제가 발생 했을 때 자신만의 대처 방법, 디버깅 방법 등을 전달하는 것이 꼭 필요하다고 생각한다. 특정 주제를 가지고 긴~여정의 강의 계획이 있는 경우에는 굳이 불필요할 수도 있겠지만. 지금 글을 쓰며 내가 전달한 것들 중 또 하나 생각난 부분은, 자신이 객체를 만들때 이름을 무엇으로 할 지, 이것에 대한 고민 또한 충분히 하는 것이 좋다는 것이다. 미래의 나를 위해서도, 내 코드를 읽어야 할지도 모르는 누군가를 위해서도. 반복되는 코드들을 작성해야 하는 일이 있을 때도, 자신만의 일정한 컨벤션을 지켜 변수 명을 작성하는 습관은 매우 중요하다. 이러한 사소한 습관은 당신을 더 빨리 퇴근할 수 있게 만들어 줄 것이다.\n여기서 또 하나 추가하라면, R에서는 반복문 사용을 지양하고, 함수형 프로그래밍을 이용해 코딩하는 습관을 꼭 들이라는 것3. 마지막으로 R 코딩 시 매우 안좋은 습관을 하나만 더 이야기 하자면, 빈 객체를 만들어 각 요소에 할당을 하는 것이 아닌, a <- c(a, add)와 같이 벡터를 덮어 씌우는 습관 정도이다.\n\n청중들이 갖는 공통적인 특징을 고려한 강의 설계는 듣는 사람들을 기쁘게 한다.\n\n내 교육을 들으러온 사람들이 R을 배우고자 하는 목적은 의학 연구였으나, 대부분 청중들은 R 초심자4에 해당했다. 강의 설계를 최대한 이 특징을 고려해서 했다. R 실력이 어느정도 되는 분들로 구성되어있었다면 더 많은 것들을 전달할 수 있었을 텐데, 조금 아쉬움이 있다. 짧은 강의 시간에 R 코드들을 한 줄 한 줄 이해가도록 설명해줘야 했기에.\n가장 반응이 좋았던 부분은 웹베이스로 클릭을 통해 외부 파일을 읽고 PSM(Propensity score matching), 생존분석을 수행할 수 있는 앱을 소개했던 것이다. 문건웅 교수님이 운영 중인 웹에서 하는 R 통계에서 제공한다. PSM의 경우 Youtube 영상도 제공 중이다:\n\n가장 인상 깊었던 부분은 앱을 통해 수행한 분석 결과를 PPT로도 추출이 가능한데, 그림들이 벡터 그래픽으로 이루어져 있어 그림을 자유롭게 수정할 수 있다는 점이다. 이 부분에서 교수님들로부터 가장 좋은 반응을 이끌어냈던 것 같다. 연구를 수행할 때 투고하고자 하는 학회지에서 요구하는 포맷(e.g. 글씨체)을 매번 맞춰서 그림을 다시 그리기란 참 귀찮은 일이기 때문이다. 그리고, 글씨체외에 그려진 그림까지 세세하게 수정이 가능한데, 결과 조작은 결코 해서는 안된다고 당부를 드렸다.😂"
  },
  {
    "objectID": "posts/2022-10-09-monthly-memory-202206/index.html#부족했던-점",
    "href": "posts/2022-10-09-monthly-memory-202206/index.html#부족했던-점",
    "title": "월간 회고록: 2022년 6월",
    "section": "부족했던 점",
    "text": "부족했던 점\n\n나는 과연 그들에게 거인이 되어주었는가?\n\n연구 쪽에 유명한 격언이 있다. “거인의 어깨위에 서서 더 넓은 세상을 바라보라.” 사람들이 좋은 논문을 찾아서 읽고, 좋은 책을 사서 읽고, 돈을 지불하고 좋은 강의를 들으러 다니는 이유도 여기에 있다고 본다. 그렇다면, 과연 내 강의는 청중들에게 더 많은 시야를 확보해줌으로써 그들이 보던 세상보다 더 넓은 세상을 보여주었다고 할 수 있을까? 사실, 첫 번째 주제는 내가 연구하던 과제와 어느정도 관련이 있기도 했고 평소에 회귀분석, 시계열 자료분석에 관심이 많았기에 어느정도 그 역할을 했을 것이라고 생각한다. 그러나, 생존분석과 매칭은 내가 실무에 나와서 또는 대학원에서 연구까지 수행을 해본 적은 없는 주제에 해당했다. 특히나, 매칭은 내가 이번 강의를 준비하며 처음으로 공부했던 주제이기도 하고. 그래서, 강의를 준비하며 참 고민이 많았다. 청중들이 교육을 듣는 목적은 자신의 연구를 독립적으로 수행하는 데에 조금이라도 도움이 되기 위함인데, 과연 연구 동향, 해당 주제와 관련한 최신 R 라이브러리와 같은 것들을 내가 잘 알려줄 수 있을지? 후자는 잘해냈다고 생각하지만, 전자에 대해서는 아는 바가 거의 없어 이와 관련한 이야기는 드리지 못했다. 내가 강의를 하며 내 자신에게 가장 아쉬운 점이였다.\n\n강의 분량\n\n3개의 각 주제마다 2시간의 시간을 부여받았다고 했다. 내 계획은 해당 주제에서 배우는 기술들의 모티베이션, 분석 과정 및 분석 결과 해석에 꼭 필요로 되는 이론적 베이스를 설명한 뒤에 준비해 온 R 코드로 튜토리얼을 보여 준 뒤에 직접 실습해보는 시간을 가지는 것이었다. 그래서, 최대한 이론적 부분은 수식을 최대한 생략한 채로 컴팩트하게 준비했고, 튜토리얼도 초심자가 이해하기 쉽도록 최대한 자세하게 설명하려고 했다. 이러다보니 2시간은 턱없이 부족했다. 그래서, 직접 실습을 수행할 수 있는 시간은 조금밖에 부여해드리지 못했다. 아무리 코드를 따라친다고 한들, 초심자들에게는 예상치 못한 오류들이 발생할 수 있기 때문이다. 이 부분을 제대로 확보하지 못한 것도 참 아쉬웠다."
  },
  {
    "objectID": "posts/2022-10-09-monthly-memory-202206/index.html#뜻밖의-관심",
    "href": "posts/2022-10-09-monthly-memory-202206/index.html#뜻밖의-관심",
    "title": "월간 회고록: 2022년 6월",
    "section": "뜻밖의 관심",
    "text": "뜻밖의 관심\n마지막으로 짤막하게 하고싶은 이야기가 있다. 앞서 말했듯이, 세 번째 강의 주제인 매칭은 겉으로 종종 듣기만 하다가 처음으로 공부를 해 본 주제였다. 강의를 위해 매칭과 Weighting 방법론 중 하나인 IPTW에 대해 공부를 했다. 그리고, 자연스럽게 인과추론(Causal Inference)의 모티베이션에 대해 알게되었다. 나는 개인적으로 어떤 기술이나 알고리즘을 공부할 때 꼭 그 기술의 모티베이션, 백그라운드에 대해 찾아보는 편이다. 그래야, 왠지모를 찝찝함이 없어진다. 그렇기 때문에, 매칭과 Weighting에 대해 공부하며, 인과추론에 대해 공부하게 되는 것은 당연한 수순이었다.\n인과추론도 사실 처음 들어보는 분야는 아니였다. 그저, 매칭처럼 겉으로 듣기만하고 관심을 가지지 않았을뿐. 그런데, 찾아보니 왠걸? 인과추론의 모티베이션은 내게 너무나도 인상깊었다. 나는 예측 모델링 보다는 추론적 관점의 모델링에 유독 관심이 많았다. 학부, 대학원에서 통계학을 전공하며 전통적인 회귀분석의 모티베이션에 대해 고민했던 적도 있고, 실무에 나와서 시계열 회귀모형에 기반한 모델을 다루며 \\(Y\\)와 \\(X\\)간의 relationship을 추정해야 하는 일을 하고있었기 때문이다. 내가 안고있던 고민들은 인과추론이라는 학문이 갖는 모티베이션과 정확하게 일치했다. 그래서, 공부를 해보고 싶다는 마음이 정말 강하게 들었다.\n정말 뜻밖의 상황에, 새로운 분야에 관심을 가지게 될 줄이야. 이러한 관심을 갖고, 올해 우리나라에서 열린 인과추론 서머 워크샵 영상들을 찾아보고 공부를 하면서 또 하나의 흥미로운 소식을 접했다. G마켓에서 인과추론에 관심이 있는 Data Scientist를 찾고있다는 사실. 올해 초에 이직을 결심했던 나에게, 너무나도 매력적인 포지션이였다. 그리고, 2022년 10월 현재 나는 이 포지션에서 채용되어 일을 하고있다. 인생에 어떤 기회와 운은 정말 언제 찾아오는지 모르는구나..를 다시 한 번 더 느꼈다. 그 누가 알았을까? 이 강의를 준비하며 인과추론에 관심을 갖게되고, 이 관심이 성공적인 이직까지 이어질 줄은. 강의를 부탁받으며, 귀한 기회라 생각하고 진심으로 강의 준비를 해봐야겠다는 다짐을 했던 내게 고맙다는 말을 전한다."
  },
  {
    "objectID": "posts/2022-10-09-monthly-memory-202206/index.html#맺음말",
    "href": "posts/2022-10-09-monthly-memory-202206/index.html#맺음말",
    "title": "월간 회고록: 2022년 6월",
    "section": "맺음말",
    "text": "맺음말\n깨달은 점, 부족했던 점에 대해 읊다보니 오랜만에 반말로 글을 쓰게됐네요. 혹시나 불편하셨다면 사과드립니다.😅 모든 글이 그렇겠지만, 월간 회고는 쓰기 전에는 유독 귀찮은 마음이 듭니다. 제 머릿 속에서 느꼈던 감정들을 생생하게 꺼내서 전달하려다 보니 그런 것 같아요. 제 감정들을 전달하기 위해 문장들을 여러번 쓰고 지우는데, 이때 새록새록 새롭게 떠오르는 것들이 저를 꽤 즐겁게 합니다. 오늘도 행복한 감정으로 이 글을 마무리합니다."
  },
  {
    "objectID": "posts/2022-10-12-monthly-memory-202206/index.html",
    "href": "posts/2022-10-12-monthly-memory-202206/index.html",
    "title": "월간 회고록: 2022년 6월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash\n지난 6월에는 당시 재직 중이던 가천대 길병원 내 교수님들을 대상으로 하는 R 핸즈온 고급통계교육 강의를 맡게되었습니다. 매주 화요일 2시간씩 3주간 총 3회 강의를 진행했어요. 강의 주제는 정해져있었습니다. 첫 번째는 회귀분석과 ARIMA 모형, 두 번째는 생존분석, 세 번째는 매칭이었습니다. 강의 경험이라곤 대학원생 시절 실습 조교를 맡았던 것, 대전 통계교육원에 외부 출강을 하시던 지도교수님을 따라 보조강사를 맡았던 것 뿐이었습니다. 그래고, 스스로 자료를 준비하고 강의를 해보는 경험은 꽤 특별할 것다는 생각을 해서, 설레는 마음으로 준비를 했던 기억이 납니다. 그래서, 이번 6월 회고록에서는 강의 소회에 대해 써보려고 합니다. 그리고, 강의를 준비를 하며 갖게된 뜻밖의 관심이 좋은 결과로 이어질 수 있었던 부분 까지도요. 오늘은 생생한 감정의 전달을 위해 반말로 글을 작성해봤습니다."
  },
  {
    "objectID": "posts/2022-10-12-monthly-memory-202206/index.html#깨달은-점",
    "href": "posts/2022-10-12-monthly-memory-202206/index.html#깨달은-점",
    "title": "월간 회고록: 2022년 6월",
    "section": "깨달은 점",
    "text": "깨달은 점\n\n누군가를 가르치는 것은 정말 어려운 일임을 다시 한 번 느꼈다.\n짧은 강의 시간에 각 주제에 어떤 내용을 담아야할 지 결정하는 것 또한 매우 어려운 일이다.\nR 초심자들을 대상으로 R 강의를 할 때는 간략한 R 코드 튜토리얼보다는, R과 Rstudio 설치 및 Rstudio 설정 팁을 먼저 전달하는 것이 좋아보인다.\n\n일종의 아이스브레이킹 장치가 될 수도 있을 듯 하다. 나 같은 경우 강의 시간이 짧아서 미리 설치를 부탁하였고, Rstudio의 메인 화면 구성과 메인 화면의 각 탭과 세부 탭들이 의미하는 바, 간략한 설정 팁들을 공유해주었다.\n특히, SPSS와 같은 툴을 대부분 다뤄왔던 청중들의 특징을 고려해서, Rstudio에서 클릭을 통해 외부파일을 읽는 방법과 패키지를 설치하는 방법을 소개했더니 반응이 꽤 좋았다. 이게 R을 딥하게 배우고자 하는 사람에게 장기적으로 도움이 될 지는 모르겠지만..😅 어차피 자연스레 R이 익숙해지면 손으로 코딩하는 것이 더 편하다고 느낄 것이라 생각했다. 코딩을 하는 사람들이면 알다시피, 키보드에 올려진 두 손 중 하나를 마우스로 옮기는 것은 매우 귀찮은 일이기 때문이다.\n\n물고기를 잡아서 가져다주는 것 보다는, 물고기 잡는 법을 알려주자.\n\nhelp()를 활용하는 방법에 대해서는 꼭 전달하자. R에서 가장 중요한 것이라고 생각한다.\n또 하나 초심자들에게 유용한 정보는 패키지에 관한 help()도 제공된다는 점이다. help(package = \"패키지명\")을 해주면 해당 패키지의 일종의 Full name도 알 수 있다. 그리고, 해당 패키지는 어떤 함수들로 구성되어 있는지, 각 함수들의 설명 등을 확인할 수 있다.\nR 코드에 오류가 났을 경우에 대처 방법1과 나만의 디버깅 방법2을 소개해주었다. 어차피 제한된 시간에 R 실력을 비약적으로 상승시킬 순 없다. 책에서 배운 것들 또는 강의에서 배운 것들을 실무에 적용하다보면, 분명히 예상치 못한 오류에 마주하게 될 것이다. 특히, 초심자라면 더더욱. 이때마다 이 오류를 잡기 위해 삽질을 하게될텐데, 이러한 삽질이 비로소 프로그래밍 실력을 크게 향상시켜준다고 생각한다. 그래서, 내 생각에 짧은 R 강의에서는 R 실력을 키울 수 있는 자신만의 노하우, 실무에서 문제가 발생 했을 때 자신만의 대처 방법, 디버깅 방법 등을 전달하는 것이 꼭 필요하다고 생각한다. 특정 주제를 가지고 긴~여정의 강의 계획이 있는 경우에는 틈틈히 이러한 것들을 전달할 수 있지만, 시간이 얼마 없는 짧은 R 강의에서는 그저 자신이 준비해온 코드들만 읊고 설명하는데에 집중을 할수도 있다. 시간이 좀 부족하다는 생각이 들더라도, 자신만의 대처 방법, 디버깅 방법이나 책에서는 배울 수 없는 자신만의 노하우를 전달하는 데에 시간을 꼭 들이자.\n지금 글을 쓰며 내가 전달한 것들 중 또 하나 생각난 부분이 있다. 자신이 객체나 함수를 만들때 이름을 무엇으로 할 지, 이것에 대한 고민 또한 충분히 하는 것이 좋다는 것이다. 미래의 나를 위해서도, 내 코드를 읽어야 할지도 모르는 누군가를 위해서도. 반복되는 코드들을 작성해야 하는 일이 있을 때 또는 특정 프로젝트에 필요한 정말 긴 코드를 작성해야 할때, 자신만의 일정한 컨벤션을 지켜 변수명을 작성하는 습관은 정말 중요하다. 이러한 사소한 습관은 당신을 더 빨리 퇴근할 수 있게 만들어 줄 것이다.\n여기서 또 하나 추가하라면, R에서는 반복문 사용을 지양하고, 함수형 프로그래밍을 이용해 코딩하는 습관을 꼭 들이라는 것3.\n마지막으로 R 코딩 시 매우 안좋은 습관을 하나만 더 이야기 하자면, 반복문 작성 시 빈 객체를 만들어 각 요소에 할당을 하는 것이 아닌, a <- c(a, add)와 같이 벡터를 덮어 씌우는 습관 정도이다.\n\n청중들이 갖는 공통적인 특징을 고려한 강의 설계는 듣는 사람들을 기쁘게 한다.\n\n내 교육을 들으러온 사람들이 R을 배우고자 하는 목적은 의학 연구였으나, 대부분 청중들은 R 초심자4에 해당했다. 강의 설계를 최대한 이 특징을 고려해서 했다. R 실력이 어느정도 되는 분들로 구성되어있었다면 더 많은 것들을 전달할 수 있었을 텐데, 조금 아쉬움이 있다. 짧은 강의 시간에 R 코드들을 한 줄 한 줄 이해가도록 설명해줘야 했기에.\n가장 반응이 좋았던 부분은 웹베이스로 클릭을 통해 외부 파일을 읽고 PSM(Propensity score matching), 생존분석을 수행할 수 있는 앱을 소개했던 것이다. 문건웅 교수님이 운영 중인 🔗웹에서 하는 R 통계에서 제공한다. PSM의 경우 Youtube 영상도 제공 중이다:\n\n가장 인상 깊었던 부분은 앱을 통해 수행한 분석 결과를 PPT로도 추출이 가능한데, 그림들이 벡터 그래픽으로 이루어져 있어 그림을 자유롭게 수정할 수 있다는 점이다. 이 부분에서 교수님들로부터 가장 좋은 반응을 이끌어냈던 것 같다. 연구를 수행할 때 투고하고자 하는 학회지에서 요구하는 포맷(e.g. 글씨체)을 매번 맞춰서 그림을 다시 그리기란 참 귀찮은 일이기 때문이다. 그리고, 글씨체외에 그려진 그림까지 세세하게 수정이 가능한데, 결과 조작은 결코 해서는 안된다고 당부를 드렸다.😂 초심자들을 위한 좋은 앱을 무료로 제공해주시는 문건웅 교수님께 다시 한 번 감사의 말씀을 전한다."
  },
  {
    "objectID": "posts/2022-10-12-monthly-memory-202206/index.html#부족했던-점",
    "href": "posts/2022-10-12-monthly-memory-202206/index.html#부족했던-점",
    "title": "월간 회고록: 2022년 6월",
    "section": "부족했던 점",
    "text": "부족했던 점\n\n나는 과연 그들에게 거인이 되어주었는가?\n\n연구 쪽에 유명한 격언이 있다. “거인의 어깨위에 서서 더 넓은 세상을 바라보라.” 사람들이 좋은 논문을 찾아서 읽고, 좋은 책을 사서 읽고, 돈을 지불하고 좋은 강의를 들으러 다니는 이유도 여기에 있다고 본다. 그렇다면, 과연 내 강의는 청중들에게 더 많은 시야를 확보해줌으로써 그들이 보던 세상보다 더 넓은 세상을 보여주었다고 할 수 있을까?\n사실, 첫 번째 주제는 내가 연구하던 과제와 어느정도 관련이 있기도 했고 평소에 회귀분석, 시계열 자료분석에 관심이 많았기에 어느정도 그 역할을 했을 것이라고 생각한다. 그러나, 생존분석과 매칭은 내가 실무에 나와서 또는 대학원에서 연구까지 수행을 해본 적은 없는 주제에 해당했다. 특히나, 매칭은 내가 이번 강의를 준비하며 처음으로 공부했던 주제이기도 하고.\n그래서, 강의를 준비하며 참 고민이 많았다. 청중들이 교육을 듣는 목적은 자신의 연구를 독립적으로 수행하는 데에 조금이라도 도움이 되기 위함인데, 과연 연구 동향, 해당 주제와 관련한 최신 R 라이브러리와 같은 것들을 내가 잘 알려줄 수 있을지? 후자는 잘해냈다고 생각하지만, 전자에 대해서는 아는 바가 거의 없어 이와 관련한 이야기는 드리지 못했다. 강의를 하며 내 자신에게 가장 아쉬운 점이였다.\n\n강의 분량\n\n3개의 각 주제마다 2시간의 시간을 부여받았다고 했다. 내 계획은 해당 주제에서 배우는 기술들의 모티베이션, 분석 과정 및 분석 결과 해석에 꼭 필요로 되는 이론적 베이스를 설명한 뒤에 준비해 온 R 코드로 튜토리얼을 보여 준 뒤에 직접 실습해보는 시간을 가지는 것이었다. 그래서, 최대한 이론적 부분은 수식을 최대한 생략한 채로 컴팩트하게 준비했고, 튜토리얼도 초심자가 이해하기 쉽도록 최대한 자세하게 설명하려고 했다.\n이러다보니 2시간은 턱없이 부족했고, 직접 실습을 수행할 수 있는 시간은 조금밖에 부여해드리지 못했다. 아무리 코드를 따라친다고 한들, 초심자들에게는 예상치 못한 오류들이 발생할 수 있기 때문이다. 강의 시간에 맞게 분량을 준비하는 것은 참 어렵다고 느꼈다. 그만큼 전달하고 싶은 내용이 많았다는 뜻이기도 하고, 그것들을 쳐내는 것은 무척이나 어려웠다."
  },
  {
    "objectID": "posts/2022-10-12-monthly-memory-202206/index.html#뜻밖의-관심",
    "href": "posts/2022-10-12-monthly-memory-202206/index.html#뜻밖의-관심",
    "title": "월간 회고록: 2022년 6월",
    "section": "뜻밖의 관심",
    "text": "뜻밖의 관심\n마지막으로 짤막하게 하고싶은 이야기가 있다. 앞서 말했듯이, 세 번째 강의 주제인 매칭은 겉으로 종종 듣기만 하다가 처음으로 공부를 해 본 주제였다. 강의를 위해 매칭과 Weighting 방법론 중 하나인 IPTW에 대해 공부를 했다. 이 과정에서 자연스럽게 인과추론(Causal inference)의 모티베이션에 대해 알게되었다. 나는 개인적으로 어떤 기술이나 알고리즘을 공부할 때 꼭 그 기술의 모티베이션, 백그라운드에 대해 찾아보는 편이다. 그래야, 왠지모를 찝찝함이 없어진다. 그렇기 때문에, 매칭과 Weighting에 대해 공부하며, 인과추론에 대해 공부하게 되는 것은 당연한 수순이었다.\n인과추론도 사실 처음 들어보는 분야는 아니였다. 그저, 매칭처럼 겉으로 듣기만하고 관심을 가지지 않았을뿐. 그런데, 찾아보니 왠걸? 인과추론의 모티베이션은 내게 너무나도 인상깊었다. 나는 예측 모델링 보다는 추론적 관점의 모델링에 유독 관심이 많았다. 학부, 대학원에서 통계학을 전공하며 전통적인 회귀분석의 모티베이션에 대해 고민했던 적도 있고, 실무에 나와서 시계열 회귀모형에 기반한 모델을 다루며 \\(Y\\)와 \\(X\\)간의 relationship을 추정해야 하는 일을 하고있었기 때문이다. 내가 안고있던 고민들은 인과추론이라는 학문이 갖는 모티베이션과 정확하게 일치했다. 그래서, 공부를 해보고 싶다는 마음이 정말 강하게 들었다.\n예상치도 못한 뜻밖의 상황에, 이렇게 설레는 마음을 품게되는 새로운 분야를 만날 줄이야. 이때 이후로, 퇴근을 하고 새로운 내 공부 루틴은 올해 우리나라에서 열린 인과추론 서머 워크샵 영상들을 찾아는 것이었다. 그리고, 얼마 뒤 또 하나의 흥미로운 소식을 접했다. G마켓에서 인과추론에 관심이 있는 Data Scientist를 찾고있다는 사실. 올해 초에 이직을 결심했던 나에게, 너무나도 매력적인 포지션이였다. 그리고, 기회라고 생각했다. 서류 지원을 하며 썼던 지원 동기는 그 어떤 때보다 진심이었다. 진심이 닿았을까? 운이 좋게도, 2022년 10월 현재 나는 G마켓에서 일을 하고있다. 그 누가 알았을까? 이 강의를 준비하며 인과추론에 관심을 갖게되고, 이 관심이 성공적인 이직까지 이어질 줄은. 인생에 어떤 기회와 운은 정말 언제 찾아올 지 모르며, 이를 잡는 것 또한 그 사람의 능력이다. 누군가에게는 운이라고 생각되어 지지 않는 그 기회는, 아무도 모르게 지나갈 수 있다. 강의를 부탁받으며, 귀한 기회라 생각하고 진심으로 강의 준비를 해봐야겠다는 다짐을 했던 내게 다시 한 번 고맙다는 말을 전한다."
  },
  {
    "objectID": "posts/2022-10-12-monthly-memory-202206/index.html#맺음말",
    "href": "posts/2022-10-12-monthly-memory-202206/index.html#맺음말",
    "title": "월간 회고록: 2022년 6월",
    "section": "맺음말",
    "text": "맺음말\n깨달은 점, 부족했던 점에 대해 읊다보니 오랜만에 반말로 글을 쓰게됐습니다. 사실 교육 마지막 날에 강의를 마무리하며, 교수님들께 제 강의가 연구에 조금이라도 도움이 되었으면 한다고, 넌지시 제 강의가 어땠냐고 물어봤었습니다. 답정너죠. 다행히, 반응은 좋았습니다.😂 웃어주시는 분들도 많았고 연구에 도움이 많이 될 것 같다고 말씀해주시는 분들도 있었어요. 아무 반응이 없었다면 그야말로 아찔하네요. 일종의 답정너로 받은 박수이지만, 어떤 교수님께는 좋은 강의 감사하다며, 작은 보답까지 해주셨습니다.😀 강의를 마칠때 받았던 박수, 좋은 말씀들은 지난 3주간 업무까지 병행해가며 강의 준비를 했던 지난 시간들에 대한 큰 보상을 받는 듯한 느낌이었습니다. 다음에 이러한 강의 기회가 있다면 더 완벽하게 준비해봐야 겠다는 다짐도 했습니다. 정말 의미있고 재밌었던 경험이었습니다.\n모든 글이 그렇겠지만, 월간 회고를 쓰기 전에는 유독 더 귀찮은 마음이 듭니다. 제 머릿 속에서 느꼈던 감정들을 생생하게 꺼내서 전달하려다 보니 그런 것 같아요. 제 감정들을 전달하기 위해 문장들을 여러번 쓰고 지우는데, 이때 문득 떠오르는 그때의 감정들이 저를 꽤 즐겁게 합니다. 오늘도 행복한 감정으로 이 글을 마무리합니다. 강의 자료들은 제 블로그 우측 상단 리소스 탭의 발표 아카이브에서 만나보실 수 있습니다."
  },
  {
    "objectID": "posts/2022-10-12-about-data-science/index.html",
    "href": "posts/2022-10-12-about-data-science/index.html",
    "title": "데이터 과학에 관해",
    "section": "",
    "text": "그림1. 데이터 과학 (Source: 유튜브 채널 <인과추론의 데이터과학>)\n유튜브 채널 <인과추론의 데이터과학>의 2020년 6월 강의 영상을 지난 7월에 인과추론에 관심을 가지며 우연히 보게되었죠:\n강의 연사는 UNGC의 박지용 교수님입니다. 해당 영상의 38분부터 약 10분 간 위 그림 1을 가지고 데이터 사이언스에 대한 생각과 우리나라 데이터 사이언스 업계에 관한 생각을 풀어내십니다. 너무나도 흥미로웠고 재밌었어요. 제가 오랜 시간 생각하고 고민했던 지점들과 일치하는 부분이 상당히 많아서 였을까요? 그래서, 영상 내용이 참 반갑기도 했습니다. 그래서, 영상에서 인상 깊었던 대목과 그 대목이 인상 깊었던 개인적 이유에 대해 써보려고 해요. 사실, 영상을 보자마자 기록해두고 싶은 맘이 있어 지난 7월 커리어리에 글을 썼었습니다. 좀 더 정제해서 그때 감정을 떠올려 글을 정리해보려고 합니다."
  },
  {
    "objectID": "posts/2022-10-12-about-data-science/index.html#데이터-과학-붐",
    "href": "posts/2022-10-12-about-data-science/index.html#데이터-과학-붐",
    "title": "데이터 과학에 관해",
    "section": "데이터 과학 붐",
    "text": "데이터 과학 붐\n\n데이터 과학은 최근 몇년간 큰 붐이 일었다. 근데 내가 개인적으로 느끼는 아쉬운 점은 우리나라에서의 데이터 과학에 관한 유행은 한쪽으로(예측 방법론) 치우쳐져있지 않나 하는 개인적 우려를 가지고 있다.\n\n저 또한 개인적으로 우리나라 데이터 사이언스 업계는 예측 모델링에 상당히 치우쳐 있다고 생각을 했었습니다. 2년 전의 영상인데도 같은 지점을 지적하고 계셨어요. 박지용 교수님이 2020년에 했던 개인적 우려는 2022년에도 여전한 듯 합니다. 올해 초 이직을 결심하며 우리나라 데이터 사이언스 업계의 수십개의 채용 공고를 들여다 보았는데요. “추론”이라는 키워드는 딱 한 번만 볼 수 있었습니다. 물론, 해당 포지션에서 풀고자 하는 문제에 추론은 정말로 필요없었을 수도 있습니다. 그러나, “Data Science를 구성하고 있는 대부분의 세부 분야들이 그렇듯이, 추론”과 “예측” 또한 두부 자르듯이 둘을 완전히 나눠서 볼 수 있는 것은 아닙니다. 그래서인지, 수십개의 채용공고 중에 “추론”이라는 키워드가 들어간 채용 공고는 제게 꽤나 특별하게 느껴졌습니다. 공교롭게도, 현재 저는 그 곳에서 일을 하고 있네요. 당시, 실제로 어떤 철학과 방식으로 인과추론 방법론을 활용해 예측 모델링에 들어갈 feature를 추출해내는지 매우 궁금한 상태로, 설레는 맘을 품고 서류를 지원하던 기억이 납니다."
  },
  {
    "objectID": "posts/2022-10-12-about-data-science/index.html#데이터-과학이란",
    "href": "posts/2022-10-12-about-data-science/index.html#데이터-과학이란",
    "title": "데이터 과학에 관해",
    "section": "데이터 과학이란?",
    "text": "데이터 과학이란?\n\n“데이터 과학은 하나의 특정 학문이라기 보다는 굉장히 포괄적이라고 생각을 한다. 특정 현상이나 대상에 대해 데이터를 수집하고 또는 만들어내고, 그 데이터를 가지고 분석을 수행하여 의사결정에 도움이 되는 인사이트를 추출하는 과학적 접근법들을 통칭하는 것이 데이터 과학이라고 생각한다. 그래서, 어떤 특정 학문 분야가 데이터 과학을 한다기 보다는, 데이터 과학의 어떤 철학을 가지고 많은 분야에서 연구를 하고있다고 생각함.”\n\n저는 “데이터 과학”이라는 것을 하고는 있지만, “데이터 과학”을 모르는 사람들에게 이 영역을 어떻게 정의할 수 있을까에 대한 고민을 가지고 있었습니다. 데이터 과학은 수학, 통계학 등 순수학문의 위에 실용학문이 얹어진 융합학문이라고 생각했었기에 더더욱 정의가 어려웠어요. 사실, 학문이라는 표현 또한 적절할 지는 모르겠습니다. 과연 “데이터 과학”을 학문이라 표현하는 것이 적절할까요? 개인적으로는 “도구”라는 단어가 더욱 적절하다고 생각합니다. 그래서, 박지용 교수님이 위와 같이 정의한 데이터 과학이 제가 생각했던 데이터 과학과 가장 가까웠던 것 같습니다.\n데이터 과학과 마찬가지로 “빅데이터”에 대한 정의도 여전히 상당히 모호하다고 생각합니다. 단순히 크기가 큰 데이터? 오죽하면 아래와 같은 유명한 말이 있죠.\n\n빅데이터는 십대의 섹스와 같다. 다들 그것에 대해 말하지만, 진짜로 어떻게 하는지는 아무도 모르고, 나만 빼고 모두가 하고 있을 거라 생각한다. 그래서 다들 자기도 하고 있다고 주장한다. - 댄 애리얼리 (Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, and everyone thinks everyone else is doing it, so everyone claims they are doing it. - Dan Ariely)\n\n그래서, 저는 개인적으로 빅데이터라는 단어를 특별하게 의미를 담아 정의하는 것을 그렇게 좋아하지 않습니다. 빅데이터를 정의하는 데에는 단순히 크기가 큰 데이터 정도로 정의하는 것으로 충분하다고 생각하지만, 그 크기에 대한 기준에는 또 주관이 개입하기 마련이니까요. 또 다른 이들은 “빅데이터”라는 단어에 그 이상의 의미를 부여하기도 합니다. 그래서, “빅데이터 공부하기”, “빅데이터 이해하기”라는 말이 존재하는 것이 아닐까요?"
  },
  {
    "objectID": "posts/2022-10-12-about-data-science/index.html#통계-머신러닝-인과추론",
    "href": "posts/2022-10-12-about-data-science/index.html#통계-머신러닝-인과추론",
    "title": "데이터 과학에 관해",
    "section": "통계, 머신러닝, 인과추론",
    "text": "통계, 머신러닝, 인과추론\n\n“직접적인 데이터 과학 툴로써는 예측을 위한 머신러닝이나 인과추론을 위한 계량 경제학은 사실 각자의 목적을 가지고 독립적인 분야로 발전하고 있다. 그렇다고해서 통계가 중요하지 않은가? 절대 그렇지 않다. 사실 머신러닝이나 계량 경제는 기본적으로 통계에 이론적 바탕을 두고 있다. 그래서, 나는 통계라는 것을 데이터 과학이 떠있는 바다 그 자체라 표현하고 싶다. 그 바다가 없다면 사진 속 쌍끌이 배 자체가 항해를 할 수 없기 때문. 통계가 인과추론을 위해 반드시 필요하다고 생각하진 않으나, 데이터 분석에 관한 깊이 있는 이해를 위해서는 통계학은 필수적이다.”\n\n학부, 대학원을 통계학으로 전공하며 회귀분석에 관심이 꽤 있었던 제게, 추론(inference)이라는 영역은 항상 예측(prediction)보다 더 매력적으로 다가왔습니다. 그래서, 추론(inference)이라는 영역은 데이터 과학을 이끌어가는 수많은 학문들 중 통계학만의 것이라고 생각했었는데, 계량 경제학이 인과추론을 이끌어가는 학문이였다는 것은 이 영상을 통해 알게되었습니다. 참 흥미로웠어요.😀추론적 관점의 모델링에 대한 관심은 회귀분석이 갖는 가치에 대해 생각해보며 시작되었어요. 제 입으로 말하기도 부끄럽지만, 통계학과 대학원에서 회귀분석 수업을 진행하시는 교수님이 “회귀분석은 구식이다.”라는 충격적인 말을 들은 때가 바로 이 생각의 계기였죠. 저는 결코 그렇지 않을 것이라 생각했기 때문이죠. 회귀분석이 가지는 가치에 대해 고찰하며, 책에서 찾아보기는 어려운 개인적인 궁금증이 생길 때면 지도교수님을 찾아가 질문을 던지기도 했습니다.\n통계학에서는 회귀분석을 기본적으로 추론적 관점으로 접근합니다. 그래서, 잔차분석은 회귀분석의 꽃이라 불리기도 해요. 다중공선성에 관한 고민을 하는 이유도 바로 이 추론에 관심이 있기 때문이죠. 머신런이에서 지도학습 기법을 소개할 때에도 회귀분석 기법은 맨처음에 등장합니다. 이때에 회귀분석은 철저하게 예측 모델링 관점에서 소개되죠. 그래서, 다중공선성, 잔차분석에 관한 이야기는 나오지 않어요. 이러한 두 관점의 차이는 Ridge, Lasso와 같은 벌점 회귀(penalized regression) 방법론에 대한 모티베이션에서도 드러납니다. 통계학에서 벌점 회귀 방법론의 근본적 모티베이션은 다중공선성으로 인해 발생하는 회귀계수 추론의 어려움이지만, 머신러닝에서 벌점 회귀 방법론이 갖는 근본적 모티베이션은 Regularization1입니다. 다른 말로 하면, 선형 회귀에는 unobserved data set에 관한 성능을 조절할만한 모수가 없다는 것인데요. 과적합(overfitting)이 되는 것을 방지할 수 있는 장치가 없다는 이야기라고도 표현할 수 있겠네요.\n물론, 통계학과에서 회귀분석을 가르칠 때 “추론적 모델링 관점”이라는 등의 설명은 따로 하지 않아요. 그러나, 저는 “회귀분석은 구식이다.”라는 충격적인 통계학과 교수님의 워딩을 듣고 회귀분석이 갖는 가치에 대한 고찰을 시작했고, 추론적 모델링의 관점과 예측 모델링의 관점에 따라 같은 방법론도 다른 방식으로 설명되고 이론 전개가 가능하다는 것을 이해할 수 있었습니다. 그래서, 그 충격적인 말을 해준 교수님께 지금은 참 감사한 마음이 듭니다. 물론, 이러한 고찰을 하지 않아도 다음 학기에 지도교수님과 고차원 데이터 분석을 공부하며 고도화된 형태의 벌점 항을 갖는 penalized regression model이 고차원 데이터(n<p)의 예측 모델링에서 갖는 가치를 알게 되긴 했지만요.2 이렇게 두 관점에 대한 이해가 어느정도 정립되고 난 뒤에 접한 인과추론 방법론은 내게 무척이나 흥미로울 수 밖에 없었습니다.\n예측 방법론, 인과추론 방법론 둘 중 어느 것이 더 중요하다고 이야기 하고싶은 것이 절대 아닙니다. 데이터 과학을 이끌어가고 있는 다양한 세부 분야들이 그렇지만, 이 둘 또한 결코 두부 자르듯이 나눌 수 있는 방법론, 관점도 아니라고 생각합니다. 인과추론이 가지는 가치는 추론 모델링에서만 발휘되지 않습니다. 예측 모델링에서도 인과추론이 갖는 가치는 충분히 존재합니다. 실제로 인공지능, 머신러닝 분야의 최대 학회 중 하나인 NeurIPS에서도 Causal Inference Workshop이 열리고 있죠. 두 방법론과 관점 모두 데이터 과학에서 너무나도 중요합니다. 다만, 우리나라 데이터 과학 업계는 여전히 예측 방법론에 지나치게 치중되어 있다는 점을 이야기하고 싶습니다. 둘은 필히 균형을 맞추어 양립하여 가야한다고 생각합니다. 그래야, 데이터를 기반으로 바보같은 의사결정을 내리는 일을 피할 수 있게 해줄거라고 생각해요 글 문두에서 제시한 그림 속에서 통계라는 바다 위에 떠있는 데이터 과학을 이끌어가는 쌍끌이 배가 곧 인과추론 방법론과 예측 방법론이듯이 말이죠."
  },
  {
    "objectID": "posts/2022-10-12-about-data-science/index.html#맺음말",
    "href": "posts/2022-10-12-about-data-science/index.html#맺음말",
    "title": "데이터 과학에 관해",
    "section": "맺음말",
    "text": "맺음말\n너무나도 인상깊었던 영상이라 소감이 정말 길었네요.😂이 글에 풀어놓은 제 생각들이 여러분들이 데이터 과학을 바라보는 새로운 관점을 제시하는 계기가 되었으면 합니다. 좋은 영상을 이렇게 무료로 풀어주는 인과추론의 데이터 과학 유튜브 채널 운영진 님들께 정말 감사한 마음을 전합니다. 그리고, 인과추론 분야에 관한 수준 높은 강의를 무료로, 그것도 한국어로 제공해주시는 UNGC 박지용 교수님께도 큰 감사함을 전합니다."
  },
  {
    "objectID": "posts/2022-10-20-monthly-memory-202207/index.html",
    "href": "posts/2022-10-20-monthly-memory-202207/index.html",
    "title": "월간 회고록: 2022년 7월",
    "section": "",
    "text": "Photo by Fredy Jacob on Unsplash\n지난 7월에는 길게 적어보고 싶을 정도로 특별했던 일은 없었던 것 같아요. 7월 회고록을 10월 중순이 되어서야 적고있어서 그런걸까요?😂 이런 달에는 3가지 측면으로 회고를 해보려고 해요. 잘했던 점, 성장했던 점, 부족했던 점에 대해서요. 사실, 길게 적고 싶은 특별한 일 하나를 적는 것 보다는 이게 더 본질적인 회고록인 것 같긴하네요. 특별할 것은 없었지만, 그렇다고 또 심심하진 않았던 7월의 회고를 시작해봅시다."
  },
  {
    "objectID": "posts/2022-10-20-monthly-memory-202207/index.html#잘했던-점",
    "href": "posts/2022-10-20-monthly-memory-202207/index.html#잘했던-점",
    "title": "월간 회고록: 2022년 7월",
    "section": "💯 잘했던 점",
    "text": "💯 잘했던 점\n\nG마켓 Data Scientist 지원\n\n페이스북 Causal Inference KR 커뮤니티를 운영하고 계신 선호님으로부터, G마켓에서 인과추론에 관심이 있는 Data Scientist를 모집한다는 소식을 접했어요. 아마, 기억을 되새겨보면 7월 3일에 서류를 지원했던 것 같아요. 지금은 저의 팀장님이 되셨네요. 신기합니다. 한창 인과추론에 관심을 가지기 시작한 때라, 퇴근하고 <인과추론의 데이터 과학>에서 주관하여 진행했던 Korea Summer Workshop on Causal Inference 2022에서 올려준 영상들을 보고있던 때였거든요. 부푼마음을 안고 서류를 준비했던 기억이 나네요. 좋게 봐주셔서 이렇게 G마켓에서 일하게 되었습니다.\n서류를 합격하고 얼떨떨한 마음을 가지고 1차면접을 준비했어요. 7월 15일 오후 2시에 Zoom으로 화상면접을 봤습니다. 이직로그는 따로 작성해볼 생각이 자세한 사항을 쓰진 않겠습니다. 1차면접을 복기하며 썼던 글을 보니, 꽤 즐거웠던 면접이었어요. 물론, 긴장도 많이하긴 했지만.\n2차면접도 화상면접이었습니다. 8월 3일 오전 10시에 봤었군요. 2차면접도 시간가는 줄 모르고 이야기하고 나왔던 것 같아요. 지금도 기억나는게, 면접관님이 자신이 누구고 어떤 업무를 하고 있는지 먼저 설명해주시면서 면접을 시작했는데, 이때 자연스럽게 긴장이 풀렸던 것 같습니다. 2차면접도 기분좋게 보고 나왔던 기억이 납니다.\n업무를 병행하며 서류 준비, 면접 준비 하랴 꽤나 고생했던 것 같습니다.\nG마켓 기업 홈페이지가 신설되었어요. 둘러보고 가셔요😀:\n\nhttps://corp.gmarket.com/\n\n\n퇴근하고 공부하는 습관을 어느정도 들였던 달인 것 같아요. 퇴근하고는 늘 인과추론 공부를 하고, 면접 준비를 하고 그랬던 것 같습니다.\n책 읽기 습관도 들이기 시작했던 7월\n\n신수정님이 쓰신 일의 격을 읽기 시작했어요. 페이스북에서 정말 좋은 말들을 많이 전해주시는 어른이라, 한껏 기대에 부풀어 책을 구매했어요. 책은 역시나 저를 실망시키지 않았습니다. 그런데, 아직도 읽고있네요? 매일 꾸준히 책 읽기 습관 들이는게 뭐라고 이렇게 힘들까요? 읽을땐 재밌는데 말이죠.. 책을 집어들기 까지가 9만리입니다.."
  },
  {
    "objectID": "posts/2022-10-20-monthly-memory-202207/index.html#성장했던-점",
    "href": "posts/2022-10-20-monthly-memory-202207/index.html#성장했던-점",
    "title": "월간 회고록: 2022년 7월",
    "section": "🚀 성장했던 점",
    "text": "🚀 성장했던 점\n\n인과추론을 눈꼽만큼 알게됨\n\n앞서 말씀드렸듯이, 퇴근하고 인과추론 공부를 했었어요. <인과추론의 데이터 과학>에서 주관하여 진행했던 Korea Summer Workshop on Causal Inference 2022에서 제공하는 영상들로 좀 캐주얼하게 공부를 시작했습니다. 인과추론에 막 관심을 가지기 시작한 단계라, Bootcamp를 먼저 들었는데 정말 재밌었어요. 인과추론이라는 학문의 전체적인 틀에 대해 먼저 설명을 해주시고, 범위를 하나하나 좁혀나가며 설명을 해주십니다. 이런 강의를 무료로 수강할 수 있다는 것은 정말 큰 행운인 것 같습니다. 영상들은 여기서 확인하실 수 있습니다: 🔗 영상 보기\nBootcamp 외에도 Industry 영역에서의 인과추론에 대한 영상도 하나 봤었어요. 김설기님이 멀티암드밴딧(Multi-Armed Bandit), 톰슨샘플링(Thomson sampling)을 Industry에 적용하셨던 경험에 대해 이야기해주셨는데, 정말 유익했습니다. 따로 내용을 정리해두지 않아서 정확하게 기억은 안나는데, 기존의 A/B testing에 비해 멀티암드밴딧을 사용했을 때 가지는 이점을 이야기해주셨던 것 같아요. 깃허브에 알고리즘도 올려두신 것으로 알고 있는데, 제 업무에 적용해볼 수 있다면 꼭 한 번 시도해보고 싶네요. 어쨌든, 잘 기억이 안나니 다시 한 번 강의를 봐야겠어요.😅 그때그때 공부했던 내용을 기록해두는 것은 참 중요하다는 것을 다시 느낍니다.\n\n본격적인 논문화 작업 시작\n\n맡고있던 연구과제에 분석이 얼추 다 끝나서 본격적인 논문화 작업을 시작한 달입니다. 제 1년이 들어있는 과제인 만큼, 논문에서 하고자 하는 말을 잘 전달하고 강조하기 위해 어떤 그림을 그려야할 지 고민이 많았습니다.\n분석한 양이 정말 방대하고, 결과로 봐야할 것들도 참 많아서 그림 그리는데에 한 달 내내 시간을 가장 많이 쓴 것 같습니다. 이 과정에서 ggplot2를 다루는 기술이 또 한 걸음 더 성장한 것 같네요.\n분석 과정에서 작게 개발한 알고리즘에 관한 재현가능한 Tutorial을 만들었습니다. 논문에 꼭 넣고싶었거든요. Reproducible tutorial로. 무에서 유를 창조한 알고리즘은 당연히 아니고요. best subset selection과 grid search를 이용한 초모수(Hyperparameter) 튜닝 과정을 결합해서 분석 모형(distributed lag non-linear models)을 최적화 시키는 과정을 개발했다고 보시면 될 것 같습니다.\n\n면접이란 무엇인가?\n\nG마켓 1차 면접, 2차 면접을 준비하며 정말 많은 유튜브 영상을 보고, 얻은 내용들을 다 정리했어요. 1차 면접과 2차 면접 각각에서 본질적으로 보고자하는 바에 대해 어느정도 파악할 수 있었습니다. 나중에 한 번 따로 정리해보려고 합니다.\n그리고, 면접을 준비하며 인생에 순간순간의 길목에서 이제까지 해왔던 선택들을 돌아볼 수 있었어요. 내가 그 당시 왜 그런 결정을 했는지에 대해서 말이죠. 다행히 “그냥” 내린 선택이라든지, 친구따라 강남가는 선택이라든지와 같은 것들은 없었습니다."
  },
  {
    "objectID": "posts/2022-10-20-monthly-memory-202207/index.html#부족했던-점",
    "href": "posts/2022-10-20-monthly-memory-202207/index.html#부족했던-점",
    "title": "월간 회고록: 2022년 7월",
    "section": "👿 부족했던 점",
    "text": "👿 부족했던 점\n\n주말을 활용하지 않음\n\n월간 회고를 이렇게 블로그에 포스팅하고 있지만, 사실 주간 회고도 쓰고있는데요. 주간 회고는 노션에 개인적으로 쓰고 있으며, 이런 주간 회고들이 모여서 월간 회고가 된다고 보시면 될 것 같아요. 주간 회고에는 주말에 했던 작업들을 따로 기록하곤 하는데, 7월에는 주말에 작업한 것들이 없네요? 주말엔 놀고 쉬기 바빴나봐요. 그래서, 이렇게 블로그에 쓸 글들이 쌓여있는 거였군요.😀 면접 준비하랴, 업무 하랴 바쁘다면서.. 주말은 또 쉬고 싶었나 봅니다. 업무 외에 제가 하고싶은 것들을 공부하고 쌓아나가기 위해서는, 주말을 꼭 활용해야하는데 말이죠. 다시 한 번 반성합니다.\n\n책 읽기 습관\n\n주간 회고를 들여다보니, 역시 책 읽기 습관을 제대로 못들인 것에 대한 후회가 담겨있군요. 면접 준비와 업무를 병행한다는 핑계로.. 자기전이라도 최소 10-20분 정도는 꼭 책을 읽고 눈을 감으려고 여전히 노력하고있습니다."
  },
  {
    "objectID": "posts/2022-10-20-monthly-memory-202207/index.html#맺음말",
    "href": "posts/2022-10-20-monthly-memory-202207/index.html#맺음말",
    "title": "월간 회고록: 2022년 7월",
    "section": "맺음말",
    "text": "맺음말\n제가 보낸 2022년의 7월은 위와 같았습니다. 여러분들의 7월은 어떠셨나요? 회고라는게 쓰기 전에는 참 귀찮은데, 쓰면서는 참 재밌습니다. 그때 생각과 감정들을 어느정도 다시 떠올릴 수 있어서요. 7월에 가장 반성하는 점은 주말을 활용하지 않았던 점 같아요. 토, 일 1박 2일 여행을 다녀온다고 하더라도, 다녀와서 일요일 저녁 시간은 충분히 활용할 수 있었을 텐데 말이에요. 다시 한번 저는 참 나태한 사람임을 느낍니다. 회고를 통해 이 점을 주기적으로 깨달을 수 있어서 좋아요.😂 주말에 약속이 있더라도 이틀 내내 무너지지는 않기로 다시 한 번 깊게 다짐해봅니다."
  },
  {
    "objectID": "posts/2022-10-31-abtest-terms/index.html",
    "href": "posts/2022-10-31-abtest-terms/index.html",
    "title": "A/B 테스트 용어 사전",
    "section": "",
    "text": "Photo by Romain Vignes on Unsplash\nA/B test, 더 넓게는 온라인 종합 대조 실험(online controlled experiment)의 이해에 필요한 용어들을 간략하게 정리해보고자 합니다. 용어들은 앞으로 꾸준하게 추가될 예정입니다. 새로운 분야에 대한 공부를 시작할 때, 새롭게 알게 된 용어에 대한 확실한 정의는 매우 중요하니까요."
  },
  {
    "objectID": "posts/2022-10-31-abtest-terms/index.html#ab-테스트",
    "href": "posts/2022-10-31-abtest-terms/index.html#ab-테스트",
    "title": "A/B 테스트 용어 사전",
    "section": "A/B 테스트",
    "text": "A/B 테스트\n다 아시겠지만 한 번 정의하고 넘어가려고 합니다. A/B 테스트는 두 개의 변형(variant) A와 B를 사용하는 온라인 종합 대조 실험의 가장 간단한 형태라고 할 수 있습니다. 여기서 대조군에는 기존의 기능, 프로덕트 또는 알고리즘3을 부여받은 사용자들이 랜덤하게(randomized) 배치되고, 실험군은 핵심 지표(metrics) 개선을 기대하며 도입하는 새로운 어떤 것을 부여받은 사용자들이 랜덤하게 배치될 겁니다. 온라인 종합 실험에서 랜덤성(Randomization)은 매우 중요합니다. 어떠한 요인도 사용자들을 각 변형에 배정하는 데에 영향을 주도록 허용해서는 안됩니다. 랜덤성은 “확률에 기초한 의도적 선택”을 의미한다는 점을 기억하시기 바랍니다."
  },
  {
    "objectID": "posts/2022-10-31-abtest-terms/index.html#oec",
    "href": "posts/2022-10-31-abtest-terms/index.html#oec",
    "title": "A/B 테스트 용어 사전",
    "section": "OEC",
    "text": "OEC\nOEC는 전체평가기준으로 Overall Evaluation Criterion의 약자입니다. 실험 목적에 해당하는 계량적인 지표를 의미합니다. 통계학에서는 이를 반응변수(reponse variable), 종속변수(dependent variable)라 칭하기도 하죠. 그외 결과(outcome), 평가(evaluation), 적합도 함수(fitness function)을 동의어로 사용하기도 합니다(Quarto-vonTibadar 2006). OEC는 단기적으로는 실험 기간 동안 측정할 수 있어야하며, 동시에 장기적으로는 전사의 전략적 목표와 맞닿아 있어야 합니다. 예를 들어, 특정 온라인 서비스의 OEC는 사용자별 활동 일 수(active days per user)가 될 수 있습니다. 단순하게 매출과 같은 후행지표를 OEC로 선정해서는 안됩니다. 전사적으로 좋은 OEC가 설정되지 않았다는 것은, 리소스를 낭비하고 있다는 뜻일 수 있습니다."
  },
  {
    "objectID": "posts/2022-10-31-abtest-terms/index.html#파라미터",
    "href": "posts/2022-10-31-abtest-terms/index.html#파라미터",
    "title": "A/B 테스트 용어 사전",
    "section": "파라미터",
    "text": "파라미터\n파라미터(parameter)는 OEC 또는 기타 관심 지표에 영향을 미칠 것으로 간주되는 통제 가능한 실험변수를 뜻합니다. 요인(factors), 변수(variables)라 칭하기도 합니다. 파라미터에는 값이 할당되는데, 이를 수준(level)이라 말하더군요. 통계학에 익숙하신 분들은 범주형 변수를 떠올리시면 될 것 같습니다. 예를 들어, A/B 테스트에서 파라미터는 2개의 수준을 갖는 단일 파라미터에 해당할 것입니다. 온라인 환경에서는 이처럼 여러 개의 수준을 갖는 단일 파라미터 설계를 사용하는 것이 일반적입니다. 여러 파라미터를 사용하는 테스트는 다변수 테스트(MVTs, Mutivariate tests)4라 부릅니다. 온라인 환경에서는 글꼴 색상과 크기에 관한 실험을 하며 글꼴 색상과 크기에 관한 최적 조합을 찾고자할 때 사용되곤 합니다."
  },
  {
    "objectID": "posts/2022-10-31-abtest-terms/index.html#변형군",
    "href": "posts/2022-10-31-abtest-terms/index.html#변형군",
    "title": "A/B 테스트 용어 사전",
    "section": "변형군",
    "text": "변형군\n변형군(variants)를 파라미터라는 용어를 동원하여 정의하자면, 파라미터에 값을 할당하여 테스트 하는 사용자 그룹이라고 할 수 있습니다. 예를 들어, A/B 테스트에서 A와 B는 대조군(control group), 실험군(treatment group)이라 불리는 변형군이 있습니다. 그냥 편하게 실험을 구성하는 각 그룹을 변형군이라고 칭한다고 보시면 될 것 같습니다. 실험군에 대해서만 변형군이라고 칭하는 일부 문헌도 존재한다고 합니다."
  },
  {
    "objectID": "posts/2022-10-31-abtest-terms/index.html#가드레일-지표",
    "href": "posts/2022-10-31-abtest-terms/index.html#가드레일-지표",
    "title": "A/B 테스트 용어 사전",
    "section": "가드레일 지표",
    "text": "가드레일 지표\n가드레일 지표(guardrail metrics)란, 조직이 무엇을 변화시키지 않으려 하는지 식별하기 위해 꼭 필요한 지표라고 할 수 있겠습니다. 이름에 가드레일이 들어가는 이유 또한 이와 맞닿아 있습니다. 예를 들어, 바다 위를 떠다니는 유람선에서 승객들에게 제공하는 음식을 개선하기 위한 실험을 생각해봅시다. 이때, 바다 위를 항해하는 유람선에서 승객들의 안전 지표는 다른 어떤 요인들과 비교해도, 더 중요한 지표라고 할 수 있겠죠. 즉, 승객 안전은 이 실험에서 가드레일 지표라 할 수 있습니다. 승객들의 안전만큼은 결코 변화시키지 않고(낮추지 않고), 실험을 진행하겠다는 말입니다. 이를 위해서는 실험에 대한 OEC에 승객들의 안전에 관한 변수의 가중치를 매우 높여주면 됩니다. 안전에 따라 OEC가 민감하게 변화하도록 말이죠. 이 문제를 온라인 환경으로 가져와보면 소프트웨어 충돌을 생각해볼 수 있습니다. 도입한 기능이 제품의 충돌을 증가시키는 경우 유저들의 경험에는 심각한 영향을 끼칠 것입니다.\n\n\n\n\n\n\n시리즈 더 만나보기\n\n\n\n\n\n[1] A/B 테스트 용어사전"
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html",
    "href": "posts/2023-01-01-move-to-another-company/index.html",
    "title": "2022년 이직 로그",
    "section": "",
    "text": "The illustration by Mary Amato\n이직을 결심하고 생각보다 빠른 시기에 결실을 맺을 수 있었습니다. 작년 2월 즈음 이직을 결심하고, 8월 4일 G마켓 Data Scientist 포지션에 최종 합격 통보를 받을 수 있었는데요. 이직을 결심한 구체적인 이유, 나만의 지원 기준 등을 담은 이직 로그와 G마켓으로의 이직 여정에 대해 적어보려고 합니다."
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#이직을-결심한-이유",
    "href": "posts/2023-01-01-move-to-another-company/index.html#이직을-결심한-이유",
    "title": "2022년 이직 로그",
    "section": "이직을 결심한 이유",
    "text": "이직을 결심한 이유\n첫 번째로 이제 더이상 이 곳1에서는 성장할 수 없을 것 같다는 확신이 있었기 때문입니다. 연구원 각자 독립적으로 연구 과제를 맡아 업무를 하다보니, 제가 수행 중인 연구에서 데이터 분석 및 모델링을 하고 고민하는 지점이 있을 때 함께 이야기를 나눌 수 있는 사람이 없었습니다. 훌륭한 멘토의 존재까지 바란 것은 아닙니다. 그러나, 마주한 문제에 대해 함께 이야기하고 해결책을 나눠보는 상대 조차 없다는 것은 제게 깊은 갈증이었습니다.\n두 번째 이유는 센터 내부적으로 제가 완전히 무지한 연구 분야로의 연구 전문성을 키우고자 하는 계획있었기 때문입니다. 그래서, 2023년에도 이 곳에 머물러 있을 경우, 내가 기여할 수 있는 바는 더 적어질 것이고 이에 따라 개인의 성장 또한 정체될 것이라 생각했습니다. 과연 이 일이 내가 하고 싶은 일이 맞는지에 관한 고민도 생길 것은 분명했고요.\n제 애정이 깃들어있는 첫 직장에 대해 안좋은 뉘앙스의 이야기만 한 것 같아서 몇 가지 장점을 짤막하게 덧 붙여 써봅니다. 학계 내에서 제공받을 수 있는 임금 수준에서 최고 수준의 대우를 받을 수 있는 곳입니다. 분위기 또한 자유롭고 수평적이라, 자신이 맡고 있는 과제만 잘 처리하면 되는 근무 환경을 갖추고 있습니다. 첫 직장에 감사한 부분도 한 가지 말씀드리고 싶은게 있습니다. 석사를 갓 졸업하고 현업에 첫 발은 내딛은 제게 연구 과제를 단독으로 맡겨주셨는데요. 어떤 사람에게는 막중한 업무 부담으로 다가왔을 수도 있겠지만, 제가 당시에 가장 잘 할 자신이 있는 연구 분야2였기 때문에 그 부담감을 조금이나마 즐길 수 있었던 것 같습니다. 그 덕택에 1년반이라는 시간동안 많은 성장을 이룰 수 있었습니다."
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#지원-기준",
    "href": "posts/2023-01-01-move-to-another-company/index.html#지원-기준",
    "title": "2022년 이직 로그",
    "section": "지원 기준",
    "text": "지원 기준\n학계보다 비교적 빠르게 변화하는 환경인 산업계에서 일을 해보고 싶었습니다. 조금 더 구체적으로 말해보자면, 산업계에서 유의미한 지표 설계, A/B 테스트 결과를 바탕으로 한 통계적 가설검정과 인과추론 등을 수행해보고 싶었습니다. 그래서, 제조업 기반의 회사는 최대한 피하고 온라인 서비스를 바탕으로 비즈니스를 하는 기업으로 가고 싶다는 막연한 생각을 가지고 있었어요. 온라인 서비스가 비즈니스 모델인 기업이 전사 차원에서 데이터 기반 의사 결정 문화가 조금 더 잘 확립되어 있지 않을까 하는 생각도 가지고 있었고요. 저는 당시 산업계에 몸을 담은 적도, 여러 분야의 기업들에서 일을 해본 적도 없으므로 온전히 제 개인적인 추정이었음을 말씀드립니다.\n마지막으로 가지고 있었던 또 하나의 지원 기준은 채용 공고의 디테일이었습니다. 채용 공고에 쓰여진 지원 자격, 세부 직무 내용에 관한 디테일이 곧 그 조직에서 해당 포지션을 얼마나 필요로 하고 있는 지를 나타내 준다고 생각합니다. 다시 말하면, 그 포지션의 필요성에 대해 얼마나 고민했는 지를 나타내준다고 봐요. 그래서, 늘 채용 공고는 꼼꼼하게 읽어봤습니다. 잘 쓰여진 채용 공고는 더 많은 지원자들, 더 수준 높은 지원자들의 어플라이를 이끌어 낼 수 있다고 생각합니다.😀\n앞서 말씀 드린 정도의 지원 기준은 꽤 불확실하다고 생각하시는 분들도 있을 것 같습니다. 이미 눈치 채신 분들도 있으시겠지만, 저는 가고 싶은 기업이나 세부 분야를 특정해 본 적이 없었던 사람입니다. 이를 특정하지 못했던 이유를 요즘 들어 생각해봤어요. 곰곰이 생각해보니 학부생때 부터 Data Science/Analysis를 일로 삼고싶다는 생각만 했지, Data Science/Analysis라는 도구를 활용해서 어떤 문제를 해결하고 싶은 건지에 관한 고민은 결코 깊게 해본 적이 없더군요. 분야를 막론한 수많은 기업들이 왜 Data Science/Analysis에 관심을 두고 있고 이를 통해 구체적으로 어떤 Business problem을 풀고자 하는 지, 구체적인 성공 사례는 어떤 것들이 있는 지에 관한 관심 또한 없었죠. 그래서, 자연스레 가고 싶은 기업, 특정 산업 분야를 정하지 못하지 않았나 생각합니다.\n이 부분을 정하고 차근차근 준비를 해나가야 채용 시장에서 유리한 포지션을 가져갈 수 있다고 생각을 하고 있거든요. 저는 그러한 포지션을 갖춰 나가지 못했던 사람이고요. 그래서, 이직 준비를 하면서도 꽤나 막막했습니다. 제가 가지고 있던 주 무기(e.g. R, 시계열 자료분석)들은 온라인 서비스를 통해 비즈니스를 하고 있는 기업의 Data Scientist/Analyst 포지션에서 요구하는 핵심 역량은 아니었거든요. 데이터 사이언티스트, 데이터 분석가가 되고 싶다는 꿈을 품었던 학부생 때부터, 어떤 문제를 해결하고 싶은 지에 대해 고민하는데에 충분한 시간을 투자했다면 어땠을까 하는 조금의 후회가 남네요."
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#이직-참고-자료",
    "href": "posts/2023-01-01-move-to-another-company/index.html#이직-참고-자료",
    "title": "2022년 이직 로그",
    "section": "이직 참고 자료",
    "text": "이직 참고 자료\n\n이력서\n저는 이력서를 노션으로 정리하고 있습니다. 노션으로 to do list, 주간 회고 등을 관리하고 있었기 때문에 익숙한 도구이기도 했고, 마침 이력서를 노션으로 관리하고 있는 개발자 분들도 꽤나 있으시더군요. 이력서와 포트폴리오 제작기는 지난 4월 회고록에 상세하게 적어놨어요. 제 최신 이력서는 여기서 만나보실 수 있고요. 꼭 구직 활동을 시작할 때에만 이력서를 정리한다기 보다는 주기적으로 업데이트 해나가시는 것을 권장드립니다.\n\n\n자기소개서\n요즘 자유 양식의 이력서, 자기소개서를 요구하는 곳도 점점 늘어나고 있어 크게 도움이 되실 지는 모르겠지만, 참고했던 자료들을 나열해두겠습니다. 저는 자유 양식의 자기소개서를 작성할 때에도 꽤나 도움을 받았습니다.\n\nAND (ft.인싸담당자) <마스터 자소서 | EP 03. 입사후포부>\n\n입사 후 포부 항목가 막막하신 분들께 추천합니다.\n\n면접왕 이형 <면접관이 싹 정리해주는 뽑아쓰면되는 지원동기 5가지>\n\n지원 동기에 관한 5가지 고리를 알려줍니다. 자신에게 맞는 고리를 가져다 쓰면 됩니다. 지원 동기는 자유 양식의 자기소개서에서도 매우 중요하다고 생각합니다. 꼭 보셨으면 하는 영상입니다.\n그 외 면접왕 이형 채널에는 구직자에게 정말 도움이 될만한 영상들이 많습니다. 자기소개서나 면접 준비할 때 가장 추천하고 싶은 채널입니다.\n\n\n\n\n면접\n\n제출한 이력서, 자기소개서, 포트폴리오로부터 예상 질문 정리하고 답변 생각해보기\n다음의 질문 리스트에 관한 답 생각해보기\n\n\n\n3가지 키워드로 정리한 경력직 면접 예상 질문 (서현직)\n\n\nAND (ft.인싸담당자) <면접에서 합격하는 1분자기소개, 이 영상 하나로 종결 (5가지 방법 + 예시)>\n\n모든 면접의 시작은 자기소개입니다. 면접 질문을 미리 예상하고, 답변을 외우는 형태로 면접을 준비하는 것은 개인적으로 권하지 않지만 1분 자기소개 정도는 외울 가치가 있다고 생각합니다. 1분 자기소개로부터 나오는 꼬리 질문들은 자기 페이스대로 면접을 끌고갈 수 있게해주기 때문입니다.\n\n임원 면접(2차면접) 팁\n\n아래 영상들을 보시면 임원 면접에서 보고자하는 결이 무엇인지 파악하실 수 있을 겁니다. 실무진 면접(1차 면접)에서 보고자 하는 바도 자연스레 아실 수 있을 겁니다.\nAND (ft.인싸담당자) <임원면접에서 합격하는 사람들은 무엇이 다를까? 최종 관문만 넘고 취뽀하자!>\nAND (ft.인싸담당자) <임원면접 가기 전, 시간 없다면 이 영상만이라도! 임원면접 금기사항 3가지!>\nAND (ft.인싸담당자) <Jacob도 감탄한 면접만 20번 떨어진 대기업 합격자의 임원면접 꿀팁!! [면접 꿀 TIP]>\n드림즈크루 <실무진 면접과 임원 면접은 어떻게 다를까? 차이를 알면 합격이 보여요.>\n면접왕 이형 <임원면접관이 뽑을 수 밖에 없는 면접준비방법>\n취업사이다 <전 삼성 인성 면접관 나상무 선생님이 말해주는 면접관의 합격 결정 그래프 [렛유인 실시간 취업고민상담소 EP.99]>: 강력 추천드립니다.😀"
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#서류-전형",
    "href": "posts/2023-01-01-move-to-another-company/index.html#서류-전형",
    "title": "2022년 이직 로그",
    "section": "서류 전형",
    "text": "서류 전형\n6월 즈음 인과추론에 큰 관심을 가지게 되었는데3, 인과추론이라는 키워드를 채용 공고에서 접하고 설레는 마음으로 서류 준비를 했던 기억이 새록새록 납니다. 제가 지원을 했던 당시에는 채용 사이트 리뉴얼 전으로 자사 이력서 양식으로 지원을 했습니다.4 지금은 자유 양식으로도 이력서를 받아 주는 것으로 알고 있습니다. 당시 이력서에 포트폴리오 링크를 첨부할 수 있는 란이 있어, 노션 이력서와 포트폴리오 링크, 개인 블로그 링크를 적어냈던 것으로 기억합니다. 그리고, 자기소개서는 최대 1000자의 자유형식이어서 지원 동기와 직무 경험을 정리해서 제출했습니다."
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#차-면접",
    "href": "posts/2023-01-01-move-to-another-company/index.html#차-면접",
    "title": "2022년 이직 로그",
    "section": "1차 면접",
    "text": "1차 면접\n다대일 면접으로 1시간 동안 진행됐습니다. 본격적인 면접 시작 전에 충분히 편안한 마음을 가질 수 있게끔 아이스브레이킹을 해주셨던 기억이 납니다. 그리고, 가장 기억에 남는 말은 면접관과 구직자 입장이 갑과 을의 관계로 생각하는 경우가 있는데 그렇게 생각하지 않았으면, 그리고 우리와 fit이 잘 맞는지 대화한다고 생각해줬으면 한다고 먼저 말씀을 먼저 해주셨던 것입니다. 그 분이 지금은 제 팀장님이 되셨네요.😀 덕분에 정말 편하게 면접을 진행할 수 있었던 것 같습니다. 그리고, 자기소개와 제 대표 프로젝트를 설명하는 것으로 면접을 시작했습니다. 자세한 질문들은 공개적인 자리라 쓰지 못하는 부분을 양해해주셨으면 합니다.\n개인적으로 기술적으로 깊은 디테일5, 이런저런 일들을 했다고 나열하는 식의 What에 집중하는 면접 스타일 보다는, 왜 문제 정의를 이렇게 했고, 왜 그 방법론을 사용했으며 그 과정 속에서 어떤 고민을 해서 어떤 결과물을 산출해낼 수 있었는지와 같이 Why와 How에 집중하는 면접을 선호합니다. 1차 면접에서 이와 같이 제가 선호하는 형태의 질문들을 많이 받을 수 있었어요. 그래서, 1시간 동안의 면접임에도 시간 가는 줄 모르고 이야기를 나누었던 기억이 납니다. 면접 준비를 하실 때에는 What을 나열하는 것에 끝나는 것이 아닌, Why와 How를 통해서 내가 어떤 난관에 당면했었는지, 그리고 그 순간에 왜 그런 선택을 했는지에 대해 꼭 돌이켜 보시기 바랍니다."
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#차-면접-1",
    "href": "posts/2023-01-01-move-to-another-company/index.html#차-면접-1",
    "title": "2022년 이직 로그",
    "section": "2차 면접",
    "text": "2차 면접\n2차 면접은 2대1 면접으로 진행될 예정이었으나, 한 분의 사정으로 인해 1대1로 진행되었습니다. 마찬가지로 1시간 동안 진행을 했고요. 상세하게 본인 소개를 해주셨고, 내가 속하게 될 팀과 어떤 관계에 있고 함께 어떤 일을 하는지 까지 먼저 설명을 해주셨습니다. 이 부분에서 어느정도 긴장을 풀 수 있었던거 같아요. 1차 면접과 마찬가지로 면접자를 본인과 동등한 입장에서 존중해주시는 느낌을 받았습니다. 마찬가지로 자기소개로 시작을 했습니다.\n아, 그리고 면접과 관련해서 한 가지 말씀드리고 싶은 바가 있어요. 내가 잘 아는 질문이더라도, ’의도적으로 천천히 대답하기’입니다. 저는 긴장하면 말이 빨라지는 습관이 있습니다. 그래서, 과거 면접에서 경험했던 것들을 보면 이 습관으로 인해 내가 말하고자 하는 바와는 저 멀리.. 머리는 아니라고 말하는데 입은 이미 움직이고 있었던 그런 경험이 있습니다. 그래서, 올해 이직 과정에서 면접을 준비하며 가장 먼저 머릿 속에 넣어놨던 바는 면접관의 질문의 의도와 핵심이 무엇인지 충분히 생각하고 천천히 침착하게 답변하자는 것이었습니다. 전혀 예상하지 못한 질문이 나왔을 때는 당황하지 않고, 조금만 생각할 시간을 달라고 요청하기도 했습니다. 성급한 답변은 면접에서의 본인의 일관성을 깨뜨리는 답변을 야기할 수 있으므로 조심하는게 좋다고 생각합니다.😀 머리는 아니라고하는데 입이 움직이고 있는 상황은 다시 생각해도 끔찍하네요.\n2차 면접은 일반적인 임원 면접 스타일과 같았습니다. 실무와 관련한 기술적 질문이 아닌 좀 더 일반적인 형태의 질문을 많이 받았어요. 기본적인 지원동기, 내 가치관과 지금까지 삶을 살아옴에 있어서 했던 선택들(e.g. 통계학 전공을 택한 이유, 대학원 진학 이유)에서 왜 그러한 선택을 했는지에 대해 주로 질문을 받았습니다. 재밌었습니다. 과거를 돌이켜보면 제 인생에 있어서 중요한 결정들은 주변의 의견은 딱히 신경을 쓰지 않고, 모두 제가 맞다고 판단되는 방향으로 선택을 해왔습니다. 선택하기 전에는 물론 충분한 시간의 심사숙고의 과정이 있었고요. 그래서, 면접에서 이러한 질문을 받는 것이 두렵지 않고 오히려 즐거웠던 것 같습니다. 제 과거에 했던 생각들, 가치관의 형성 과정들을 면접관님이 집중해서 들어주시고 꼬리 질문을 해주시는게 오히려 감사하기도 했습니다. 자신의 커리어와 관련해서 해왔던 선택들을 돌아보고, 어떤 이유에서 그런 선택들을 해왔는지 되새겨보는 과정을 가지시면 2차 면접(임원 면접)에 많은 도움이 되실겁니다.\n여기에 더해서 다음의 2가지 질문에도 답해보시기 바랍니다.\n\n“미래에 나는 어떤 사람이 되고 싶은가?”\n“커리어에서 궁극적으로 이루고 싶은 바는 무엇인가?”\n\n저도 비슷한 유형의 질문을 받았기도 했고, 충분히 2차 면접에서 받을만한 질문이라 생각합니다. 1차 면접이 실무진들이 “당장 이 사람과 일을 함께 할 수 있을까?”를 주로 판단하는 자리라면, 2차 면접은 실무진보다 조금 더 고위급의 인사들이 “이 사람이 가진 포텐셜은 얼마나 될까?”를 중점적으로 판단하는 자리니까요."
  },
  {
    "objectID": "posts/2023-01-01-move-to-another-company/index.html#최종-합격-및-처우-협의",
    "href": "posts/2023-01-01-move-to-another-company/index.html#최종-합격-및-처우-협의",
    "title": "2022년 이직 로그",
    "section": "최종 합격 및 처우 협의",
    "text": "최종 합격 및 처우 협의\n2차 면접을 보고 이틀 뒤에 채용 담당자 분의 전화로 최종 합격 통보를 받았습니다. 제가 하고싶은 일을 할 수 있는 포지션으로 가게 됐다는 생각에 너무나도 기뻤습니다. 당시 제 근로 소득을 증빙하는 과정에서 조금 복잡한 부분들이 많이 있어서 증빙할 자료도 많이 필요했었는데, 담당자분께서 이 상황을 이해해주시고 많은 얘기들을 천천히 들어주셨습니다. 그래서, 처음이라 더욱 걱정스러웠던 처우 협의도 원활하게 마무리할 수 있었습니다. 1차 면접부터 최종합격, 처우협의까지의 모든 과정에서 지원자를 진심으로 존중해준다는 느낌을 받을 수 있었습니다."
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html",
    "title": "연간 회고록: 2022년",
    "section": "",
    "text": "The illustration by Mary Amato\n지난 한 해가 지나간지 얼마 채 되지도 않은 것 같은데, 어느덧 2월에 들어섰네요. 역시 시간은 우리를 기다려주지 않습니다. 이제서야 연간 회고록을 쓰는 저도 참..😂 연간 회고록은 커리어와 개인적인 부분을 나눠서 진행해보려고 해요. 이번 한 해는 제게 참 많은 변화가 있었어요. 기존에 가지고 있던 생각에도 변화가 참 많았습니다. 그 중심에는 학계에서 산업계로의 이직이 큰 몫을 한 것 같아요. 한 해 동안 느꼈던 바들을 한 번 잘 풀어서 써보겠습니다."
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#산업계로의-이직",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#산업계로의-이직",
    "title": "연간 회고록: 2022년",
    "section": "산업계로의 이직",
    "text": "산업계로의 이직\n지난 1월에 포스팅했던 이직로그에서 말씀드렸듯이, 저는 작년 2월에 이직을 결심했고 9월부터는 G마켓의 Data Scientist 포지션에서 업무를 하게되었습니다. 눈 깜짝할 사이에 5달이라는 시간이 흘렀군요. 업무 적응은 아직 현재 진행형인 듯 합니다. 여전히 눈을 감고.. 코끼리 다리는 만지는 듯한.. 이른바 군맹무상의 상태인 듯 해요. 언제쯤 이 코끼리를 멀리서 크게 바라볼 수 있을까요?😭\n\n\n\n현재 나의 상태..\n\n\n5달 동안 많이 배웠습니다. 기존에 가지고 있었던 생각의 변화도 많았고, 새로운 도구를 배우기도 했고요. 쭉 한 번 적어보겠습니다.\n\nGeneralist? Specialist?\n지금은 이 둘을 굳이 분간하고 싶지 않습니다만, 저는 이 글에서도 넌지시 밝혔듯이, Data Science 업계에서 살아남기 위해서는 Specialist가 되어야하지 않나 하는 생각을 가지고 있었어요. 그래서, 대학원 때도 100점짜리 하나를 가지고 나가기 위해 노력을 했고, 이론적 측면에서는 시계열 자료분석 소프트웨어 측면에서는 R이 그것에 해당했습니다. 물론, 두 측면을 마스터했다는 의미는 절대 아닙니다.😀 가장 자신있다고 말할 수 있는 ’하나의 영역’을 만들고 싶었던 거죠. 대학원을 졸업하고 병원 연구센터에서 시계열 자료를 모델링하며 연구에 몰두할때만해도 이 생각에는 딱히 변화가 없었죠. 코끼리 다리를 만지는 듯한.. 느낌도 없었고, 업무 적응에 딱히 긴 시간이 걸리지도 않았으니까요.\n그러나, G마켓으로 이직하면서 이 생각에는 많은 변화가 찾아왔습니다. 일단, 제가 모르는게 너무 많다는 생각을 했어요. 1년반정도 재직을 했던 첫 직장에서는 느껴보지 못한 어려움과 감정을 느꼈습니다. 100점짜리 하나를 가지고 나가기 위해 노력했기에, 그 외에 영역에는 모르는 부분이 많았죠. 이때부터 조금씩 생각을 했습니다. 아.. 100점짜리 하나를 만들어 나가는 것 보다는 80점짜리를 3개 이상 만드는게 더 좋은 결정이었을 수도 있겠구나.\n\n\n\nT자형 인재 (출처:EliceAcademy)\n\n\n무의식적으로 T자형 인재를 지향해야 한다고 생각했던 저는, 100점짜리 하나를 만들어서 나가야 한다는 생각에 아래로 뾰족한 ’전문 분야에 대한 깊은지식’만을 갖추는 데에 매몰되어있었습니다. 그리고, 이로 인한 어려움은 산업계로 이직을 하면서 피부로 느낄 수 있었죠. 이러한 생각을 할때 즈음 신수정 님이 쓰신 <일의 격>에 이런 구절이 쓰여져 있더군요.\n\n100점짜리 하나 보다는 80점 짜리를 3개 이상 만들어봐라.\n\n\n\n\n신수정 <일의 격> (출처: yes24)\n\n\n제 무릎을 탁 치게되는 구절이었습니다. 여기서 2가지 일화를 더 겪으면서 제가 학부생, 대학원생 때 지향하던 바가 조금 잘못되었구나를 확신했어요. 첫 번째는 이직한지 한 달이 되던 무렵 선호님1과 저녁을 먹으면서 우연히 Generalist, Specialist에 대해 이야기를 나누면서 였고, 두 번째는 지난 12월에 팀 워크샵에서 실장님이 T자형 인재를 가지고 한 시간 정도 본인의 이런저런 의견을 이야기 해주시는 것을 들으면서 였어요. 일의 격의 구절, 선호님의 논지, 실장님의 논지는 크게 봤을 때 같은 이야기를 하고 있는 것 처럼 들렸어요. 이쪽 업계에서는 100점짜리 하나가 있는게 유리하다고 생각을 해왔던 제 생각과는 정반대의 결을 가진 생각으로 말이죠. 100점짜리 하나를 만들어야 한다는 생각이 어쩌면 저한테는 꽤 편했던 핑계였던 것 같아요. 이제는 이런 생각을 버리고, 80점 짜리를 3개 이상 만들기 위해 이것 저것 재지않고 새로운 것을 배우려고 노력하고 있어요.\n\n\nR? Python?\n제가 가장 사랑하는 언어는 여전히 R이지만, 산업계에서 R이 차지하고 있는 파이를 직접 마주하고 나니 많은 생각을 하게 됐어요. 저는 수년간 이어져오고 있는 R vs Python, 최근에는 R vs Python vs Julia 까지😂..\n\n위 영상도 어느덧 1년이나 지났군요. R과 Python을 비교하는 글은 언제나 꾸준히 올라왔습니다. 이 떡밥은 앞으로도 결코 사라지지 않을 것이라고 생각합니다. 이 떡밥은 SNS에서 관심을 끌기에는 워낙 좋은 떡밥이라, 둘 중 한 언어에 편향되게 글을 쓰는 사람들 또한 참 많았습니다. 아래 공유드리는 두 글 또한 이 떡밥과 관련한 글인데, 꽤 중립적인 입장에서 쓴 글이라고 생각되서 공유해봅니다.\n\n\n현재 제 생각은 페이스북에 적었던 아래 두 포스팅으로 갈음합니다. 첫 번째 포스팅을 요약하자면, 우리나라 업계의 Data Scientist로 활약하기를 꿈꾼다면 그냥 마음 편하게 Python을 하라는 얘기에요. 협업 중에 Python 유저로서는 겪지 않을 불편함을, R 유저들은 필연적으로 겪어야만 하거든요. 그만큼 산업계에서 R 유저들이 차지하는 파이가 작으니까. R이 Python 보다 구리다는 얘기는 아니고요.😀\n\n두 번째 포스팅은 R이 Native인 사람이 Python을 다루며 했던 생각입니다. 가볍게 읽어주시면 될 듯 합니다.\n\n\n\n세상에 불필요한 배움은 없다"
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#generalist-specialist",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#generalist-specialist",
    "title": "연간 회고록: 2022년",
    "section": "Generalist? Specialist?",
    "text": "Generalist? Specialist?\n지금은 이 둘을 굳이 분간하고 싶지 않습니다. 그러나, 과거에 저는 이 글에서도 넌지시 밝혔듯이 Data Science 업계에서 살아남기 위해서는 Specialist가 되어야하지 않나 하는 생각을 가지고 있었어요. 그래서, 대학원 때도 100점짜리 하나를 가지고 나가기 위해 노력을 했고 이론적 측면에서는 시계열 자료분석, 소프트웨어 측면에서는 R이 그것에 해당했습니다. 이 둘에 대한 수준을 절대적인 기준의 높은 경지까지 끌어올렸다는 의미라기 보다는, 자신있다고 말할 수 있는 영역들을 만들어냈다고 봐주시면 감사할 것 같습니다. 대학원을 졸업하고 병원 연구센터에서 시계열 자료를 모델링하며 연구에 몰두할때만해도 이 생각에는 딱히 변화가 없었죠. 코끼리 다리를 만지는 듯한.. 느낌도 없었고, 업무 적응에 딱히 긴 시간이 걸리지도 않았으니까요.\n그러나, G마켓으로 이직하면서 이 생각에는 많은 변화가 찾아왔습니다. 일단, 제가 모르는게 너무 많다는 생각을 했어요. 1년반정도 재직을 했던 첫 직장에서는 느껴보지 못한 어려움과 감정을 느꼈습니다. 당시 느꼈던 감정이 잘 녹아있는 페이스북 포스팅입니다.\n\n100점짜리 하나를 가지고 나가기 위해 노력했기에, 그 외에 영역에는 모르는 부분이 참 많았어요. 이때부터 조금씩 생각을 했습니다. 아.. 100점짜리 하나를 만들어 나가는 것 보다는 80점짜리를 3개 이상 만드는게 더 좋은 결정이었을 수도 있겠구나.\n\n\n\nT자형 인재 (출처: EliceAcademy)\n\n\nSpecialist를 지향해왔던 저는 100점짜리 하나를 만들어서 나가야 한다는 생각에 아래로 뾰족한 ’전문 분야에 대한 깊은지식’만을 갖추는 데에 매몰되어있었습니다. 현 사회에서 요구하는 인재의 유형이라 할 수 있는 T자형 인재와는 거리가 꽤 멀었죠. 그리고, 이로 인한 어려움은 산업계로 이직을 하면서 피부로 느낄 수 있었습니다. 이러한 생각을 할때 즈음 신수정 님이 쓰신 <일의 격>에 이런 구절이 쓰여져 있더군요.\n\n100점짜리 하나 보다는 80점 짜리를 3개 이상 만들어봐라.\n\n\n\n\n신수정 <일의 격> (출처: yes24)\n\n\n제 무릎을 탁 치게되는 구절이었습니다. 여기서 2가지 일화를 더 겪으면서 제가 학부생, 대학원생 때 지향했던 바가 조금 잘못되었구나를 확신했어요. 첫 번째는 이직한지 한 달이 되던 무렵 선호님1과 저녁을 먹으면서 우연히 Generalist, Specialist에 대해 이야기를 나누면서 였고, 두 번째는 지난 12월에 팀 워크샵에서 실장님이 T자형 인재를 가지고 한 시간 정도 본인의 이런저런 의견을 이야기 해주시는 것을 들으면서 였어요. 일의 격의 구절, 선호님의 논지, 실장님의 논지는 크게 봤을 때 같은 이야기를 하고 있는 것 처럼 들렸어요. 이쪽 업계에서는 100점짜리 하나가 있는게 유리하다고 생각을 해왔던 제 생각과는 정반대의 결을 가진 생각으로 말이죠. 100점짜리 하나를 만들어야 한다는 생각이 어쩌면 저한테는 꽤 편했던 핑계2였던 것 같아요. 이제는 이런 생각을 버리고, 80점 짜리를 3개 이상 만들기 위해 이것 저것 재지않고 새로운 것을 배우려고 노력하고 있어요. 사회에 나온지 얼마 안된 시기에 이러한 생각의 전환을 이루어 낸 게, 참 행운이라고 생각하고 있어요."
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#r-python",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#r-python",
    "title": "연간 회고록: 2022년",
    "section": "R? Python?",
    "text": "R? Python?\n자, 지겹고 무의미한 논쟁을 한번 꺼내 볼까요? 제가 가장 사랑하는 언어는 여전히 R이지만, 산업계에서 R이 차지하고 있는 파이를 직접 마주하고 나니 많은 생각을 하게 됐어요. R vs Python, 최근에는 Julia 까지 데싸 업계에서는 여전히 가장 감질맛 나는 떡밥이 아닌가 생각합니다\n\nR과 Python을 비교하는 글은 언제나 꾸준히 올라왔습니다. 링크드인에도 이 떡밥은 종종 풀렸습니다. 해외에서도 뜨거운 주제였죠. 논쟁의 마무리에서 정답과 같은 중론은 “What’s your job?” 이었습니다. 당신이 풀고자 하는 문제가 무엇인지에 따라 두 언어를 선택하면 된다는 이야기죠. 이러한 중론에 저는 100% 동의했습니다. 예컨데, 딥러닝을 포함한 머신러닝을 위시한 예측 모델링에 꿈이 있는 사람이라면, Python을 택하여 공부하는게 아무래도 취업 시장에서 유리한 포지션을 가져갈 수 있겠죠? 저도 학부생, 대학원생 때 이 떡밥에 관심이 많았던 사람이라, 과거에 슬기로운 통계생활 블로그에 이와 관련한 주제로 글을 쓰기도 했어요. 아래 두 글은 꽤 중립적인 입장에서 두 언어를 비교하고 있다고 생각했던 글들이여서 가져와 봤어요.\n\n\n이러한 중립적 관점에서 기술된 글과는 반대로, R과 Python을 막론하고 특정 언어를 찬양하며 다른 언어를 깎아내리는 글은 읽을 가치도 없고, 그 자체로 저자의 무지함을 드러내는 글이라 생각합니다. 장단점을 논할 순 있겠지만요.\n산업계로 넘어와 조금 바뀐 제 생각은 페이스북에 적었던 아래 두 포스팅으로 갈음합니다. 첫 번째 포스팅을 요약하자면, 우리나라 업계의 Data Scientist로 활약하기를 꿈꾼다면 그냥 마음 편하게 Python을 하는게 낫지않나 하는 얘기에요. 이쪽 업계에 R 사용자가 차지하는 파이는 꽤 작아서 협업 중에 Python 유저로서는 겪지 않을 불편함을 R 유저들은 필연적으로 겪어야만 하거든요.\n\n이제 지루한 논쟁에 정답을 내려드리겠습니다.\n\n둘 다 하세요.\n\n두 번째 포스팅은 R이 Native인 제가 Python을 다루며 했던 생각입니다. 요약하자면, Pandas는 Pandas 감성으로, tidyverse는 tidyverse 감성으로 다루자는 얘깁니다. 정답이 없는 부분이니, 가볍게 읽어주시면 좋을 듯 합니다.😀"
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#shiny-streamlit",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#shiny-streamlit",
    "title": "연간 회고록: 2022년",
    "section": "Shiny? Streamlit?",
    "text": "Shiny? Streamlit?\n예전부터 대시보드를 한 번 개발해보고 싶다는 생각을 쭉 해왔었습니다. G마켓에서 업무를 시작한지 두달이 조금 넘었을 무렵, 팀 내에서 개발하여 서비스하고 있는 추천 기술들의 몇몇 대표적인 성과 지표들을 모니터링할 목적으로 대시보드 개발이 필요로 된다는 이야기를 들었어요.\n\n네, 제가 덮썩 물었습니다. 처음에는 개인적인 욕심으로 Shiny를 고려했었어요. 이정도면 R 짝사랑 그만해도 되겠죠? 그런데, 이 대시보드를 제가 평-생 유지보수 할 수 있다는 보장이 없으니, 적절하지 않은 도구였어요. 팀 내에 R 사용자가 없거든요. 사내에도 드물고.😭 여기서 욕심을 좀 더 부려서, 당시 나온지 세달? 네달?이 채 안된 Shiny for Python을 팀 내에 분석가 분들께만 먼저 제안해봤어요. 동의해주셨지만, 곰곰이 생각해보니 Python 기반으로 대시보드 개발할건데 굳이 왜 알파 버전에 불과하고 레퍼런스도 별로 없는 Shiny for Python을 쓰려고 하는지 문득 의문이 들었죠. 여기에 또 굳이 Quarto를 엮겠다고.. Python에는 더 좋은 선택지의 대시보드들이 많잖아요.\n그래서, Streamlit과 Plotly의 Dash 둘 중에 깊은 고민을 하기 시작했어요. 우리팀에서 개발할 대시보드의 목적 상 많은 자유도가 필요로 되진 않았어요. 이 측면에서 Streamlit 보다 훨씬 다양한 기능들이 오픈되어 있는 Dash까지 고려할 필요가 있나 생각을 했죠. 그리고, Streamlit이 가볍고 빠르게 빌드할 수 있고, 기본 UI로도 충분히 요즘 감성에 맞게 이뻤기 때문에 더 끌렸어요. 그래서, 팀 내 대시보드 개발은 Streamlit으로 진행하기로 합니다.\nPython은 R의 성숙도에 비해 훨씬 딸려서 조금 걱정하긴 했는데 개발하는 내내 재밌었어요. Pandas로 전처리하는 재미도 있었고요. siuba라는 패키지를 이용해서 tidyverse syntax랑 엮어서 pandas dataframe을 핸들링하는 재미도 있었고. 사실, 다 뻥이고요. R로 하면 그~음방 하는데 하는 생각이 머리 속을 떠나지 않았습니다.😂 Python을 조금 다룰 줄 아시고, 대시보드를 개발해보고 싶으시다고요? Streamlit으로 가볍게 시작해보세요. Streamlit에서 공식적으로 제공하는 30 Days of Steamlit으로 기본적인 기능들을 익히고, 아무 데이터나 가지고 무작정 시작해보세요. 참 쉽고 재밌는 툴입니다. 금방 실력이 느실거에요. 30 Days라 하지만 바짝하면 하루이틀이면 끝냅니다.😀 이것저것 재면 시작을 안하게 되요. 가볍게 시작해서 무식하게 끝내는 사람이 됩시다!"
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#우리팀-분위기",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#우리팀-분위기",
    "title": "연간 회고록: 2022년",
    "section": "우리팀 분위기",
    "text": "우리팀 분위기\n5개월간 겪은 우리팀 분위기는 어땠냐고요?\n\n\n\n\n\n대답은 유느님의 리액션으로 대신해봅니다. 제가 꿈꾸던 조직에 온 것 같아요. 우리팀에서 5개월 간 받은 느낌들을 짤막하게 적어볼게요.\n\n팀원이 풀지 못하는 일은 곧 나의 일\n\n각자 업무로 정말 바쁘시지만, 슬랙에 도움이나 정보를 요청하면 정말 다들 내 일처럼 붙어서 도와주십니다.\n\n새로운 시도가 환영 받는 곳\n\n이베이코리아로 시작해 현재 신세계 그룹에 편입된 지마켓이 있기까지 꽤나 이커머스에 역사가 있는 오래된 기업이죠. 그럼에도 불구하고, 우리팀에서 어떤 새로운 시도는 환영받는 분위기 입니다. 현재 전사적인 분위기도 동일하게 형성되어 있다고 생각합니다. 본 포지션에 지원할때 가장 큰 매력을 느꼈던 “인과추론에 대한 관심” 또한 이커머스 업계의 Data Science에서 아직까진 새로운 시도라고 보고 있습니다.\n\n자율성, But 열정! 열정!\n\n팀 분위기는 매우 자율적이에요. 다른 말로 자유롭다고 표현할 수도 있겠네요. 이러한 분위기와는 어울리지 않게 정말 다들 각자 맡은 일을 열정적으로 처리하십니다. 월급 받는만큼만 딱 일하는 분들이 아니라, 자신의 만족을 위해 일하는 분들! 항상 팀원 분들께 자극받고 있어요.\n\n수평적인 분위기 그 자체\n\n저만 이렇게 생각하는 건 아니겠죠? 제가 이 팀에 빌런은 아니겠죠?.. 써놓고 보니 갑자기 살짝 두렵네요.😂 이 글을 읽으실지는 모르시겠지만, 팀에 이런 분위기와 문화를 형성해준 선호님께 다시 한 번 감사의 말씀을 전합니다. 이런 좋은 환경에 부스팅을 받아서 2023년에는 더욱 정진하겠습니다.😀"
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#목표-달성-여부",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#목표-달성-여부",
    "title": "연간 회고록: 2022년",
    "section": "목표 달성 여부",
    "text": "목표 달성 여부\n노션에 작성해두었던 2022년의 목표를 모두 달성할 수 있었던 뜻깊은 한해였습니다.\n\n\n\n\n\n하지만, 여기에 적어 두지않은.. 꼭 습관을 만들자고 다짐’만’ 하던 것들을 적어봅니다. 올해 2월부터 달려갑니다. 제 1월은 어디갔죠?\n\n책 읽기\n\n2023년엔 작게라도 시작합시다. 한 달에 한 권씩!\n\n영어 공부\n\n아침에 일찍일어나서 영어 공부 한다고 다짐만 1년 했습니다. 하루 30분!\n\n부동산, 증시, 경제 뉴스 읽기\n\n아.. 이것도 습관을 안들이니 바쁘다는 핑계로 안하게 되네요. 하루 30분!\n\n\n써놓고 보면 참 별 거 아닌 습관들인데, 당연하게 몸이 움직이도록 습관을 만들기란 참 어려운 것 같습니다. 올해는 꼭 실천하겠습니다. 커리어 방면의 목표는 개인 노션에 비밀리에 작성하고 2023년 회고록에서 나눠보겠습니다. 사실 아직 구체적으로 못정했어요. 몇 시간 날잡아서 고민하는 시간을 만들어야할 것 같습니다."
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#뽀모도로",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#뽀모도로",
    "title": "연간 회고록: 2022년",
    "section": "뽀모도로",
    "text": "뽀모도로\n\n\n\n\n\n현재 사무실 출근, 재택 근무를 혼합한 형태로 근무를 하는데, 아무래도 집에서 근무를 하면 조금 더 딴짓하게 되고 늘어지는 경향이 있더군요. 퇴근 후에 공부할 때는 더 그렇고요. 이를 지켜만 보고 있을 순 없었습니다. 그래서, 다시 뽀모도로 라는 방법으로 시간 관리를 하기 시작했습니다. 뽀모도로 방법을 요약해볼게요.\n\n1 뽀모도로: 25분\n쉬는시간: 5분\n\n단, 4 뽀모도로 후 쉬는 시간은 15분\n\n\n뽀모도로 중에 당신은 결코 자리에서 일어날 수도 휴대폰을 만지작 거릴 수도 없습니다. 25분만은 업무, 공부에만 집중하세요. 휴대폰이 울린다고요? 아이폰을 쓰신다면 업무 모드를 켜서, 메시지나 전화들을 블락시킬 수 있답니다. 아, 그리고 뽀모도로 간에 만약에 초집중을 하고 있다면 쉬는 시간을 가져가지 않고 바로 뽀모도로를 다시 활성화시키곤 합니다. 근데, 너무 귀찮을 것 같죠? 저걸 어떻게 하나하나 다 체킹하나요. 맥 사용자라면 다음의 앱을 사용해보세요.\n\n\n\n\n\n저는 유료 기능까지 결제해서 사용하고 있어요. 만원 조금 넘는 금액에 평생 무료입니다. 유료 기능을 사용하면, 다양한 태깅을 사용할 수 있고 일간, 주간 보고서도 작성해줘요. 만족스럽게 쓰고있습니다. 뽀모도로는 워낙 유명한 시간 관리법이라 이외에도 다양한 앱이 있습니다. 윈도우에도 검색해보시면 이와 비슷한 프로그램들이 많아요. 집중력이 자주 흐트려지는 분들은 뽀모도로 시간관리법을 도입해보세요. 만족스러운 결과를 얻으실 수 있을겁니다."
  },
  {
    "objectID": "posts/2023-02-01-yearly-memory-2022/index.html#운동",
    "href": "posts/2023-02-01-yearly-memory-2022/index.html#운동",
    "title": "연간 회고록: 2022년",
    "section": "운동",
    "text": "운동\n기술 블로그에는 참 안 어울리는 주제인데요. 군대를 전역하고 쭉 맨몸운동을 하다가, 4학년 1학기 영국에 교환학생을 갔을때 쇠질에 재미를 붙여서 지금까지 헬스장을 쭉 다니고 있습니다. 한 2년 전부터는 2분할에서 3분할로 넘어와서, 지금도 여전히 3일 분할을 하고있습니다. 이 놈의 운동은 여전히 어렵네요. 운동 얘기를 꺼낸 이유는 올해부터는 조금 칼식단을 지켜보려고 해서입니다. 사실, 지금까지 운동을 해오며 칼 식단을 해본 적이 없습니다. 공부, 운동에 식단까지 할 자신이 없었거든요. 올해는 좀 제가 짜둔 칼식단을 최대한 지키면서, 체지방을 좀 깎아보려고 합니다. 시작한지 한 일주일 정도 된 것 같아요. 매일 아침 공복에 몸무게를 재고 있는데, 아직까진 순조롭습니다. 언젠가 정체기가 오겠죠? 최대한 근손실을 막기 위해3, 한 달에 1키로 씩 덜어내는게 목표입니다. 체지방 13% 정도까지 달려가보겠습니다. 이 부분도 2023년 회고록에서 인바디 인증을 약속드리겠습니다.😀\n\n\n\n2020년 여름\n\n\n혹시, 이 글을 읽는 분들 중에 근비대(근육 키우기)를 목적으로 웨이트, 쇠질을 시작하고 싶은데 뭐부터 해야할 지 모르겠는 분들을 위해서 몇 자 덧 붙여봅니다. 먼저, 저는 경험이 아예 없는 분들이 바로 PT로 시작하는 것은 조금 조심하셨으면 합니다. 피트니스 업계에는 워낙 사짜들이 많아서요. 이게 이렇게 하는게 맞는 방법인지도 모른채로, 큰 돈 주고 잘못 배울수도 있습니다. 예를 들어, 근비대가 목적이면 벤치는 가슴으로 밀어야하는데 온 몸의 협응근을 동원하는 리프팅식으로 배운다든지요. 이 부분은 초보자라면 결코 캐치할 수 없습니다. 혼자 들어봤을때보다 쉽게 잘 들리니까 좋은 거라고 생각할 수 밖에 없겠죠?\n조금 조심스러운 부분인데, 유튜브 채널 딱 3개만 권하겠습니다.4 근비대가 목적이신 분들은 강경원, 설기관, 김성환 딱 이 3개 채널만 보고 운동하시면 됩니다. 채널이 3개여서 너무 많다고요? 강경원만 보세요. 거기서 시키는 대로만 하세요. 강경원 채널 들어가서 ‘초보자’ 검색해보면 쫙 나옵니다. 거기서 말하는 것들 달달 외워서 시키는 대로만 딱 3달만 해보세요. 몸에 정말 큰 변화가 찾아오실 겁니다. 3달 해봐도 정말 모르겠다. 혼자 운동하는게 너무 힘들고, 누군가에게 배우고싶다. 그때 PT를 받으십시오. 아니면, 정말 이 부위는 자극이 먹는지 모르겠다. 원포인트 레슨도 좋습니다. 아니면 짤막하게 3-5회 도요. 그럼, 아무것도 모를 때 받는 PT와는 정말 다를 겁니다. 우선, 내 몸에 자극이 오는지 안오는지를 캐치할 수 있으니까요. 그때쯤이면 궁금한게 많아져서, PT 선생님께 이것저것 질문하며 뽑아먹을 것도 많을겁니다.😀 몇 자 덧 붙인다 해놓고 두 문단이나 적었네요. 이만 줄이겠습니다."
  },
  {
    "objectID": "posts/2023-02-07-da-my-view/index.html",
    "href": "posts/2023-02-07-da-my-view/index.html",
    "title": "면접을 앞두고 분석 경험을 회고하는 나만의 방식",
    "section": "",
    "text": "The illustration by Mary Amato\nData Analyst/Scientist 포지션에 서류, SQL 테스트, 코딩 테스트 등은 패스한 뒤, 1차 면접1을 앞두고 있는 상황이라고 가정해보겠습니다. 이때 가장 먼저 신경써야할 부분은 무엇인지 한 번 고민을 해보자고요. 우선, 자기소개서, 포트폴리오를 포함한 이력서에 언급한 것들을 다시 되돌아 보는 것이 가장 먼저겠죠? 이것들이 담고있는 내용들 중 가장 중요한 것은 이제까지 해왔던 분석 경험2들을 천천히 회고해보는 것이라고 생각합니다.\n무엇을 어떻게 회고해야 면접에서 좋은 점수를 받을 수 있을까요? 업계에 몸을 담은지 오래되지 않은 사람의 개인적인 의견이니 가볍게 들어주시면 될 듯합니다.🤗 혹시, 이 글을 읽는 분들 중 시니어 분들이 계시다면 함께 의견 나눠주시면 좋을 것 같습니다. 지금부터 소개드릴 저만의 회고 방식은 “데이터 분석을 대하는 저만의 자세”로도 치환할 수 있을 듯 합니다."
  },
  {
    "objectID": "posts/2023-02-07-da-my-view/index.html#why",
    "href": "posts/2023-02-07-da-my-view/index.html#why",
    "title": "면접을 앞두고 분석 경험을 회고하는 나만의 방식",
    "section": "Why",
    "text": "Why\n\n방법론\n풀고자 하는 문제와 주어진 데이터 관점에서 항상 어떤 방법론을 사용할 지 깊게 고민한 흔적을 드러낼 수 있어야 합니다. 여기에 한 가지 더 얹자면, 모형의 계산량 대비 효율에 관한 고민까지 했었던 사람이라면 더 좋은 점수를 딸 수 있을 것이라 생각합니다.\n\n\n전반적인 분석 과정\n문제 해결을 위해 분석의 각 단계에서 그러한 결정을 한 본인만의 명확한 이유, 고민한 흔적들이 있어야합니다."
  },
  {
    "objectID": "posts/2023-02-07-da-my-view/index.html#how",
    "href": "posts/2023-02-07-da-my-view/index.html#how",
    "title": "면접을 앞두고 분석 경험을 회고하는 나만의 방식",
    "section": "How",
    "text": "How\n\n챌린징\n분석 과정 중 챌린징한 상황을 마주했을때, 이를 해결하기 위해 어떤 노력들을 해봤는가에 대해서 생각해보세요. 아무리 풀기 쉬워 보이는 문제라도 어떤 데이터를 마주하느냐, 또는 어떤 세부적인 사항들을 고려해야하는가에 따라서 얼마든지 챌린징한 상황을 마주할 수 있습니다. 그리고, 현업에서 우리는 상대방을 설득할 수 있을만한 합당안 근거로 이 상황들을 해결해나가야 합니다.\n\n\n계산량, 계산속도\n우리에게 주어져있는 시간, 리소스, 하드웨어, 소프트웨어는 한정적입니다. 모델 적합 또는 수많은 반복 작업이 필요할 때에, 여기에 드는 계산량 및 계산 속도를 개선하기 위해 어떤 노력을 해봤는지 회고해보세요.\n\n\n시각화\n사람들은 텍스트가 아닌 그림을 기억합니다. 그래서, 분석 과정 곳곳에 인사이트를 주기 위해 잘 설계된 시각화 그림을 제공하는 것은 정말 중요합니다. 그림에 들어간 색이 너무 많을때, 이를 줄이기 위한 고민을 해보셨나요? 전달하고 싶은 부분만을 명확하게 전달하기 위해, 시각화를 끊임없이 개선해본 경험이 있으신가요? 얼마나 많은 고민을 녹여내어 어떤 시각화를 수행해오셨나요? 이런 것들에 대한 회고를 해보세요. 남들이 다 하는 수준의 가벼운 시각화 코드를 가져와서, Copy & Paste로 따라하는 데에만 그치지 마셨으면 합니다."
  },
  {
    "objectID": "posts/2023-02-07-da-my-view/index.html#맺음말",
    "href": "posts/2023-02-07-da-my-view/index.html#맺음말",
    "title": "면접을 앞두고 분석 경험을 회고하는 나만의 방식",
    "section": "맺음말",
    "text": "맺음말\n오늘은 면접을 앞두고 분석 경험을 회고하는 제 방식에 대해 공유드려봤습니다. 부제는 데이터 분석을 대하는 저의 자세가 되겠습니다.😀 분석 과정에 있어서 앞서 소개한 측면들에 관한 고민을 하는 습관을 가지고 있지 않다면, 위 질문에 대답하기란 꽤 어려울 수도 있습니다. What은 크게 중요치 않다고 했는데요. 저는 글의 서두에서 당신이 서류는 통과, 기타 테스트는 통과한 1차 면접을 앞두고 있는 사람이라 가정했습니다. 즉, What은 이미 통과되었다고 볼 수 있겠죠. 면접에서 Why와 How를 잘 설명하고, 내가 풀어본 문제가 지원한 회사가 풀고자 하는 What과 어느정도 align이 된다면? 그 면접의 합격은 따놓은 당상입니다.\n자, 이러한 로직을 2차 면접3으로 확장해볼까요? 1차 면접에서 보고자하는 결이 “내가 이 사람을 데리고 일을 할 수 있을까”라면, 2차 면접에서 보고자 하는 결은 당신의 포텐셜에 가깝습니다. 내가 이제까지 해온 분석 경험을 내가 이제까지 해온 인생 경험들로 치환해보세요. 그리고, 지금의 나로 성장하기까지 해왔던 수많은 선택들을 회고해보세요. 거기서 다시 Why와 How를 중심으로 깊게 고민해보세요. 그럼, 2차 면접도 어렵지 않으실 겁니다. 면접과 관련한 팁들은 2022년 이직 로그에서도 이야기를 했었는데, 관심있으신 분들은 참고해보세요.\n이제 글을 마무리 하고자 합니다. 사실, 회고 방식에서 소개한 측면 외에 분석 과정에서 자신에게 끊임없이 던져보아야 할 가장 중요한 질문이 있습니다. 이에 대해 얘기해보면서 글을 마칩니다.\n\n나는 이 문제를 왜 풀고 있는가?\n\n분석이 길어지다 보면, 때로는 본질적으로 어떤 문제를 풀었어야했는지 종종 잊을때가 있습니다. 분석 스텝바이스텝에서 이 부분을 끊임없이 상기시키세요.\n위 질문은 “이 문제를 풀어서 실질적으로 어떤 유의미한 것을 얻을 수 있는가?”에 대한 질문으로 치환될 수 있습니다. 이 문제를 풀음으로써 궁극적으로 얻고자 하는 바가 무엇인지에 대해서도 끊임없이 상기시키세요. 분석 퀄리티를 높여나가는 데에 큰 도움이 되실겁니다."
  },
  {
    "objectID": "posts/2023-02-21-monthly-memory-202301/index.html",
    "href": "posts/2023-02-21-monthly-memory-202301/index.html",
    "title": "월간 회고록: 2023년 1월",
    "section": "",
    "text": "The illustration by Mary Amato\n새해가 밝은지 얼마 지나지 않은듯 한데, 어느덧 2월도 저물어가고 있네요. 얼른 지난 1월을 회고해보려고 해요. 매주 일요일 적었던 1월의 주간 회고를 돌이켜보니, 1월도 느낀 바가 참 많았던 한 달이었더군요. 산업계로 이직을 하고부터는 매달 새로운 것을 느끼고, 새로운 생각을 하게 되는 것 같아요. 여전히 이 곳은 제게 아주 챌린징한 곳인가 봅니다. 지난 1월의 이야기를 일과 개인적인 부분으로 나눠서 이야기해보겠습니다."
  },
  {
    "objectID": "posts/2023-02-21-monthly-memory-202301/index.html#일",
    "href": "posts/2023-02-21-monthly-memory-202301/index.html#일",
    "title": "월간 회고록: 2023년 1월",
    "section": "일",
    "text": "일\n\nStreamlit 캐싱\n이직 후 맨 처음 맡았던 업무는 대시보드 개발이었는데요. 연간 회고록에서 말씀드렸듯이, 대시보드 개발은 Streamlit으로 진행하고 있습니다. 1월 첫째 주에 우선적으로 올릴 지표들, 제가 필요로 된다고 생각했던 반응형 그래프 기능들을 다 반영해서 팀 내에 첫 공유를 드렸었습니다. 그 중 앱의 로딩 속도가 조금 느린데, 개선이 가능하냐는 피드백이 있었는데요. 저도 이 부분은 공유 세션을 가지기 전에 생각했던 부분이긴 했습니다.\n\n\n\n\n\n앱 스크립트를 짤 때 초기에만 1번 Hive를 찔러서 테이블을 가져오는데, 왜 이렇게 앱 로딩 속도가 느릴까? 지나치게 많은 반응형 그래프를 한꺼번에 한 페이지에 로딩을 해서 가져와야하기 때문일까? 이러한 개인적인 추측을 했었죠. 그러나, 원인은 Streamlit이 가진 본질적인 특징에 있었습니다.😀\n그렇다면, Streamlit의 어떤 특징 때문에 앱 로딩 속도가 이렇게 느렸던 걸까요? Streamlit은 앱 사용자가 이것저것 눌러보며 앱의 특정 기능을 실행하면, 스크립트를 처음부터 끝까지 다시 실행시키는 로직을 가지고 있어요. Streamlit으로 대시보드 개발을 손쉽게 만들어주는 데에 이 로직이 큰 역할을 한다고 해요. 문제는 스크립트를 재실행시킬 때마다 함수가 반복적으로 다시 실행되고 객체 또한 반복적으로 다시 불러와지기 때문에, 앱 로딩 속도에 큰 영향을 미친다는 점이죠. 아주 가벼운 형태의 앱이면 문제가 되지 않겠지만요.\n이러한 본질적인 특징때문에 제가 앱 스크립트에 첫 줄에서 딱 한 번만 Hive를 찔러서 가져와도, 제가 만들어둔 반응형 그래프를 유저가 한 번이라도 클릭을 한다면 다시 이 스크립트를 처음부터 다시 실행시키기 때문에 로딩 속도가 느릴 수 밖에 없었던 것입니다. 이 문제는 새로고침을 해도 물론이었으며, 여러 페이지로 구성한 앱 내에서 다른 페이지로 넘어갈 때에도 문제가 됐습니다.\n이는 Streamlit에서 제공하는 캐싱 기능을 통해 아주 손쉽게 해결할 수 있었습니다. 스크립트 초반부에 캐싱 코드 한 줄만 딱 넣어주면, 첫 실행 시 함수 호출 결과와 객체를 저장함으로써 앱 로딩 속도를 대폭 개선해줬죠. 얼마전 Streamlit 1.18.0이 정식 릴리즈되면서, 테스트 중이었던 캐싱 코드도 정식으로 릴리즈되었어요. 관심 있으신 분들은 Streamlit의 블로그 포스트와 Documentation을 참고해보시기 바랍니다.\n\n\n시계열 자료분석 (부제: 세상에 쓸모없는 배움은 없다.)\n여기서는 시계열 자료분석을 수행할 일이 없을 줄 알았는데요. 그럴 일이 생겼습니다!\n\n\n\n\n\n시계열적으로 접근하는 데이터 분석은 언제나 웰컴이에요. 제게 가장 익숙한 문제의 유형 중 하나이기 때문입니다. 여기서 자세하게 어떤 배경에서 어떤 시계열적 접근이 필요로 됐는지 말씀을 드리진 않겠지만1, 여기서 또 한번 느낀 바는 역시나 세상에 쓸모없는 배움은 없다는 것입니다.\n여러분들이 지금 배우고 있는 것들이 추후에 업무에서 어떻게 쓰일 지는 아무도 모릅니다. 항상 최선을 다하세요. 그리고, 걱정하지 마세요. 배워두면 좋을 것 같은데? 근데, 좀 우선순위는 아닌거 같은데? 이런 생각이 드는 것들이 많으신가요? 시간이 허락 된다면 주저하지 말고 배워보세요. 주말에 시간을 내서 이틀 정도만 바짝 해보세요. 새로운 도구의 기본적인 것들을 익히는 데에는 생각보다 긴 시간이 걸리지 않습니다.\n언젠가는 내가 가지고 있는 것과 시너지 효과를 내줄 수 있는 날이 올겁니다. 제 경우에는 블로그에 글쓰기가 그랬고, Git이 그랬고, Python이 그랬고, SAS가 그랬어요. 이 외에도 더 많은 경우가 있지만 이만 줄이겠습니다. 시간이 없다는 핑계, 우선순위가 아닌 것 같다는 핑계가 머리 속을 지배하고 있을 때는 잡스 형님이 Connecting the dots이라는 주옥같은 명언을 남긴 스탠포드 대학교 명연설을 한 번 보세요. 떨어져 가는 학습 동기를 채워넣어주는 최고의 동영상 중 하나입니다.\n\n저는 과거에 주저하기만 하고 결국 우선순위가 아니라는 핑계로 새로운 것에 대한 배움을 미뤄왔던 사람이거든요. 여러분들은 그러지 않으셨으면 합니다. 그래서, 요즘 이렇게 배워야할게 산더미인 듯 해요.😭"
  },
  {
    "objectID": "posts/2023-02-21-monthly-memory-202301/index.html#개인",
    "href": "posts/2023-02-21-monthly-memory-202301/index.html#개인",
    "title": "월간 회고록: 2023년 1월",
    "section": "개인",
    "text": "개인\n\n글또 8기\n지난 1월 말부터 글또2 8기에 참가하고 있어요. 글또는 개발자들의 글쓰기 역량을 강화하고 글을 꾸준히 쓸 수 있도록 돕는 커뮤니티인데요. 지난 한해를 회고하면서 글또 다음 기수에는 꼭 참가를 해보고 싶다는 생각을 했었어요. 그 이유에 대해 조금 이야기 해볼게요. 대학원에 진학하며 개인 블로그를 지금까지 꾸준히 운영해오고 있는데요.3 그런데, 대학원을 졸업하고 글쓰기에는 조금씩 나태해져 가고 있는 제 자신을 발견할 수 있었어요. 업무, 이직, 새로운 산업에 적응 등을 핑계로 말이죠.\n그래서, 새로운 자극이 필요하다고 생각이 들었어요. 넘치는 의욕으로 지난 1차 제출 기간에는 2개의 글을 제출했었어요. 2차 제출 기간의 첫 글은 지금 쓰고있는 이 월간 회고록이 될 듯합니다. 1차 기간에 제출했던 2개의 글도 기술적인 주제의 글은 아니여서, 다음 글은 꼭 기술적인 주제로 써보겠습니다. 블로그를 회고록, 칼럼이 지배해가고 있는데, 이제 기술적인 글을 쓸 때가 되지 않았나 생각하고 있습니다. 아무튼, 글또 8기에 계획된 12번의 글 제출 기간 동안 꾸준하게 글을 써내려가는 모습을 이 블로그를 통해 보여드리겠습니다.😀\n\n\n커피챗\n1월 둘째주 토요일에 제가 선망하고 있었던 R 고수.. 진환님과 만남의 시간을 가질 수 있었어요. 만남의 계기는 진환님의 블로그를 보다가 보내게 된 메일 한통이었어요. 전부터 한번 쯤 꼭 만나뵙고 이야기를 나눠보고 싶었어요. 그래서, 메일을 몇 통 주고받다가 제가 마지막에 불쑥 만나볼 수 있냐고 이야기를 드렸어요. 이런 제안을 드리는 것은 처음이라, 썼다 지웠다를 한 10번 정도 반복한 것 같습니다.😂\n다행이 제 제안을 좋게 봐주셨고, 오프라인에서 만나 이야기를 나누면서 정말 시간 가는 줄 모르고 대화를 나눴어요. 그리고, 여기에 자세한 이야기는 적지 않겠지만, 세상이 참 좁다는 것을 한 번 더 느꼈습니다. 기억에 남는 이야기 중 하나를 해보자면, 수면 시간과 관련된 이야기였는데요. 하루에 보통 수면 시간을 얼마나 가져가시는 지 만나서 꼭 여쭤보고 싶었어요.\n제가 받은 대답은 충격이었습니다..\n\n\n\n\n\n7-8시간씩 주무신다고 합니다. 저랑 수면 시간이 비슷한데.. 진환님 블로그를 들여다보면, 왜 진환님의 하루는 30시간 같죠? 깨어있을 때의 시간이 중요함을 다시 한 번 느꼈습니다. 진환님과의 만남도 제겐 정말 좋은 자극제였습니다. 이런 분들은 평소에 어떤 생각을 하시고, 어떻게 지내시는지 참 궁금했거든요. 다음에 다시 뵐 수 있는 기회가 있었으면 하네요.😀"
  },
  {
    "objectID": "posts/2023-02-21-monthly-memory-202301/index.html#맺음말",
    "href": "posts/2023-02-21-monthly-memory-202301/index.html#맺음말",
    "title": "월간 회고록: 2023년 1월",
    "section": "맺음말",
    "text": "맺음말\n여러분들의 1월은 어떠셨나요? 저는 여전히 배워야할 게 산더미인 요즘입니다. 그래서, 2023년에는 저를 최대한 바쁘게 만들어보고 있습니다. 퇴근하고 운동을 갔다가, 저녁 식사 후 필요한 공부를 하는 생활이 무의식의 습관으로 자리 잡게되는 한 해가 되길 바라며 글을 마칩니다."
  },
  {
    "objectID": "posts/2023-03-12-monthly-memory-202302/index.html",
    "href": "posts/2023-03-12-monthly-memory-202302/index.html",
    "title": "월간 회고록: 2023년 2월",
    "section": "",
    "text": "The illustration by Mary Amato\n지난 2월을 회고해보려고 합니다. 어느덧, 3월 중순이 훌쩍 다가왔네요. 이제까지 써왔던 월간 회고록들에 비해 비교적 빠른 시기에 회고록을 적어보는 듯 합니다. 블로그가 회고록, 칼럼 투성이가 되어가고 있네요. 다음 글은 꼭 기술 주제로 쓴다고 약속하겠습니다. 개인적으로 쓰고 있는 주간 회고들을 둘러보니, 2월에도 참 많은 일들이 있었네요. 늘 그랬듯이, 그 중 함께 공유 드리고 싶은 내용들로 회고록을 채워봅니다. 어쩌다보니 이번 글은 이야기를 들려드리는 어투라기 보다는.. 혼잣말 하는 어투로 채우게 됐네요! 너그러이 이해부탁드립니다.😀"
  },
  {
    "objectID": "posts/2023-03-12-monthly-memory-202302/index.html#일",
    "href": "posts/2023-03-12-monthly-memory-202302/index.html#일",
    "title": "월간 회고록: 2023년 2월",
    "section": "일",
    "text": "일\n\n모바일 홈 개인화\n우리팀1에서 요즘 집중하고 있는 부분은 모바일 홈 개인화이다. 얼마전 미디어를 통해 이 소식을 전하기도 하였다:\n\n\n\n출처: news1.kr\n\n\n지난해에 몇달동안 여러 팀이 붙어 꽤나 많은 리소스를 투입했던 프로젝트이기에, 데이터 분석이라는 도구를 통해 해당 도메인의 추천 서비스를 성장시키는데에 도움을 줘야하는 나로서도 막중한 책임감을 가지고 있다. 얼마전 미디어를 통해 이러한 G마켓의 모바일 홈 개인화 소식이 대중들에게 전해졌다는 소식을 듣고, 조금 걱정스러운 마음이 들었다. 10% 고객에게만 배포한 현 단계에서 미디어를 통해 이 소식을 전하는게 맞는가에 관한 생각이 있었기 때문이다.\n나 또한 이러한 홍보를 통해 모바일 홈에 관심을 끌게 되어 애초에 홈에 관심이 없던 유저들의 이목까지 끌어낼 수 있지 않을까? 하는 생각을 했던 적이 있다. 그러나, 얼마 지나지 않아 이 부분은 실험설계 관점에서 봤을 때 상당히 위험한 행위일 수 있겠다는 생각을 했다. 가령, 우리가 신약의 효과를 검정하고자 하는 실험을 설계한다고 해보자. 보통 이러한 종류의 실험을 설계할 때에는 기본적으로 Single-Blind Test를 수행하거나, 여기서 한 발 더 나아가 Double-Blind Test까지 수행하는 경우가 있다. 두 셋팅의 근본적 목적은 순수한 처리효과2를 보기위함이다. 전자는 피실험자3 본인이 어떤 그룹4에 속했는지 알려주지 않는 실험을 말하며, 후자는 약을 처방하는 의사조차도 본인이 처방할 약이 신약인지 위약인지 모르는 실험을 말한다. 신약의 순수한 효과를 알아보기 위해 이정도 셋팅까지 한다는 말이다.5\n이 관점에 바라보았을 때, 내가 처음에 무심코 했던 “마케팅과 홍보를 좀 잘하면 홈에 관심없던 고객들까지 유입을 시킬 수 있을거 같은데?”라는 생각은 굉장히 위험하다는 것이다. 이러한 홍보는 결국 모바일 홈 개인화가 우리에게 가져다주는 순수한 효과를 보지 못하게끔 만든다. 그래서, 미디어를 통해 G마켓의 모바일 홈 개인화 소식이 전해졌다는 소식을 들었을 때, 모바일 홈 개인화가 가져다 주는 효과가 과대추정(overestimate)되지 않을까 하는 걱정스런 마음이 들었다. 그러나, 이 부분 또한 얼마지나지 않아 괜한 노파심이라는 생각이 들었다. 냉정하게 생각해봤을 때 G마켓에 관심을 두고 우리 소식을 찾아보는 고객이 몇이나 될까? 지상파 뉴스, CF를 통해 대대적인 홍보를 한 것도 아니고. 애초에 G마켓의 AI 기술에 관심을 가지고 있는 고객들 자체가 극소수일 것이기 때문에 모바일 홈 개인화의 효과가 과대추정될 일은 없을거라고 생각하고있다.😀\n\b다만, 모바일 홈에 내보내고 있는 추천 서비스를 고도화해 나가면서 이에 노출되는 고객 비중들을 차차 늘려가며, 기존의 모바일 홈에 비해 고객들에게 더나은 쇼핑 경험을 제공하고 있다는 판단이 들었을 때는 과감한 홍보 등을 통해 신규 고객들을 유입시키고자 하는 움직임이 필요로 된다고 본다.\n\n\n유저 행태 분석\n개인화 시킨 모바일 홈을 일부 고객에게 배포한 뒤에는, 당연한 얘기겠지만 유저 행태 분석이 필요로 됐다. 서비스 출시 후, 유저 행태 분석이 필수적으로 따라 나와야하는 이유에 대해 생각을 해봤다:\n\n보수적인 관점\n\n개인화는 유저들에게 더 나은 경험해주는 서비스임이 자명하다. 하지만, 새로운 것을 도입할 때에는 보수적인 관점에서 사고를 해볼 필요가 있다.6 이 관점에서 우리가 개인화한 홈이 유저들에게 기존의 홈보다 더 나은 쇼핑 경험을 제공하고 있는가에 대한 검증은 꼭 필요로 된다.\n\n서비스 고도화 관점\n\n고객들이 남긴 로그에는 그들의 잔심이 남아있다. 그들이 남긴 잔심을 데이터를 뜯어봄으로써 확인할 수 있다면, 이보다 좋은 서비스 고도화 방법은 없다. 단순한 모델 튜닝만으로 극적인 성과 향상을 기대하긴 어렵다. 다들 알겠지만, 도메인 지식과 비즈니스 인사이트가 투영된 Feature engineering은 Parameters tuning에 비해 비약적인 성과 향상을 가져다 준다. 전자의 방법에서 한 발 더 나아가, 유저 행태 분석을 통해 실제 유저들의 흥미를 이끌어 내는 요소, 유저들이 우리 서비스에서 원하는 바를 파악하여 이 부분까지 Feature engineering에 반영한다면 더욱 큰 시너지 효과를 낼 수 있을 것이다. 아울러, 우리는 Output 그 자체가 아닌 특정 Output을 이끌어내는 Input에 관심이 있기 때문에 이 과정에서 자연스레 인과추론이 필요로 될 수도 있다.\n\n유저 행태 분석과는 조금 동떨어진, 선행되면 좋을 것 같은 부분\n\n유저 행태 분석 전에 좀 더 근본적인 부분에 대해 생각해볼 필요도 있다.\n\n지금 우리 서비스를 성장시키기 위해 필요한 단 하나의 지표(OMTM, One Metric that Matters)는 무엇인가?\n그 전에, 우리가 성장시키고자 하는 서비스는 현재 어떤 단계에 놓여있는가? 매출을 높이고 싶은 단계에 놓여있는가? 출시한 서비스에 관심을 갖는 유저들을 최대한 모아야하는 단계에 놓어있는가?\n즉, OMTM은 서비스를 성장시키고자 하는 방향에 따라 언제든지 달라질 수 있다.\n\n\n\n아무튼, 이번 기회에 (부끄러운 이야기일수도 있지만) 처음으로 유저 행태 분석을 수행해보았다. 1-2주 안에 의미있는 인사이트를 뽑아내어 공유 세션을 가지고 싶었는데, 처음 수행해보는 형태의 분석이여서 그런지 중간중간 분석 방향을 두번 세번정도 엎었다..😭 액셔너블한 분석 결과, 실질적인 비즈니스 인사이트를 가져다줄 수 있는 분석 결과를 가져가려고 노력하다보니, 나 자신이 만족할만한 분석 결과가 좀처럼 나오지 않았다. 그래서, 3주라는 기간이 소요됐다. 그럼에도 불구하고, 소요된 시간에 비해 뽑아낸 인사이트는 굉장히 마음에 안들었다. 이렇게 질질 끌다간 끝이 없을 것 같아 팀 내에 공유 세션을 가졌고, 마침내 팀원들과 인사이트를 나눌 수 있었다. 오랜만에 느껴보는, 긴 기간의 분석 후의 후련함이었다.\n3주간 분석을 수행하며 느낀 바는 다음과 같다:\n\n유저 행태 분석은 정성적인 평가가 필요로 되는 부분이 많은 듯 하다. 그래서, 도메인 지식이 중요하다고들 하는게 아닌가 하는 생각이 들었다. 반대로, 정성적인 평가밖에 할 수 없어 보이는 부분들을 어떻게 정량적으로 평가해볼 수 있을지에 대한 고민도 필요로 되어 보인다. 어찌됐든, 정성적 평가는 주관이 개입하기 마련이기 때문이다. 이러한 이유에서 팀이 필요하고, 집단 지성이 필요로 되는 것이 아닐까?\n앞서 액셔너블한 분석 결과, 실질적 비즈니스 인사이트를 가져다줄 수 있는 결과를 가져가고 싶다고 했었다. 팀내 공유 세션이 끝난 뒤, 요 부분을 핑계삼아 분석 결과 공유가 조금 늦어진 것 같다고 슬랙에 말씀을 드렸다.😂 그랬더니, 선호님(팀장님)이 또 명언을 남겨주셨다.\n\n인사이트가 꼭 있어야만 한다는 부담은 덜어내셔도 됩니다. 정리해주신 팩트를 보면서 인사이트는 같이 논의해도 돼요. 그게 팀이죠. ’어떤 관점으로 데이터를 정리해봤는데 딱히 뭔가 안보인다’는 것 자체만으로도 충분한 정보이자 인사이트가 됩니다.\n\n나는 앞으로도 액셔너블한, 비즈니스 인사이트가 있는 분석 결과를 가져가기 위해 노력할테지만, 선호님의 의견은 많은 위로가 됐고 내게 또다른 새로운 관점을 심어주었다. 이런 분이 리딩하시는 팀에서 데이터 분석을 할 수 있다는게 참 행운이 아닐까 하는 생각을 가지고 있다.\n분석의 뎁스가 깊어지다보면, 종종 분석 목적을 잊게 될때가 있다. 다른 말로 하면, 배가 산으로 갈때가 있다. 데이터 분석을 하며 가장 중요한 부분이 이 부분이 아닐까 생각한다. 분석의 스텝바이스텝 Why를 생각해보는 습관, 끊임없이 내게 질문을 던지고 본 분석을 시작했던 배경, 목적과 잘 align되는 방향으로 분석이 진행되고 있는지에 대해 생각해보는 습관. 이 두 가지가 데이터 분석의 다가 아닌가 생각해본다. 방법론은 이러한 고민 속에서 자연스레 뒤따라 나오는 것이고."
  },
  {
    "objectID": "posts/2023-03-12-monthly-memory-202302/index.html#개인",
    "href": "posts/2023-03-12-monthly-memory-202302/index.html#개인",
    "title": "월간 회고록: 2023년 2월",
    "section": "개인",
    "text": "개인\n\n데이터 분석가/과학자 제 1회 밋업\n지난 2월 23일에는 데이터 분석가 & 데이터 과학자 제 1회 밋업에 참가했다. 장소 마련의 한계로, 추첨을 통해 참가 인원을 선발할 수 밖에 없었는데 운이 좋게도 참가할 수 있는 기회를 얻을 수 있었다. 빅쏠에서 데이터 과학자로 근무하고 계신 이진형님이 데이터 분석가 & 데이터 과학자 모임 오픈 카톡방을 만드신데에 이어, 이렇게 오프라인 밋업까지 주최를 해주셨다.\n\n\n\n\n\n바쁜 와중에 개인적으로 시간을 내서 이러한 모임을 주최하는 것은 정말 어려운 일이다. 이 글을 읽으실 일은 없겠지만, 감사의 마음을 한번 더 표해본다. 우리나라에 개발자 생태계는 온라인에 잘 형성이 되어있는 반면 데이터 분석가/과학자 생태계는 잘 형성이 되어있지 않다는 생각이 들었는데, 이제는 데이터 분석가/과학자 직군의 온라인 생태계도 점차 갖추어져 나가고 있다는 생각이 든다. 이러한 형태의 밋업도 그렇고, PAP와 같은 커뮤니티에서 뉴스레터까지 발행하며 업계에 미치는 영향력을 점차 키워가고 있는 것들을 보면 말이다. 한편으로는 그러한 생태계 형성이 잘 안되어 있는 부분에 갈증이 있었음에도, 나 스스로는 왜 어떤 모임이나 커뮤니티를 만드는 것에 액션을 취하지 않았을까 하는 생각도 한다. 그래서, 이런 부분에 액션을 옮긴 분들께는 더욱 깊은 존경심과 감사함을 가지고 있다.\n이번 밋업에서는 세 분이 발표를 해주셨다. 발표 주제는 다음과 같았다:\n\n<데이터의 불모지, 옥외광고 데이터 전략팀은 무슨일을 할까?>\n\n부제: 없는 데이터 만들어 일하는 방법\n발표자: 포커스 미디어 데이터 전략팀 강슬기\n\n<채용 데이터 분석하기>\n\n발표자: 두들린 한일석\n\n<가입자는 늘어나는데 왜 MAU는 그대로일까?>\n\n발표자: 이진형\n\n\n추천 서비스라는 프로덕트를 성장시키기 위한 데이터 분석을 하고 있는 내가 가장 관심이 가는 주제는 마지막 주제였고, 세 분 모두 발표를 너무 잘해주셔서 정말 재밌게 들었다. 발표가 끝나고, 궁금한 부분들은 질문도 드리면서 적극적으로 참여했다.😂 발표 후 질문은 곧 상대방의 발표를 집중해서 관심있게 들었다는 것의 증거라고 생각하기에, Q&A 시간에 가능하다면 꼭 질문을 드리고자 한다.7 발표 자료는 여기서 만나볼 수 있다.\n발표가 끝난 뒤에는 각자 자유롭게 네트워킹할 수 있는 시간을 가질 수 있었다. 이러한 유형의 모임은 처음이라.. 굉장히 어색했다. 다행히 많은 분들이 적극적으로 움직여주신 덕분에 여러 사람들과 이야기를 나눠볼 수 있었다. 나는 꽤나 내향적인 사람이지만, 이러한 유형의 모임에는 최대한 적극적으로 참가해보려고 한다. 발표를 들으며 인사이트를 얻는 부분도 있지만, 무엇보다 많은 에너지를 얻어갈 수 있기 때문이다. 나와 비슷한 고민을 하는 사람들이 있구나, 이렇게 열심히 하는 사람들이 있구나 하는 것을 눈으로 보고, 그런 사람들과 이야기를 나누는 것은 내게 정말 많은 에너지를 준다. 당시 밋업을 마치고 돌아오면서도 더 열심히 살아봐야겠다는 생각을 했다.\n그리고, 밋업에서 당시 내가 업무에서 몰입하고 있던 유저 행태 분석의 인사이트 추출에도 일정 부분 힌트를 얻을 수 있었다. 밋업을 마치고 집에 도착했을때가 자정을 넘긴 시각이었는데, 여러 인사이트들이 내 머리를 스쳐갔고, 결국 잠을 청하지 못하고 떠오른 인사이트들을 정리하고 새벽 5시쯤 잠에 들었던 기억이 난다.\n이건 여담인데, 밋업이 열리는 당일에 선호님8도 출근을 하셨다. 혹시나 하는 마음에 밋업에 참가하시냐고 여쭤볼까 하다가 참았는데, 역시나 밋업 장소에서 만나뵐 수 있었다. 내가 먼저 사무실을 떠났는데, 슬랙 DM으로 밋업에 참가하러 간다고 말씀드렸더니 선호님도 참가를 하신다고.. 네트워킹 시간에 너어무 어색해서, 선호님 곁을 잠깐 찾아가기도 했다.😂 아무튼, 이번 밋업은 여러모로 내게 즐거운 경험이었다.\n\n\n통계학\n산업계로 이직을 하고, 통계학이 생각보다 Data Science/Analytics 분야에 차지하는 파이는 크지 않음을 느꼈던 때가 있다. 당시 일종의 허탈함이 느껴지기도 했는데, 요즘엔 오히려 내 기본에 깔려있는 통계학이 데이터를 볼때에 알게모르게 정말 많은 도움을 주고 있다는 생각을 한다. 이 생각을 문득 했을때, 바로 이 생각을 정리하고 싶은 마음이 있어 페이스북에 포스팅을 했었다. 본 주제는 페이스북에 남겼던 글로 갈음해본다.\n\n\n\n그로스 해킹\n양승화님이 쓰신 그로스 해킹 책을 거진 다 읽어간다. 학계에서 산업계로 넘어와 추천 서비스의 고도화를 위해 데이터 분석을 수행하고 있는 내게 정말 많은 도움을 준 책이다.\n\n\n\n출처: yes24\n\n\n프로덕트를 성장시키기 위해서는 데이터 분석을 할 때 어떤 방향성을 가지고 해야하는지, 어떤 부분을 조심해야하는지에 대해 많은 부분 힌트를 얻을 수 있었다. 만오천원이 채 안되는 값싼 금액에 저자가 수년간 쌓아올린 노하우들을 들여다볼 수 있다는 것은 정말 말이 안된다고 생각한다.\n가장 깊은 인사이트를 받은 부분은 지표 설계와 관련한 부분이다. 우리팀에서 제공하는 추천 서비스를 성장시키기 위해서는 대체 어떤 지표를 OMTM(One Metric That Matters)으로 삼아야 하는가에 관한 고민이 있었기 때문이다. 이 부분에 대해서도 페이스북에다가 생각을 정리했던 적이 있다:\n\n특정 서비스의 고도화를 위해 데이터 분석을 수행하고 있는 사람9이라면, <그로스 해킹>이라는 책은 한 번쯤 읽어볼 만한 책이라고 생각한다. 책의 뒷장에서는 데이터 분석, 그로스 해킹을 수행할 수 있는 인프라가 갖춰지지 않은 상황에는 어떤 것 부터 시작을 하면 되는지에 대해서도 이야기를 해주는데, 이 부분은 데이터가 흐르는 조직을 만들고자 하는 분들, 데이터 분석가로 채용되어 조직에 들어갔는데 데이터를 활용할 수 있는 환경이 전혀 갖추어져있지 않아 뭐부터 해야할지 고민이 있는 분들께도 많은 도움이 되지 않을까 생각한다."
  },
  {
    "objectID": "posts/2023-03-12-monthly-memory-202302/index.html#유저-행태-분석",
    "href": "posts/2023-03-12-monthly-memory-202302/index.html#유저-행태-분석",
    "title": "월간 회고록: 2023년 2월",
    "section": "유저 행태 분석",
    "text": "유저 행태 분석\n개인화 시킨 모바일 홈을 일부 고객에게 배포한 뒤에는, 당연한 얘기겠지만 유저 행태 분석이 필요로 됐다. 서비스 출시 후, 유저 행태 분석이 필수적으로 따라 나와야하는 이유에 대해 생각을 해봤다:\n\n보수적인 관점\n\n개인화는 유저들에게 더 나은 경험해주는 서비스임이 자명하다. 하지만, 새로운 것을 도입할 때에는 보수적인 관점에서 사고를 해볼 필요가 있다.6 이 관점에서 우리가 개인화한 홈이 유저들에게 기존의 홈보다 더 나은 쇼핑 경험을 제공하고 있는가에 대한 검증은 꼭 필요로 된다.\n\n서비스 고도화 관점\n\n고객들이 남긴 로그에는 그들의 잔심이 남아있다. 그들이 남긴 잔심을 데이터를 뜯어봄으로써 확인할 수 있다면, 이보다 좋은 서비스 고도화 방법은 없다. 단순한 모델 튜닝만으로 극적인 성과 향상을 기대하긴 어렵다. 다들 알겠지만, 도메인 지식과 비즈니스 인사이트가 투영된 Feature engineering은 Parameters tuning에 비해 비약적인 성과 향상을 가져다 준다. 전자의 방법에서 한 발 더 나아가, 유저 행태 분석을 통해 실제 유저들의 흥미를 이끌어 내는 요소, 유저들이 우리 서비스에서 원하는 바를 파악하여 이 부분까지 Feature engineering에 반영한다면 더욱 큰 시너지 효과를 낼 수 있을 것이다. 아울러, 우리는 Output 그 자체가 아닌 특정 Output을 이끌어내는 Input에 관심이 있기 때문에 이 과정에서 자연스레 인과추론이 필요로 될 수도 있다.\n\n유저 행태 분석과는 조금 동떨어진, 선행되면 좋을 것 같은 부분\n\n유저 행태 분석 전에 좀 더 근본적인 부분에 대해 생각해볼 필요도 있다.\n\n지금 우리 서비스를 성장시키기 위해 필요한 단 하나의 지표(OMTM, One Metric that Matters)는 무엇인가?\n그 전에, 우리가 성장시키고자 하는 서비스는 현재 어떤 단계에 놓여있는가? 매출을 높이고 싶은 단계에 놓여있는가? 출시한 서비스에 관심을 갖는 유저들을 최대한 모아야하는 단계에 놓어있는가?\n즉, OMTM은 서비스를 성장시키고자 하는 방향에 따라 언제든지 달라질 수 있다.\n\n\n\n아무튼, 이번 기회에 (부끄러운 이야기일수도 있지만) 처음으로 유저 행태 분석을 수행해보았다. 1-2주 안에 의미있는 인사이트를 뽑아내어 공유 세션을 가지고 싶었는데, 처음 수행해보는 형태의 분석이여서 그런지 중간중간 분석 방향을 두번 세번정도 엎었다..😭 액셔너블한 분석 결과, 실질적인 비즈니스 인사이트를 가져다줄 수 있는 분석 결과를 가져가려고 노력하다보니, 나 자신이 만족할만한 분석 결과가 좀처럼 나오지 않았다. 그래서, 3주라는 기간이 소요됐다. 그럼에도 불구하고, 소요된 시간에 비해 뽑아낸 인사이트는 굉장히 마음에 안들었다. 이렇게 질질 끌다간 끝이 없을 것 같아 팀 내에 공유 세션을 가졌고, 마침내 팀원들과 인사이트를 나눌 수 있었다. 오랜만에 느껴보는, 긴 기간의 분석 후의 후련함이었다.\n3주간 분석을 수행하며 느낀 바는 다음과 같다:\n\n유저 행태 분석은 정성적인 평가가 필요로 되는 부분이 많은 듯 하다. 그래서, 도메인 지식이 중요하다고들 하는게 아닌가 하는 생각이 들었다. 반대로, 정성적인 평가밖에 할 수 없어 보이는 부분들을 어떻게 정량적으로 평가해볼 수 있을지에 대한 고민도 필요로 되어 보인다. 어찌됐든, 정성적 평가는 주관이 개입하기 마련이기 때문이다. 이러한 이유에서 팀이 필요하고, 집단 지성이 필요로 되는 것이 아닐까?\n앞서 액셔너블한 분석 결과, 실질적 비즈니스 인사이트를 가져다줄 수 있는 결과를 가져가고 싶다고 했었다. 팀내 공유 세션이 끝난 뒤, 요 부분을 핑계삼아 분석 결과 공유가 조금 늦어진 것 같다고 슬랙에 말씀을 드렸다.😂 그랬더니, 선호님(팀장님)이 또 명언을 남겨주셨다.\n\n인사이트가 꼭 있어야만 한다는 부담은 덜어내셔도 됩니다. 정리해주신 팩트를 보면서 인사이트는 같이 논의해도 돼요. 그게 팀이죠. ’어떤 관점으로 데이터를 정리해봤는데 딱히 뭔가 안보인다’는 것 자체만으로도 충분한 정보이자 인사이트가 됩니다.\n\n나는 앞으로도 액셔너블한, 비즈니스 인사이트가 있는 분석 결과를 가져가기 위해 노력할테지만, 선호님의 의견은 많은 위로가 됐고 내게 또다른 새로운 관점을 심어주었다. 이런 분이 리딩하시는 팀에서 데이터 분석을 할 수 있다는게 참 행운이 아닐까 하는 생각을 가지고 있다.\n분석의 뎁스가 깊어지다보면, 종종 분석 목적을 잊게 될때가 있다. 다른 말로 하면, 배가 산으로 갈때가 있다. 데이터 분석을 하며 가장 중요한 부분이 이 부분이 아닐까 생각한다. 분석의 스텝바이스텝 Why를 생각해보는 습관, 끊임없이 내게 질문을 던지고 본 분석을 시작했던 배경, 목적과 잘 align되는 방향으로 분석이 진행되고 있는지에 대해 생각해보는 습관. 이 두 가지가 데이터 분석의 다가 아닌가 생각해본다. 방법론은 이러한 고민 속에서 자연스레 뒤따라 나오는 것이고."
  },
  {
    "objectID": "posts/2023-03-12-monthly-memory-202302/index.html#맺음말",
    "href": "posts/2023-03-12-monthly-memory-202302/index.html#맺음말",
    "title": "월간 회고록: 2023년 2월",
    "section": "맺음말",
    "text": "맺음말\n여러분들의 2월은 어떠셨습니까? 주간 회고를 종합해서 돌아보니, 2월에도 정말 많은 배움의 순간들이 있었네요. 글을 쓰면서 몇 번의 퇴고를 거치듯, 분석을 진행해나가며 앞단의 분석들을 다시 되돌아보듯, 주 단위, 월 단위, 연 단위로 삶을 회고해 보는 것은 정말 많은 것들을 가져다주는 것 같습니다. 저는 원래 회고록을 쓰던 사람이 아닙니다. 제가 보기에 너무 멋있고 열정적으로 살아가는 분들이 회고록을 쓰는 것을 보며, 그들의 습관을 따라해보고자 회고록을 쓰기 시작했죠.\n제 인생의 첫 월간 회고록은 지난해 4월이었어요. 시작할때는 참 귀찮은 일이었으나, 이제는 회고를 하지 않으면 마치 조립되지 않은 퍼즐 조각들이 머리 속에 가득한 느낌까지 듭니다. 매주 일요일 밤에 한주 동안 얻은 인사이트를 정리하는 것으로 회고를 시작해보세요. 문득문득 드는 생각들을 정리하는데에 정말 많은 도움이 되실 겁니다."
  },
  {
    "objectID": "posts/2023-03-26-Quasi-Experimental-Designs/index.html",
    "href": "posts/2023-03-26-Quasi-Experimental-Designs/index.html",
    "title": "Quasi Experimental Design",
    "section": "",
    "text": "The illustration by Mary Amato\n오늘은 Potential Outcome Framework 관점의 인과추론 방법론 중 하나인 준실험설계(Quasi-experimental design)에 대해 소개하고자 합니다. Potential Outcome Framework 관점에서 준실험설계는 RCT(Randomized Controlled Trial)가 불가능한 상황에 고려해볼 수 있는 방법론 중 가장 높은 수준의 인과추론을 수행할 수 있는 방법론입니다. Potential Outcome Framework 하에서 인과추론은 결국 이상적인 Counterfactual1과 가장 가까운, 비교가능한 Control 그룹을 찾아내는 것이라 할 수 있는데요. 이러한 Control 그룹을 구성하는 가장 효과적인 방법은 무작위 할당(Random assignment)라고 할 수 있겠습니다. 이를 저희는 RCT라고 표현을 하고, RCT는 Potential Outcome Framework 하에서 인과 효과를 추정하는 일종의 Gold Standard2이기도 하죠. 온라인 비즈니스모델을 갖는 기업에서 순수한 실험 효과, 처리의 순수한 효과를 보기위해서 수행하는 A/B test도 RCT의 일종이라고 볼 수 있습니다.\n이렇게 항상 RCT를 통해 인과적 효과를 추론할 수 있으면 좋겠지만, 상황적, 윤리적 관점에서 바라보았을 때 Context에 따라 RCT가 불가능한 경우는 정말 많이 존재합니다. 이때 우리는 준실험설계를 생각해볼 수 있어요. 준실험설계의 아이디어는 간단합니다. 무작위 할당 없이도 적절한 연구 디자인을 활용한다면, 충분히 비교 가능한 Control 그룹을 구성함으로써 인과추론이 가능해질 것이라는 거죠. 지금은 이 말이 조금 이해가 안되실 수도 있는데요. 준실험설계를 이용해 인과추론을 수행했던 대표적인 사례들을 소개하면서, 이에 대해 자세히 이해해보는 시간을 가져보도록 하겠습니다."
  },
  {
    "objectID": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#potential-outcome-framework-관점의-causal-hierarchy",
    "href": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#potential-outcome-framework-관점의-causal-hierarchy",
    "title": "Quasi Experimental Design",
    "section": "Potential Outcome Framework 관점의 Causal Hierarchy",
    "text": "Potential Outcome Framework 관점의 Causal Hierarchy\n잠깐 준실험설계의 대표 사례들을 이야기하기 전에 Potential Outcome Framework 관점의 Causal Hierarchy를 훑어보고 가겠습니다. 앞서 준실험설계는 RCT가 불가능한 상황에 가장 높은 수준의 인과추론을 수행할 수 있게끔 해주는 방법론이라고 말씀을 드렸었는데요. 다음의 Causal Hierarchy 그림에서도 이를 확인하실 수 있습니다.\n\n\n\nSource: 인과추론의 데이터과학\n\n\n위 그림은 Potential Oucome Framework 관점의 방법론들 각각이 갖는 인과추론의 수준을 계층적으로 표현한 그림인데요. 순서대로 짤막하게 설명을 하고 넘어가겠습니다.\n\n메타분석(Meta-Analysis): 여러가지 실험의 결과를 종합한 것\nRCT: 단일 수준에서 가장 높은 수준의 인과추론을 가능하게끔 해줌\n준실험(Quasi-Experitment): 특정 가정하에서 RCT에 가까운 인과추론을 수행할 수 있는 강력한 도구\n도구변수(Instrumental Variable): 내생성(Endogeneity)3을 제거하기 위한 도구로, 항상 이러한 도구변수를 찾을 수 있는 것은 아님\n“Desgined” Regression/Matching (based on causal knowledge or theroy): 도구변수를 찾을 수 없는 상황에 마지막으로 고려할 수 있는 방법. 기본적으로 Regression이나 Matching은 인과추론에 한계가 굉장히 많음. 그래서, 특별히 주의를 기울인 단순한 Regression/Matching과 구분하기 위해 “Designed”라는 단어를 앞에 붙여 놓으셨다고 함.\nRegression/Matching (Little causal inference): 잘 디자인 되지 않은 단순한 Regression/Matching의 인과추론 수준은 매우 낮음\nModel-Free Descriptive Statistics (No causal inference): 평균과 같은 기술 통계량들\n\n여기서 RCT부터 도구변수까지는 Selection on Unobservables Strategies라고 해서, 관측되지 않은 요인에 의한 Selection bias(선택편향)까지 통제할 수 있는 전략에 해당합니다. 그리고, RCT는 무작위 할당을 활용(exploit), 준실험과 도구 변수는 연구 디자인을 활용합니다. 마지막으로, “Designed” Regression/Matching은 Selection on Observables Strategies로 관측된 요인에 의한 Selection bias만을 통제할 수 있는 전략에 해당합니다."
  },
  {
    "objectID": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#사례-i-사회제도가-경제-성장에-인과적인-효과를-미치는가",
    "href": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#사례-i-사회제도가-경제-성장에-인과적인-효과를-미치는가",
    "title": "Quasi Experimental Design",
    "section": "사례 I: 사회제도가 경제 성장에 인과적인 효과를 미치는가?",
    "text": "사례 I: 사회제도가 경제 성장에 인과적인 효과를 미치는가?\n사회제도가 경제 성장에 미치는 영향은 경제학에서 굉장히 오래 연구된 주제라고 합니다. 간단해 보이는 가설이지만 데이터로 부터 이를 추론하기란 만만치않습니다. 우선 실험이 불가능한 가설이에요. 즉, RCT로는 확인이 불가능한 가설이란 얘기죠. 그럼, 이제 우리에게 필요한 것은 비교가능한 Control 그룹을 찾는 노력입니다. 우리가 가장 쉽게 떠올릴 수 있는 예로는 우리나라와 북한의 경우가 있습니다. 그러나, 두 국가는 본 문제에 있어서 ceteris paribus4를 만족시키기란 매우 어렵습니다. 두 국가 간의 차이에는 사회 제도 뿐아니라 수많은 요소들이 존재하죠.\n이 문제를 해결하기 위해서는 사회 제도만 다른, 비교 가능한 쌍둥이 국가 또는 쌍둥이 도시를 가져와서 비교를 수행할 필요가 있습니다. 그 중 대표적인 사례가 미국 애리조나 주와 멕시코에 위치한 서로 같은 이름을 지닌 Nogales라는 쌍둥이 도시를 통해 비교한 사례라 할 수 있습니다.\n\n\n\nSource: 인과추론의 데이터 과학\n\n\n이름이 같은 것에서 알 수 있듯이, 역사적으로 Nogales는 오랫동안 같은 도시 였다가 미국과 멕시코의 국경이 달리되며 절반은 미국의 사회제도 하에, 절반은 멕시코의 사회 제도 안에 남게 된 것 입니다. 즉, 사회제도 외에 인구 구성, 문화, 도시 인프라, 지리적으로나 두 지역은 굉장히 유사할 것이고, 이에 따라 사회제도가 경제 성장에 미칠 수 있는 영향을 보고자할 때 둘은 비교 가능한 그룹이라 할 수 있는 것이죠.\n사진 속에서 두 도시의 2019년 1인당 GDP를 비교해보시면 아시겠지만, 미국의 사회제도 속에서 노갈레스는 멕시코의 노갈레스보다 약 6배 이상 부유한 도시가 되었습니다. 이렇게 인구 구성이나 지형, 종교, 문화가 모두 비교가능한 쌍둥이 도시들 간에 경제 성장의 차이를 바탕으로, Potential Outcome Framework 하에서 사회제도가 경제 성장에 미치는 영향에 대해 어느정도 추론을 해볼 수 있겠죠. 이러한 방식으로 자연 상황에서 비교 가능한 Control 그룹을 구성함으로써, 최대한 인과적인 효과를 발라내보자는 것이 바로 준실험설계라고 할 수 있겠습니다."
  },
  {
    "objectID": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#사례-ii-최저임금이-고용률에-영향을-미치는가",
    "href": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#사례-ii-최저임금이-고용률에-영향을-미치는가",
    "title": "Quasi Experimental Design",
    "section": "사례 II: 최저임금이 고용률에 영향을 미치는가?",
    "text": "사례 II: 최저임금이 고용률에 영향을 미치는가?\n두 번째 사례는 2021년 노벨경제학상 수상자인 David Card 교수님의 대표적인 연구 사례이기도 합니다. 이 문제 또한 가설만 보면 꽤 간단히 보이지만, 이를 데이터를 통해 추론하기란 꽤나 어렵습니다. 최저임금이 상승될 경우 고용률은 감소할 것 이라는 주관적인 결론을 쉽게 내리는 경향이 있는 가설인 만큼, 데이터를 통한 객관적인 검증이 꼭 필요로 되는 가설이 아닌가 하는 생각도 듭니다.\n\n\n\nSource: 인과추론의 데이터 과학\n\n\nDavid Card 교수님이 이 가설의 검정을 위해 찾아내신 비교가능한 두 그룹은 펜실베니아와 뉴저지였습니다. 1992년 펜실베니아와 뉴저지의 최저임금을 비교함으로써, 최저임금이 고용률에 미치는 영향을 검증해보고자 하셨죠. 해당 가설에 대한 최초의 실증 분석 시도이기도 했습니다.\n당시 펜실베니아와 뉴저지의 최저임금이 비교가능했던 이유는, 1992년 4월 뉴저지의 최저임금이 4.25달러에서 5.05달러로 상승한 데에 있습니다. 반대로, 펜실베니아의 최저 임금은 그대로였고, 두 주는 인접한 지역에 위치했죠. Card 교수님은 두 주를 나누는 경계 주변에 위치한 레스토랑의 고용률을 비교해보셨습니다.\n결과는 위 사진의 우측 위 그래프를 보시면 알 수 있듯, 해당 가설에 대해 사람들이 쉽게 내리는 주관적 결론과는 조금 달랐습니다. 두 주 간에 고용률 변화는 거의 없다가, 오히려 뉴저지의 고용률이 더 높아지는 추이를 보였죠. 즉, 최저임금을 올린다고 해서 고용률이 항상 낮아지는 것은 아니라는 것입니다. 우리가 별 생각없이 특정 가설에 대해 내리는 결론은 꽤나 위험할 수도 있다는 것을 상기시켜 주는 대표적인 사례가 아닐까 생각해 봅니다."
  },
  {
    "objectID": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#맺음말",
    "href": "posts/2023-03-26-Quasi-Experimental-Designs/index.html#맺음말",
    "title": "Quasi Experimental Design",
    "section": "맺음말",
    "text": "맺음말\n준실험설계의 아이디어는 결국 Counterfactual을 대신할 수 있는 비교가능한 Control 그룹을 찾자5는 것에 있습니다. 단, 연구자가 직접 무작위 할당을 통해 비교가능한 Control 그룹을 구성하는 것이 아닌, 자연 상황에서 비교 가능한 Control 그룹을 찾아보자는 것이고요. 이렇게, 자연적으로 발생한 상황을 마치 실험처럼 활용한다는 점에서 준실험설계는 자연 실험(Natural Experiment)이라 불리기도 합니다. 본 글에서 소개한 대표적인 연구 사례들 또한 이러한 로직으로 Control 그룹을 구성하였고요. 아울러, 이러한 연구 사례들이 시사하는 바는 Potential Outcome Framework 관점에서 준실험설계는 연구자가 무작위 할당을 통해 그룹을 배정하지 않더라도, 현실 상황을 잘 이용하여 비교 가능한 대상을 설정한다면, 충분히 합리적인 인과 관계를 추론할 수 있다는 것입니다.\n본 글을 쓰는데에 기반이 된 (인과추론의 데이터과학 2022)에서는 이러한 인과 관계의 추론을 이야기하는 데에 있어서 결코 어떤 복잡한 모형이나 통계적 이론, 데이터의 양에 대한 언급은 일절 하지 않습니다. 단지, 연구 디자인을 잘 활용하여 얼마나 비교 가능한 Control 그룹을 구성할 수 있었는 지에 대해 이야기했죠. 이러한 연구 디자인은 따로 정형화되어 있지 않으며, Context에 따라 매우 다양한 디자인이 존재합니다. 이 부분에 있어서 도메인 지식은 정말 중요한 역량이 될 것으로 보이며, 동시에 자신이 풀고자 하는 실험이 불가능한 인과 추론 문제를 자연 상황에서 어떻게 풀 수 있을지 고민해보는 습관 또한 매우 중요해 보입니다."
  },
  {
    "objectID": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html",
    "href": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html",
    "title": "시계열 자료분석을 활용한 고객 관심사의 선제적 반영",
    "section": "",
    "text": "The illustration by Mary Amato\n시계열 자료분석에 관한 이야기하기를 하기 전에, 먼저 해당 방법론이 필요로 됐던 배경과 목적에 대해 설명을 드리려고 해요. 우리는 What보다 Why가 중요한 사람들이니까요.😀 2월말에 보도자료로도 나간 내용이지만, 저희 팀은 요즘 모바일 홈 개인화에 집중하고 있어요. 모바일 홈 개인화는 G마켓 모바일웹/앱의 홈 메인에서 스크롤을 조금 내렸을 때 보이는 슈퍼딜 영역1을 개인화하는 프로젝트에요. 여기서 말하는 개인화는 각 고객님들이 남긴 데이터를 바탕으로 관심을 가질만한 상품들을 순차적으로 띄워주는 것을 말해요. 관심 있을만한 상품들을 좋은 가격에 최대한 손쉽게 찾으실 수 있도록 말이죠. 즉, 개인의 관심을 반영한 일종의 모바일 홈 랭킹 시스템을 만드는 것이라고 보면 되어요.\n랭킹 시스템을 고도화하여 고객들에게 더 큰 쇼핑 만족도를 드리기 위해 모든 팀원분들이 정말 많은 노력을 기울이고 있는데요. 저는 다음과 같은 부분을 생각해봤어요:\n이 부분을 반영하기 위해서는 매년 특정 시기에 반복적으로 관심이 많아지는 상품을 정의하고, 그 “특정 시기”는 언제 시작하고 끝나는지도 알아야 했어요. 그리소, 이 문제를 해결하기 위해서는 시계열적인 접근이 필요했죠.\n이만하면 시계열 자료분석2이 필요했던 문제의 배경과 목적이 충분히 이해가 되셨을까요? 이제 방법론 얘기를 해보겠습니다. 본 문제를 해결하는 데에 사용한 방법론은 크게 3가지 입니다:\n이제 이 방법론들이 무엇이고, 왜 필요했는지에 대해 본격적으로 이야기해보겠습니다."
  },
  {
    "objectID": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#시계열-분해",
    "href": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#시계열-분해",
    "title": "시계열 자료분석을 활용한 고객 관심사의 선제적 반영",
    "section": "시계열 분해",
    "text": "시계열 분해\n시계열 분해는 시계열을 몇 가지 요소로 분해해서 보는 것을 말합니다. 실제로 관측되는 시계열 자료들은 정말 다양한 패턴3으로 나타나기 때문에, 시계열 분해는 해당 시계열이 갖는 본질적인 특성을 이해4하는 데에 정말 많은 도움이 됩니다. 고윳값 분해(eigenvalue decomposition)를 통해 행렬이 갖는 본질적인 특성을 이해할 수 있는 것 처럼 말이죠.\n시계열 분해를 수행하면 시계열을 3가지 요소로 분해할 수 있습니다:\n\n추세-순환 성분(trend-cycle5 component): 추세는 긴 기간 동안 나타나는 상승이나 하락을 의미하며, 순환(cyclic)은 고정된 주기로 나타나지 않는 자료의 상승이나 하락을 의미합니다. 이러한 변동은 보통 경제 상황, 비즈니스 주기에 의해 발생합니다.\n계절 성분(seasonal component): 계절적 요소에 의해 영향을 받는 것을 나타냅니다. 계절 성분을 순환 성분과 혼동하는 경우가 있는데, 계절 성분은 항상 특정 시기에 알려진, 고정된 기간에 나타나는 성분을 말합니다.\n나머지 성분(remainder component): 앞서 소개한 성분들로 설명되지 않는 나머지 변동을 나타냅니다. 본 변동이 큰 경우 해당 시계열 자체만을 모델링에 고려하는 것이 아닌, 다른 자료까지 추가적으로 고려하여 해당 변동을 설명해낼 필요가 있습니다.\n\n여기서 모델링 쪽에 식견이 있는 분들은 어느정도 힌트를 얻으셨겠지만, 이렇게 시계열 분해를 통해 얻게된 본질적 특성에 관한 이해는 모델링 단계에서 정말 많은 도움을 줍니다. 시계열을 분해하는 방법6은 다양합니다. 그 중 실무에서 활용한 STL decomposition에 대해 간략하게 소개하겠습니다.\n\nSTL decomposition\nSTL(Seasonal and Trend decomposition using Loess7)은 Cleveland(Cleveland et al. 1990)가 1990년에 제안한 방법론으로, 다재다능하고 로버스트한 시계열 분해 방법론입니다. STL이 이동평균을 활용한 고전적인 시계열 분해 방법론, SEATS 방법론, X-11 방법론에 비해 갖는 몇가지 이점은 다음과 같습니다:\n\n월별, 분기별 계절성 뿐만이 아닌 여러 유형의 계절성을 다룰 수 있음\n시간에 따라 변화하는 계절 요소 반영 가능\n추세-순환 성분의 평활도(smotheness)8 조절 가능\n추세-순환 성분과 계절 성분이 영향을 받지 않도록, 이상점에 로버스트한 분해 가능\n\n여러가지 시계열 분해 방법론 중 STL decomposition을 활용한 이유에는 2가지가 있습니다. 앞서 소개한 이점 중 이상점에 로버스트한 분해가 가능하다는 부분이 첫번째 이유였고, 두 번째는 주어진 문제9 해결을 위해 각 시계열 자료가 갖는 추세 강도와 계절성의 강도를 정량적으로 측정할 필요가 있었기 때문입니다.\nSTL decomposition을 통해 얻은 성분들을 활용하면 추세 강도, 계절성의 강도를 측정할 수 있습니다. 수식을 쓰지 않으려고 했는데 추세 강도와 계절성 강도를 측정하는 통계량을 소개하기 위해 편의상 간단한 수식 몇개만 적겠습니다. 시계열 \\(y_t\\)의 가법적(additive) 분해10는 보통 수식으로 다음과 같이 나타냅니다.\n\\[\ny_t = T_t + S_t + R_t\n\\]\n여기서는 \\(T_t\\)는 평활된 추세 성분, \\(S_t\\)는 계절 성분, \\(R_t\\)는 나머지 성분을 의미합니다. 요 세 성분들은 우리가 앞서 소개했던 STL decomposition을 통해 얻을 수 있습니다. 아래 그림 1에서 trend는 \\(T_t\\), season_year는 연도별(yearly) 계절성을 나타내는 \\(S_t\\), remainder는 \\(R_t\\)를 나타냅니다.\n\n\n\n그림 1. R의 fpp3 패키지에서 제공하는 1939-2019년 미국 월별 고용률 자료(us_emplyment)에서 1990년을 시작점으로 뽑아낸 자료를 STL decomposition을 활용해 가법적으로 분해한 그림\n\n\n이제 추세와 계절성의 정량적 측정에 이 성분들이 어떻게 이용되는지 설명드려보죠. 먼저, 설명의 편의상 계절성이 제거된 시계열 자료가 있다고 생각해볼게요. 만약 해당 자료의 추세가 강할 경우 추세에 의한 변동이 나머지 성분에 의한 변동보다 훨씬 클겁니다. 즉, 추세 성분과 나머지 성분을 합한 값의 분산 \\({\\rm{Var}}(T_t + R_t)\\)는 나머지 성분이 갖는 분산 \\({\\rm{Var}}(R_t)\\)에 비해 상대적으로 크겠죠. 반대로 추세가 존재하지 않거나 아주 약한 수준을 띠는 시계열의 경우 \\({\\rm{Var}}(T_t + R_t)\\)과 \\({\\rm{Var}}(R_t)\\) 간에는 별다른 차이가 없을 겁니다. 그러므로, 우리는 추세 강도를 나타내는 통계량 \\(F_T\\)를 다음과 같이 정의해볼 수 있습니다.\n\\[\nF_T = {\\rm{max}} (0, 1-\\frac{{\\rm{Var}}(R_t)}{{\\rm{Var}}(T_t + R_t)})\n\\]\n즉, 위 통계량은 추세가 강할수록 분모가 커져서 1에 가까운 값을, 반대로 추세가 약할수록 분모와 분자간 차이가 없어져서 0에 가까운 값을 가지게 됩니다. 계절성의 강도를 나타내는 통계량 \\(F_S\\) 또한 위와 같은 로직으로 정의됩니다:\n\\[\nF_S = {\\rm{max}} (0, 1-\\frac{{\\rm{Var}}(R_t)}{{\\rm{Var}}(S_t + R_t)})\n\\]\n위 2가지 통계량을 통해 우리는 시계열의 추세, 계절성의 강도를 정량적으로 평가할 수 있게 됩니다. 물론, 추세가 “강하다”, 계절성이 “강하다”에 관한 주관적 판가름은 각 문제 상황에 맞는 정성적인 평가가 뒤따라야 하겠지만요."
  },
  {
    "objectID": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#이상점-탐지-및-대치",
    "href": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#이상점-탐지-및-대치",
    "title": "시계열 자료분석을 활용한 고객 관심사의 선제적 반영",
    "section": "이상점 탐지 및 대치",
    "text": "이상점 탐지 및 대치\n앞서 STL decomposition은 이상점에 로버스트한 방법론이라고 소개를 드렸습니다. 그러나, 추세 강도와 계절성의 강도를 측정하는 데에 있었서는 사전에 이상점에 대한 처리가 꼭 필요로 됩니다. 그 이유는 STL decomposition에서 이상점에 대해 로버스트하게 분해를 수행하기 위해, 이상점을 나머지 성분으로 간주하기 때문입니다. 이러한 이유로 극단적인 이상점을 갖는 자료의 경우 나머지 성분의 변동이 매우 커져서, 추세와 계절성이 자명하게 큰 시계열임에도 추세 강도 통계량과 계절 강도 통계량은 매우 작게 계산이 됩니다. 다음과 같은 시계열의 형태가 하나의 예가 되겠죠.\n\n\n\n그림 2. R의 fpp3 패키지에서 제공하는 지역별로 나뉜 분기별 호주 여행 수요 자료(tourism) 중 호주 남부에 위치한 Adelaide Hills 지역의 분기별 여행 횟수를 나타낸 그림\n\n\n실제로 위 자료를 분해해보면 다음과 같은 패턴을 띱니다:\n\n\n\n그림 3. 위 자료에 대해 이 STL decomposition으로 이상점에 로버스트하도록 가법적으로 분해한 결과\n\n\n추세와 계절성이 잘 추정되었다고 한들, 이상점이 remainder 부분으로 빠져있기 때문에 추세가 자명한 자료 임에도 추세 강도 통계량의 값은 매우 작게 추정이 될겁니다. 실제로 추정을 수행해보면 상승 추세가 자명함에도 \\(F_t\\)는 0.488에 불과한 값으로 추정이 됩니다.\n이러한 이상점을 효과적으로 탐지하기 위해 사용할 수 있는 아주 간단한 통계적 방법론을 소개해드리겠습니다. 통계학 전공자들은 상자 그림(Box plot)을 참 좋아합니다. 연속형 자료가 갖는 분포와 분위수, 이상점까지 한 눈에 쉽게 확인할 수 있는 아주 좋은 그림이기 때문이죠. 앞서 소개한 호주 남부에 위치한 Adelaide Hills 지역의 분기별 여행 횟수를 가지고 상자 그림을 그려볼게요.\n\n\n\n그림 4. Adelaide Hills 지역의 분기별 여행 횟수에 관한 상자 그림\n\n\n그림 4에서처럼 상자 그림에서 점으로 찍히는 데이터포인트는 이상점으로 간주하는데요. 상자 그림을 그리는 로직을 활용하면, 이상점을 탐지하는 간단한 방법론을 설계할 수 있습니다. 방법론의 설계를 위해 먼저 상자 그림을 그리는 로직을 이해해야겠죠?\n먼저, 그림 4에서 상자의 윗변과 아랫변은 각각 여행 횟수의 첫 번째 분위수(\\(Q_3\\), 25%에 해당하는 값)와 세 번째 분위수(\\(Q_3\\), 75%에 해당하는 값)로 그려집니다. 상자 안에 진한 실선은 중위수(\\(Q_2\\), 50%에 해당하는값)에 해당하고요. 그리고, 상자의 세로변 길이 즉, \\(Q_3 - Q_1\\)에 해당하는 값을 우리는 IQR(Interquartile range)이라고 표현합니다. 마지막으로 상자 위아래로 뻗은 직선을 보고 우리는 상자 수염(Box-whisker)이라 표현합니다. 요 상자 수염은 상자의 윗변, 아랫변과 IQR을 활용해 그려집니다. 그림에서 위로 뻗은 상자 수염의 끝 점은 보통 \\(Q_3 + 1.5\\times{\\rm{IQR}}\\) 내에 최댓값으로, 아래로 뻗은 상자 수염의 끝 점은 \\(Q_1 - 1.5\\times{\\rm{IQR}}\\) 내에 최솟값으로 정의됩니다. 요 상자 수염을 벗어나는 데이터포인트는 이상점으로 간주되어 그림 상에서 점으로 표시됩니다.\n보시다시피 그림 4에서는 5개의 점이 표시되어 있으나, 그림 2와 3에서 파악할 수 있듯이 여행 횟수가 80이 넘는 데이터포인트만 이상점으로 간주하는게 자연스러워 보이죠. 이를 위해서는 상자 수염을 그릴때 IQR에 곱해지는 값을 키워서 보수적으로 이상점을 탐지해주면 됩니다. 해당 값을 1.5에서 3정도로 키우면 우리가 원하는 수준으로 이상점을 탐지할 수 있습니다:\n\n\n\n그림 5. 이상점을 보수적으로 간주한 상자그림\n\n\n사실, 위와 같이 하나의 시계열에 대해 이상점을 탐지하는 경우 그림을 보고 간단하게 여행 횟수가 80이 넘는 경우 NA 처리 내지는 이상점을 나타내는 컬럼을 추가해 1로 레이블링을 해주면 되겠지만, 수많은 시계열에 대해 이상점을 탐지해야하는 경우 그림 하나하나를 일일이 확인하기란 어렵습니다. 이럴 때에는 방금 우리가 상자 그림을 그리는 로직을 바탕으로 설계한 이상점 탐지 방법론을 사용하면 되겠죠. IQR에 곱해지는 값을 본 예에서는 3으로 키웠지만, 때에 따라 추세 변동에 따라 계절 변동이 함께 커지는 경우, 또는 순환 성분이 혼재되어 있는 경우 IQR에 곱해지는 값을 훨씬 더 크게 키워서 보수적으로 이상점을 탐지해내야 할 수도 있습니다. 그렇지 않으면, 간혹 계절 변동에 의해 발생되는 값 혹은 순환성에 의해 발생되는 값들까지 이상점으로 간주하는 경우가 생길 수 있기 때문입니다.\n이렇게 이상점에 대해 NA 처리 또는 컬럼으로 레이블링 한 후에 대치는 어떻게 하는게 좋을까요? 예측 모델링을 통해 해당 지점을 보간(interpolation)하는 방법도 있지만, 모델의 퍼포먼스를 세세하게 개선해야할 목적이 있거나 수많은 이상점들이 여기저기 혼재되어 있지 않는 이상, 굳이 그러한 방법론까지 고려할 필요는 없습니다. 이상점에 해당하는 지점 값을 바로 전 또는 후 지점에 해당하는 값으로 대치를 한다든지, 해당 시계열의 평균이나 중위수로 대치를 한다든지와 같이 아주 간단한 방법을 활용할 수도 있어요. 개인적으로는 시계열의 패턴을 그대로 살리는 것을 선호하여 평균이나 중위수로 대치하는 방법 보다는 전자의 방법을 택하긴 합니다.\n다음은 그림 5의 이상점을 NA 처리한 뒤에, 바로 전 분기의 여행 횟수를 대치하여 STL decomposition을 수행한 결과에 해당합니다:\n\n\n\n그림 6. 전 분기의 여행 횟수를 이상점에 대치하여 STL decomposition을 수행한 결과\n\n\n분해 결과가 훨씬 자연스러워졌죠? 이렇게 이상점을 탐지하고 대치시킨 자료에서 추세 강도 통계량을 계산해보면 0.488에 불과했던 값이 0.701로 계산이 됩니다."
  },
  {
    "objectID": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#변화점-탐지",
    "href": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#변화점-탐지",
    "title": "시계열 자료분석을 활용한 고객 관심사의 선제적 반영",
    "section": "변화점 탐지",
    "text": "변화점 탐지\n시계열 변화점 탐지는 시계열의 평균과 분산의 변화 또는 구조적 변화가 일어나는 지점을 찾아내는 방법론을 말합니다. 말로만 들으면 어려울 수 있는데, 다음 그림을 보시면 어떤 방법론인지 직관적으로 쉽게 이해하실 수 있을거에요.\n\n\n\n그림 7. R의 changepoint 패키지를 활용한 시계열 변화점 탐지의 예\n\n\n고객들이 관심있을만한 상품을 선제적으로 파악하는데에는 추세 강도를 정량적으로 측정하는 작업도 필요했지만, 끝으로 이러한 변화점 탐지 작업을 통해 추세가 변화하는 지점들까지 파악할 필요가 있었어요. 앞서 말씀드렸듯이 관심이 많아지는 특정 시기가 언제 시작하고 언제 끝나는 지에 관한 정보가 필요했으니까요. 변화점 탐지의 경우 방법론적 디테일보다는 R의 changepoint 패키지를 중심으로 실무에서 변화점 탐지를 수행해보며 얻은 노하우에 대해 이야기해보려고 합니다.\n해당 패키지는 시계열 자료의 평균, 분산이 크게 변화하는 지점을 탐지할 수 있는 알고리즘을 담고 있는데요. 이진 분할(Binary Segmentation), PELT(Pruned Exact Liner Time) 등의 알고리즘 옵션을 제공하는데, 이진 분할 방법의 경우 변화점의 최대 개수를 조정해줄 수 있습니다. 특정 시계열 자료의 변화점 개수를 도메인 지식을 통해 추정할 수 있다면, 이진 분할 방법은 아주 매력적인 옵션이 될 것입니다.\n주어진 알고리즘 옵션을 통해 시계열의 평균 또는 분산 각각의 변화에 따른 변화점을 탐지할 수 있게 되는데, changepoint 패키지에서는 평균과 분산의 변화를 동시에 고려하여 변화점을 탐지할 수도 있습니다. 평균 변화는 cpt.mean() 함수, 분산 변화는 cpt.var()함수로, 평균과 분산이 변화를 동시에 고려하기 위해서는 cpt.meanvar()함수를 사용하시면 되는데요. 추세에 따라 시계열의 분산이 함께 커지는 시계열에 대해서는 평균과 분산의 변화를 동시에 고려할 필요가 있을겁니다. 그렇지 않으면, 분산이 추세와 함께 커지는 부분을 반영하지 못하여 눈으로 보고 정성적으로 판단했을 때는 변화점이라고 판단하기 어려운 지점을 변화점이라고 탐지를 하는 경우가 생기거든요.\n해당 패키지에 관한 쉽게 쓰여진 예제 코드는 (Killick and Eckley 2014)에서 제공하고 있습니다. 더 깊은 방법론적 디테일이 궁금하신 분들도 참고하시면 좋을겁니다. 그리고, 본 라이브러리와는 조금 다른 접근으로 베이지안 추론을 활용하여 시계열의 변화점을 탐지해볼 수도 있습니다. 이 부분에 관심이 있으신 분들은 R의 {mcp} 패키지를 참고해보시기 바랍니다."
  },
  {
    "objectID": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#맺음말",
    "href": "posts/2023-04-02-time-series-analysis-for-customer-interests/index.html#맺음말",
    "title": "시계열 자료분석을 활용한 고객 관심사의 선제적 반영",
    "section": "맺음말",
    "text": "맺음말\n현재는 G마켓에서 추천 서비스를 성장시키기 위한 데이터 분석을 수행하고 있지만, 대학원을 졸업할 무렵 그나마 가장 자신있었던 연구 분야는 시계열 자료분석이었고, 이직을 하기 전에 몸을 담았던 첫 직장에서도 추론 관점11의 시계열 모델링을 주 업무로 수행했었습니다. 이직한 뒤에는 시계열적 접근이 필요한 문제를 풀 일이 없을 줄 알았는데, 신기하게도 그럴 일이 생기더군요. 분석 중간 중간 난관에 봉착할 때도 있었지만, 가장 익숙한 분야였던터라 난관을 하나씩 해결해나가는게 꽤나 재밌었습니다. 업무에서 풀어냈던 문제라 자세한 분석 과정은 설명드리지는 못했지만, 각 방법론에서 소개한 아이디어, 노하우들이 여러분들이 실무에서 풀어내실 문제에도 도움이 되는 부분이 있었으면 합니다. 마지막으로 이상점 탐지 및 대치, 변화점 탐지에서 말씀드렸던 예제 R 코드를 공유드리면서 글을 마칩니다."
  },
  {
    "objectID": "posts/2023-04-15-monthly-memory-202303/index.html",
    "href": "posts/2023-04-15-monthly-memory-202303/index.html",
    "title": "월간 회고록: 3월",
    "section": "",
    "text": "The illustration by Mary Amato\n지난 월간 회고록들은 경어체(-입니다, -요)로 작성했는데, 이제부터는 평어체로 무심한듯?.. 편하게 작성해보려고 한다. 지난 한달 간 있었던 일들을 스스로 회고해보는 글이라 그런지 평어체가 확실히 글이 잘 적어지는 듯 하다. 사실 다른 기술적인 주제의 글들도 이제는 그냥 평어체로 작성해볼까 생각 중이다. 기업의 기술 블로그나 어디에 기고하는 글도 어니고, 그냥 개인적으로 운영하는 블로그라 뭔가 부담없이 빠르게 글을 쓰기에는 평어체 + (구어체, 문어체 혼용) 조합이 최고인 듯 하다. 말하는 걸 보니 이미 어느정도 결론을 지어놓은 듯 하다.😂 그럼 이제 본론으로 들어가서 지난 3월을 돌아보자."
  },
  {
    "objectID": "posts/2023-04-15-monthly-memory-202303/index.html#일",
    "href": "posts/2023-04-15-monthly-memory-202303/index.html#일",
    "title": "월간 회고록: 3월",
    "section": "일",
    "text": "일\n\n사내 뉴스룸 인터뷰\n3월 23일 Corporate Comms 팀으로부터 사내 뉴스룸 인터뷰 요청을 받았다. 블로그에 글을 포스팅하고 난 뒤에 링크드인에 홍보를 하곤 했는데, 거기서 내 글을 봐주시고 개인 블로그에 쓴 글들을 좋게 봐주신 듯 했다. 아직 저연차이기 때문에 다른 분들께 도움이 될만한 이야기를 할 수 있을까 싶어 인터뷰에 응해도 될지 망설여졌다. 뭐, 결론적으로는 팀원분들께 의견을 조금 여쭤보고 용기를 얻어 인터뷰에 응하기로 했지만.😂\n며칠 뒤 무려 11개의 질문이 담긴 인터뷰 질문지를 받았다. 질문들을 쭉 한번 읽어보자마자 답변에 꽤 많은 시간이 걸리겠음을 직감했다. 정답이 없는 꽤 어려운 질문들도 있었고, 내 과거를 돌아봐야하는 질문들도 있었다. 답변 초고, 퇴고 후 교정까지 대략 6-7시간 정도를 쓴 듯 하다. 퇴고 및 교정에 너무 집중하다보니 의도치 않은 야근을 하게되기도 했고. 뭐 이정도는 내가 원해서 응한 인터뷰여서 의도치 않은 야근이었지만 기분 좋은?..야근이었다. 내 블로그도 아닌 회사 뉴스룸에 남는 글이 될 것이기 때문에 답변에 더더욱 신경쓸 필요가 있었고.\n아무튼, 이번 뉴스룸 인터뷰는 내게 좋은 경험이었다:\n\n왜 좋은 경험이었다고 생각하고 있는가?\n\n뉴스룸에 들어가는 인터뷰의 특성 상 우리 업무를 잘 모르는 사람들이 대상 독자일 수 있다. 그래서, 최대한 알기 쉽게 설명하려고 노력했다. Data Analytics/Science 쪽에 사전 지식이 없는 사람들을 대상으로 우리 팀에 대해, 그리고 내가 맡은 업무에 대해 쉽게 설명하는 연습을 해볼 수 있었다.\n학창 시절 데이터 분석 업무에 관한 비전, G마켓에서 입사하기 전에 했던 업무, G마켓 AI Product 팀에서 근무를 시작하게 된 계기 등과 같은 질문에 답변을 하며, 과거를 깊게 회고해볼 수 있었다. 갈림길에 선 순간들에서 왜 그런 결정을 했는지에 대해 돌아보는 것은 언제나 즐겁다. 이러한 질문들에 답변하는게 누군가는 따분한 일일 수도 있는데, 나는 왜 즐거운지에 대해 생각을 해봤다. 첫째는 고심 끝에 선택한 것들이 스택된 결과인 현재의 내 상황이 그렇게 썩.. 나쁘진 않다고 생각하고 있어서 일 것이고, 둘째는 그러한 최종 결정들에 개인적으로 납득이 가는 합당한 이유가 있었기 때문일 것이다.\n나와 같은 저연차에 사내에서 이런 인터뷰 기회 갖는 경우는 많지 않다고 보는데, 거기다 내 블로그를 좋게 봐주셔서 인터뷰를 요청하셨다고 하니 더 기분이 좋았다. 인터뷰지에 받은 질문들이 어렵기도 했고 꽤 구체적이었는데, 아마 그 이유가 내 블로그에 회고 글이나 칼럼들을 쭉 훎어봐 주셔서 그런 것 같다. 사진 촬영하는 날 담당자 분과 이런저런 이야기도 나누었는데, 내 블로그에 글들이 질문지를 만드실 때 많은 도움이 되셨다고 했다. 아무래도 잘 모르는 분야에 있는 실무자에게 그 사람의 배경과 해당 분야의 사전 지식에 대한 정보가 없는 채로 여러가지 질문을 던져야하니, 주기적으로 인터뷰이(interviewee)를 선정하고 인터뷰어(interviewer)로서 좋은 질문지를 만드는 일도 정말 만만치 않은 일이라는 생각이 들었다.\n\n\n결론적으로 뉴스룸에 최종적으로 올라간 질문은 9개이다. 빠진 질답은 하루 일과, 업무 루틴에 관한 것과 내 개인적 목표에 관한 부분인데, 내가 다른 질문들에 꽤 답변을 길게해서 충분히 빠질만 했던 것 같다.😅\n▶️ 인터뷰 보러가기"
  },
  {
    "objectID": "posts/2023-04-15-monthly-memory-202303/index.html#개인",
    "href": "posts/2023-04-15-monthly-memory-202303/index.html#개인",
    "title": "월간 회고록: 3월",
    "section": "개인",
    "text": "개인\n\nPAP 퍼블리셔 3기\n지난 3월부터 PAP - Product Analytics Playground 3기 퍼블리셔로 활동하게 됐다. 지난 1월 회고록에서 밝혔듯이, 글또 8기로도 활동하고 있는데 PAP도 전부터 눈여겨 보던 커뮤니티라 퍼블리서 모집 공고가 나오자마자 지원해봐야겠다는 생각을 했다. 개인적인 에너지에 부하가 걸리진 않을까 잠깐 생각도 해봤는데, 두 커뮤니티 모두 글을 써야하는 커뮤니티이니 크게 다를 것은 없을 거라 생각했다. 같은 업계 사람들과 이야기를 나눌 기회가 있다는 것은 덤이고.\n3월 7일 자정이 가까워 오는 시간에 퍼블리셔에 선정됐다는 메일을 받았는데, 또 그걸 못참고 메일을 보냈었다.😂\n\n\n\n\n\n꼭 붙었으면 하는 마음에 많이 급했나보다..😅 퍼블리셔 선정 논의가 다소 길어지셔서 메일 안내가 늦어지셨다고 한다. 누가 돈을 주는 것도 아닌데, 업무 외에 개인적으로 시간을 내서 이런 커뮤니티를 운영한다는게.. 정말 대단한 것 같다. PAP에 대해 조금 홍보하자면, 앞서 PAP도 글 쓰는 커뮤니티라고 했는데 그렇게 단순한 목적으로 운영되고 있는 커뮤니티는 아니다. 내가 이런 말을 하는 이유는 PAP의 비전을 읽어보면 쉽게 이해된다:\n\n데이터 드리븐 프로덕트 개발 문화와 의사결정 문화를 각자의 자리에서 이끄는 미래를 지향함\n\n데이터 기반 의사결정과 임팩트 증대에 수반되는 도구/기술/문화/조직 구조를 이야기하는 공간\n데이터 수집의 중요성, 데이터 기반 의사결정하는 것의 효용, 데이터를 바라볼 때 주의할 점들, 플랫폼을 통해 자동화할 수 있는 영역 등에 대해 직군과 관계없이 많은 사람들이 비슷한 단계에 있음\n\n\n합격 메일을 받고 다음날 초대받은 PAP 슬랙에 들어가 자기소개를 한 것으로 기억한다.\n\n\n\n\n\n자기소개 전체를 캡쳐하진 않았는데, 또 투머치토커 특성이 발동되서 주저리주저리 많이도 적었다. PAP 2기 퍼블리셔는 30분 정도였고, 3기는 더 적은 인원인 17명만 선발이 되었다. 올라온 자기소개들을 쭉 보는데 정말 대단한 분들이 많았다. 그 중에는 링크드인에서 종종 봤던 분들도 계셨고. 어떤 글을 기고해야할지.. 급 부담이 됐다. (내가 생각하기에) 대단한 분들이 퍼블리셔에 포진되어 있는 것도 그렇고, 6개월간 활동하며 2주마다 글을 써야하는 글또와 달리 PAP는 2달 간 활동하며 3개의 글을 기고해야하기 때문에 뭐랄까.. 글의 퀄리티가 굉장히 중요하겠다는 생각을 했다. 거기다 글또는 내 개인 블로그에 기고하고 링크를 제출하지만, PAP는 PAP 블로그에 다가 직접적으로 기고를 해야하기 때문에. 결코 PAP 블로그의 퀄리티에 누가 되는 글을 작성할 수 없었다.😂 사실 인과추론에 대한 글을 적고 싶었는데, 아직 PAP에 기고할 수준이 확실히 아닌 걸 알기에 익숙한 주제로 글을 써야겠다고 마음을 먹었다.😭 그렇게 첫 번째 글 주제는 시계열이되었다. 다음 글 주제는 아마 통계학과 관련한 글이 되지 않을까 싶다.\n▶️ PAP에 기고한 첫 번째 글 보러가기\n\n\n짤막한 생각\n문득 어떤 계기로 들었던 짤막한 생각들 중에 정리해두고 싶은 것들은 페이스북과 링크드인에 기록을 해두는 편이다. 지난달에는 실무에서 누군가를 리드해야 하는 레벨에 올라있는 사람들이 가지고 있는 공통점을 어느정도 파악하게되어, 이 부분에 대해 정리를 해봤었다:\n\n나도 장기적인 관점에서 누군가를 리드하고 그들의 성장을 도울 수 있는 사람이 되고 싶다는 생각이 있는데, 이미 그 레벨에 올라가 있는 사람들이 가지고 있는 능력들을 하나하나 발견하게 될 때마다 자신감이 떨어진다. 해당 레벨에 올라기기 위해 갖추어야 하는 것들을 하나씩 적어보고, 매년 하나씩만 습득한다고 해보면.. 밝은 미래가 기다리고 있을 것이라고 생각해본다.😂"
  },
  {
    "objectID": "posts/2023-04-15-monthly-memory-202303/index.html#맺음말",
    "href": "posts/2023-04-15-monthly-memory-202303/index.html#맺음말",
    "title": "월간 회고록: 3월",
    "section": "맺음말",
    "text": "맺음말\n3월은 특별히 회고할만한게 많지 않아서 금방 적겠지 하고 적었는데 어느덧 2시간이 지났다. 역시 글을 쓰기 시작하기 전까지 소요되는 시간(e.g. 글감 생각하기, 책상에 앉아서 Rstudio 키기 등..😂)이 가장 많다. 막상 글쓰기에 몰입하면 길어야 4-5시간이면 초고를 완성하기 때문이다.1 나와 같은 게으른 완벽 주의자들은 항상 기억해야 한다.\n\nDone is better than perfect."
  },
  {
    "objectID": "posts/2023-04-17-monthly-memory-202303/index.html",
    "href": "posts/2023-04-17-monthly-memory-202303/index.html",
    "title": "월간 회고록: 3월",
    "section": "",
    "text": "The illustration by Mary Amato\n지난 월간 회고록들은 경어체로 작성했는데, 이제부터는 평어체로 무심한듯?.. 편하게 작성해보려고 한다. 사실 다른 기술적인 주제의 글들도 이제는 그냥 평어체로 작성해볼까 한다. 회사 기술 블로그나 플랫폼에 기고하는 글도 아니고, 그냥 개인적으로 운영하는 블로그에 글을 쓰는데 굳이 경어체까지 쓸 필요가 있을까 싶다. 부담없이 빠르게 글을 쓰기에는 평어체 + (구어체, 문어체 혼용) 조합이 최고이기 때문에. 말하는 걸 보니 이미 어느정도 결론을 지어놓은 듯 하다.😂 그럼 이제 본론으로 들어가서 지난 3월을 돌아보자."
  },
  {
    "objectID": "posts/2023-04-17-monthly-memory-202303/index.html#일",
    "href": "posts/2023-04-17-monthly-memory-202303/index.html#일",
    "title": "월간 회고록: 3월",
    "section": "일",
    "text": "일\n\n사내 뉴스룸 인터뷰\n3월 23일 Corporate Comms 팀으로부터 사내 뉴스룸 인터뷰 요청을 받았다. 블로그에 글을 포스팅하고 난 뒤에 링크드인에 홍보를 하곤 했는데, 거기서 내 글을 봐주시고 개인 블로그에 쓴 글들을 좋게 봐주신 듯 했다. 아직 저연차이기 때문에 다른 분들께 도움이 될만한 이야기를 할 수 있을까 싶어 인터뷰에 응해도 될지 망설여졌다. 뭐, 결론적으로는 팀원분들께 의견을 조금 여쭤보고 용기를 얻어 인터뷰에 응하기로 했지만.😂\n며칠 뒤 무려 11개의 질문이 담긴 인터뷰 질문지를 받았다. 질문들을 쭉 한번 읽어보자마자 답변에 꽤 많은 시간이 걸리겠음을 직감했다. 정답이 없는 꽤 어려운 질문들도 있었고, 내 과거를 돌아봐야하는 질문들도 있었다. 답변 초고 작성, 퇴고 및 교정까지 대략 6-7시간 정도를 쓴 듯 하다. 퇴고 및 교정에 너무 집중하다보니 의도치 않은 야근을 하게되기도 했고. 뭐 이정도는 내가 원해서 응한 인터뷰여서 의도치 않은 야근이었지만 기분 좋은?..야근이었다. 그리고, 내 개인 블로그도 아닌 회사 뉴스룸에 남는 글이 될 것이기 때문에 답변에 더더욱 신경쓸 필요가 있었다.\n아무튼, 이번 뉴스룸 인터뷰는 내게 좋은 경험이었다:\n\n왜 좋은 경험이었다고 생각하고 있는가?\n\n뉴스룸에 들어가는 인터뷰의 특성 상 우리 업무를 잘 모르는 사람들이 대상 독자일 수 있다. 그래서, 최대한 알기 쉽게 설명하려고 노력했다. Data Analytics/Science 쪽에 사전 지식이 없는 사람들을 대상으로 우리 팀에 대해, 그리고 내가 맡은 업무에 대해 쉽게 설명하는 연습을 해볼 수 있었다.\n학창 시절 데이터 분석 업무에 관한 비전, G마켓에서 입사하기 전에 했던 업무, G마켓 AI Product 팀에서 근무를 시작하게 된 계기 등과 같은 질문에 답변을 하며, 과거를 깊게 회고해볼 수 있었다. 갈림길에 선 순간들에서 왜 그런 결정을 했는지에 대해 돌아보는 것은 언제나 즐겁다. 이러한 질문들에 답변하는게 누군가는 따분한 일일 수도 있는데, 나는 왜 즐거운지에 대해 생각을 해봤다. 첫째는 인생의 갈림길에서 고심 끝에 선택한 것들이 스택된 결과인 현재의 내 상황이 그렇게 썩.. 나쁘진 않다고 생각하고 있어서 일 것이고, 둘째는 각각의 결정에 개인적으로 납득이 가는 합당한 이유가 있었기 때문일 것이다.\n나와 같은 저연차에 사내에서 이런 인터뷰 기회 갖는 경우는 많지 않다고 보는데, 거기다 내 블로그를 좋게 봐주셔서 인터뷰를 요청하셨다고 하니 더 기분이 좋았다.\n(번외) 인터뷰지에 받은 질문들이 어렵기도 했고 꽤 구체적이었는데, 아마 그 이유는 내 블로그의 회고 글이나 칼럼들을 쭉 훑어봐 주셔서 그런 것 같다. 사진 촬영하는 날 담당자 분과 이런저런 이야기도 나누었는데, 내 블로그에 글들이 질문지를 만드실 때 많은 도움이 되셨다고 했다. 아무래도 잘 모르는 분야에 있는 실무자에게 그 사람의 배경과 해당 분야의 사전 지식에 대한 정보가 없는 채로 여러 질문을 던져야하니, 주기적으로 인터뷰이(interviewee)를 선정하고 인터뷰어(interviewer)로서 좋은 질문지를 만드는 일도 정말 만만치 않은 일이라는 생각이 들었다.\n\n\n결론적으로 뉴스룸에 최종적으로 올라간 질문은 9개이다. 빠진 질답은 하루 일과, 업무 루틴에 관한 것과 내 개인적 목표에 관한 부분인데, 내가 다른 질문들에 꽤 답변을 길게해서 충분히 빠질만 했던 것 같다.😅\n▶️ 인터뷰 보러가기"
  },
  {
    "objectID": "posts/2023-04-17-monthly-memory-202303/index.html#개인",
    "href": "posts/2023-04-17-monthly-memory-202303/index.html#개인",
    "title": "월간 회고록: 3월",
    "section": "개인",
    "text": "개인\n\nPAP 퍼블리셔 3기\n지난 3월부터 PAP - Product Analytics Playground 3기 퍼블리셔로 활동하게 됐다. 1월 회고록에서 밝혔듯이, 글또 8기로도 활동하고 있는데 PAP도 전부터 눈여겨 보던 커뮤니티라 퍼블리셔 모집 공고가 나오자마자 지원해봐야겠다는 생각을 했다. 개인적인 에너지에 부하가 걸리진 않을까 잠깐 생각도 해봤는데, 두 커뮤니티 모두 글을 써야하는 커뮤니티이니 크게 다를 것은 없을 거라 생각했다. 같은 업계 사람들과 이야기를 나눌 기회가 있다는 것은 덤이고.\n3월 7일 자정이 가까워 오는 시간에 퍼블리셔에 선정됐다는 메일을 받았는데, 또 그걸 못참고 메일을 보냈었다.😂\n\n\n\n\n\n꼭 붙었으면 하는 마음에 많이 급했나보다..😅 퍼블리셔 선정 논의가 다소 길어지셔서 메일 안내가 늦어지셨다고 한다. 금전적인 이득이 없음에도 업무 외에 개인적으로 시간을 내서 이런 커뮤니티를 운영한다는게, 아무리 생각해도 정말 대단한 것 같다. 이런 커뮤니티는 널리 알려야 한다. 앞서 PAP도 글 쓰는 커뮤니티라고 했는데 그렇게 단순한 목적으로 운영되고 있는 커뮤니티는 아니다. 내가 이런 말을 하는 이유는 PAP의 비전을 읽어보면 쉽게 이해된다:\n\n데이터 드리븐 프로덕트 개발 문화와 의사결정 문화를 각자의 자리에서 이끄는 미래를 지향함\n\n데이터 기반 의사결정과 임팩트 증대에 수반되는 도구/기술/문화/조직 구조를 이야기하는 공간\n데이터 수집의 중요성, 데이터 기반 의사결정하는 것의 효용, 데이터를 바라볼 때 주의할 점들, 플랫폼을 통해 자동화할 수 있는 영역 등에 대해 직군과 관계없이 많은 사람들이 비슷한 단계에 있음\n\n\n아름다운 비전이다. 팀 워크샵에서 팀의 비전을 만드는 시간을 가져보며 조직의 비전을 설정하는 것이 얼마나 어려운 일인지 알게 되었기에, 이렇게 잘 만들어진 PAP의 비전을 보며 감탄할 수 밖에 없었다.\n합격 메일을 받고 다음날 초대받은 PAP 슬랙에 들어가 자기소개를 한 것으로 기억한다.\n\n\n\n\n\n자기소개 전체를 캡쳐하진 않았는데, 또 투머치토커 특성이 발동되서 주저리주저리 많이도 적었다. PAP 2기 퍼블리셔는 30분 정도였고, 3기는 더 적은 인원인 17명만 선발이 되었다. 올라온 자기소개들을 쭉 보는데 정말 대단한 분들이 많았다. 어떤 글을 기고해야할지 급 부담이 됐다. (내가 생각하기에) 대단한 분들이 퍼블리셔에 포진되어 있는 것도 그렇지만, 6개월간 활동하며 2주마다 글을 써야하는 글또와 달리 PAP는 2달 간 활동하며 3개의 글을 기고해야하기 때문이다. 뭐랄까.. 글의 퀄리티가 굉장히 중요하겠다는 생각을 했다. 거기다 글또는 내 개인 블로그에 기고하고 링크를 제출하지만 PAP는 PAP 블로그에 직접 기고해야하기 때문에, 결코 PAP 블로그의 퀄리티에 누가 되는 글을 작성할 수 없었다.😂 사실 인과추론에 대한 글을 적고 싶었는데, 아직 PAP에 기고할 수준이 확실히 아닌 걸 알기에 익숙한 주제로 글을 써야겠다고 마음을 먹었다.😭 그렇게 첫 번째 글 주제는 시계열이되었다. 다음 글 주제는 아마 통계학과 관련한 글이 되지 않을까 싶다.\n▶️ PAP에 기고한 첫 번째 글 보러가기\n\n\n짤막한 생각\n문득 어떤 계기로 하게된 짤막한 생각들 중에 정리해두고 싶은 것들은 페이스북과 링크드인에 기록을 해두는 편이다. 지난달에는 실무에서 다수의 인원을 리드해야 하는 레벨에 올라있는 사람들이 가지고 있는 공통점을 어느정도 파악하게되어 이 부분에 대해 정리를 했었다:\n\n나도 장기적인 관점에서 누군가를 리드하고 그들의 성장을 도울 수 있는 사람이 되고 싶다는 꿈이 있는데, 이미 그 레벨에 올라가 있는 사람들이 가지고 있는 공통적인 능력들을 하나씩 알아차릴 때마다 자신감이 떨어진다. 해당 레벨에 올라기기 위해 갖추어야 하는 것들을 하나씩 적어보고, 매년 하나씩만 습득한다고 해보면.. 밝은 미래가 기다리고 있을 것이라고 애써 생각해본다.😂"
  },
  {
    "objectID": "posts/2023-04-17-monthly-memory-202303/index.html#맺음말",
    "href": "posts/2023-04-17-monthly-memory-202303/index.html#맺음말",
    "title": "월간 회고록: 3월",
    "section": "맺음말",
    "text": "맺음말\n개인적인 풍파를 잠깐 겪었던 3월이지만, 특별히 회고할 것은 많지 않아서 금방 적겠지 했는데 어느덧 2시간이 지났다. 그럼에도 불구하고, 역시 글을 쓰기 시작하기 전까지 소요되는 시간(e.g. 글감 생각하기, 책상에 앉아서 Rstudio 키기 등..😂)이 가장 많다. 책상에 앉아서 Rstudio를 키고 글을 한자 쓰는 데까지 소요되는 시간은 정말 긴데, 그에 반해 막상 글쓰기에 몰입하면 아무리 길어도 4-5시간이면 초고를 완성하기 때문이다.1 나와 같은 게으른 완벽주의자들은 항상 기억해야 한다.\n\nDone is better than perfect."
  },
  {
    "objectID": "posts/2023-05-01-what-statistics-has-taught-me/index.html",
    "href": "posts/2023-05-01-what-statistics-has-taught-me/index.html",
    "title": "통계학, 그게 왜 중요한데?",
    "section": "",
    "text": "출처: https://youtu.be/TqM0oUJM2XM\nDA/DS1 도메인에 계시는 분이라면 어디선가 통계학의 중요성을 강조하는 영상이나 아티클을 한 번 쯤은 보셨을거라 생각합니다. 통계학을 전공했던 학생이라 이런 이야기를 들을 때마다 기분이 좋긴 했지만, 통계학이 중요하다고 말하는 구체적인 이유에 대해 항상 궁금해하곤 했습니다. 그리고, 후술하겠지만 학부생때 통계학은 쓰레기라고 주장하는 모 유명대학 교수님의 의견을 듣고는, 통계학이 DA/DS라는 도메인에서 갖는 가치는 무엇인지 대해 고민하기 시작했죠.\nData Science에서 통계학은 중요한 위치를 차지한다. 예나 지금이나 중론이 되는 문장입니다. 왜 중요할까요? 한 마디로 잘 요약된 직접적인 답을 드릴 순 없겠지만, 제가 업무를 수행하며 통계학으로부터 배웠던 것들이 중요하다고 느낀 포인트들에 대해 정리하며 간접적인 답을 제시해드리려고 합니다. 통계학 전공으로 실무에 있으신 분들께는 공감의 포인트가 되는, 통계학 공부의 필요성을 느꼈으나 공부를 주저하고 있는 분들께는 확신을 줄 수 있는 글이 되었으면 합니다."
  },
  {
    "objectID": "posts/2023-05-01-what-statistics-has-taught-me/index.html#통계학이-쓰레기라고",
    "href": "posts/2023-05-01-what-statistics-has-taught-me/index.html#통계학이-쓰레기라고",
    "title": "통계학, 그게 왜 중요한데?",
    "section": "통계학이 쓰레기라고?",
    "text": "통계학이 쓰레기라고?\n학부생 4학년 때니까 2018년이었던 것으로 기억합니다. 데이터산업진흥원에서 주관하는 <빅데이터 청년인재>라는 프로그램을 통해 모 대학에 종강 기간 동안 교육을 받고온 같은 과 친구가 제게 이런 말을 합니다.\n\n통계학은 쓰레기래.\n\n교육을 받았던 모 대학에서 소프트웨어 학과(a.k.a Computer Science) 교수님이 하신 말씀이라고 하더군요. 저 같은 경우는 원래부터 교수님 말이라고 100% 맹신하는 학생이 아니었기에 한 귀로 듣고 한 귀로 흘리긴 했지만, 우리나라에서 내로라하는 대학의 교수님이 했던 말이라 내심 신경이 쓰이긴 했습니다.\n먼저, 이 말이 근거가 있을까에 대해 어떻게 알아볼 수 있을지에 대해 생각해봤어요. 그 결과, 딥러닝을 연구하고 있는, 컴싸 영역에서 알아주는 거인의 어깨를 빌려봐야겠다는 생각을 했습니다. Ian Goodfellow가 쓴 Deep Learning이라는 책(Goodfellow, Bengio, and Courville 2016)을 구매했죠. 딥러닝 책이지만 코드 한 줄 없이 이론적 설명에 집중한 책이랍니다.😨\n\n\n\n\n\n이 책은 먼저 신경망 기반 알고리즘이 오랜기간 발전해 온 역사를 설명해주는 긴 Introduction을 한 뒤에, Chapter I에서 딥러닝을 이해하기 위해 갖추어야할 기본기들에 대해 정리해줍니다. Chapter I에서 선형대수, 확률 및 정보이론, 최적화, 머신러닝 기본2에 대해 차례대로 이야기해주는데, 이 부분이 무려 130여 페이지를 차지하죠. Introduction부터 Chapter I까지 공부하며 21세기에 딥러닝이 빛을 보기 시작한 이면에는 정말 많은 학문들의 융합이 필요했다는 것을 깨달았습니다. 다음은 Chapter I의 확률과 정보이론 섹션의 Introduction으로부터 일부 발췌한 내용입니다:\n\nIn artificial intelligence applications, we use probability theory in two major ways.\n\nFirst, the laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probablity theory.\nSecond, we can use probablity and statistics to theoretically anlayze the behavior of proposed AI systems.\n\nProbability theory is a fundamental tool of many disciplines of science and engineering.\n\n이정도면 앞서 “통계학은 쓰레기다.”라는 소프트웨어 학과 교수님의 의견에 우아한 반문이 될 거라 생각합니다. Ian Goodfellow가 그렇다는데 어쩌겠습니까? 이 맛에 거인의 어깨를 빌리는거죠.😂 아울러, 이 부분은 추후에 살펴본 바이지만 지금까지도 tabular data의 예측에 대장으로 자리잡고 있는 XGBoost 논문(Chen and Guestrin 2016)에서도 같은 논지의 의견을 얻을 수 있었습니다.\n\nThere are two important factors that drive these successful applications:\n\nusage of effective (statistical) models that capture the complex data dependencies\nscalable learning systems that learn the model of interest from large datasets."
  },
  {
    "objectID": "posts/2023-05-01-what-statistics-has-taught-me/index.html#통계학으로부터-배운-포인트들",
    "href": "posts/2023-05-01-what-statistics-has-taught-me/index.html#통계학으로부터-배운-포인트들",
    "title": "통계학, 그게 왜 중요한데?",
    "section": "통계학으로부터 배운 포인트들",
    "text": "통계학으로부터 배운 포인트들\n이제 업무를 수행하며 통계학을 배우길 잘했다는 생각이 들었던 포인트들에 대해 본격적으로 이야기 해보겠습니다. 몇 가지 소주제로 나눠 내용을 정리해봤습니다.\n\nEDA\n\n전체 집합의 결과와 부분 집합의 결과는 다를 수 있다.\n\n이는 심슨의 역설(Simpson’s Paradox)이라고 한다. 이해를 돕기 위해 신장결석 치료법에 관한 실제 의학 연구 사례를 하나 가져와 봤다.\n\n\n\n출처: 위키백과\n\n\n전체 그룹의 결과를 보면 마치 치료법 B가 더 좋아보인다. 그러나, 결석 크기를 given 시켜보면 결과는 반대로 나온다. 작은 결석과 큰 결석 두 경우 모두 치료법 A가 더 뛰어난 성공률을 보인다. 이러한 부분은 EDA를 할 때 뿐만이 아닌, 집계된 숫자를 볼 때에도 항상 유념해야한다.\n\n\n\n출처: 위키백과\n\n\n위 그림은 이러한 현상을 조심해야 하는 이유를 직관적으로 말해준다.\n\n\\(X\\)와 \\(Y\\)의 상관(correlation)은 \\(Z\\)가 given될 경우 다를 수 있다.\n단변량(univariate), 이변량(bivariate)으로 바라본 분석 결과는 다변량으로 바라볼 경우 결과가 달라 질 수 있다. 이러한 이유에서 EDA 후의 데이터에 관한 정교한 검증에는 결국 모델링이 필요로 된다.\n\n\n통계학의 이런 부분들로부터 도움을 받았습니다:\n\nSimpson’s Paradox\nConditional Probability\nConditional Distribution\n\n\n\n\n실험설계, 결과 검정 및 해석\n\n순수한 처리 효과(treatment effect)를 볼 수 있도록, 동시에 실험 비용을 최소화할 수 있도록 실험(Randomized Clinical Trials3, Online Controlled Experiments4)을 설계해야 한다.\n설계한 실험 디자인에 따라 적절한 분석 방법론5을 활용해 실험 결과를 검정해야한다.\n\n단, 실험 설계의 배경 및 목적에 따라 분석 방법론은 조금씩 달라 질 수 있다.\n확보할 수 있는 Sample size가 매우 작다면 비모수적 검정 방법론을 사용하는게 좋다. 각 모수적 검정에 대응하는 비모수적 검정이 있다는 것을 참고하기 바란다.(e.g. Independent two-sample t-test -> Wilcoxon rank sum text)\n\n이미 매출의 규모가 큰 충분히 성장한 서비스일수록 관측된 효과 크기(effect size)가 작아도, 이를 통계적으로 유의한 차이를 보이는 결과라고 결론을 내릴 수 있도록 충분히 큰 Sample size를 확보할 필요가 있다.\n\n가령, 매출이 1조인 기업의 매출 1%를 개선하면 100억이 추가 리턴 되지만, 매출이 100억인 기업의 매출 1%를 개선하면 1억이 리턴된다. 같은 1%이지만 갖는 의미는 다르다. 전자와 후자에서 가져다주는 매출 개선의 양에는 큰 차이가 있으며, 후자에서 1% 개선에 비해 전자에서 1% 개선은 훨씬 어려울 것이다. 이에 따라 전자와 같은 상황에서는 1%의 차이라도 유의한 개선이라고 결론을 내릴 수 있는 실험을 설계할 줄 알아야 한다.\nCVR을 생각해볼 수도 있다. buying conversion의 단위가 1만일 때와 1천일 때에 따라 CVR 1% 개선이 갖는 의미는 다르다. buying conversion의 단위를 1만, 객단가를 10만원으로 가정했을 때, buying conversion count의 1% 개선은 100번의 추가 구매가 리턴되어 약 1,000만원의 추가 매출을 가져다 주지만, buying conversion count의 단위가 1천일 때에는 10번의 추가 구매가 리턴되어 약 100만원의 추가 매출밖에 가져다주지 못한다. 앞선 예와 마찬가지로 CVR이 1% 개선되는 것은 같지만 우리에게 가져다주는 매출 변화의 양은 다르다.\n\n중심극한정리6에 관한 올바른 이해\n\n중심극한정리는 기본적으로 “표본평균”의 “분포”에 관한 정리이다. 중심극한정리에 의해 우리는 Sample size가 충분히 크면 모집단의 형태에 관계없이 표본평균의 분포는 정규분포로 근사함을 보일 수 있다.\n수리통계학 관점에서 얘기하면 표본평균의 분포 수렴(convergence in distribution)에 관한 이야기라 할 수 있다.\n더 우아하게 줄여보면 표본평균의 극한분포(limiting distribution)에 관한 이야기라 할 수 있다.\n이에따라, 정규분포를 따르는 표본평균에 기반한 통계량들로 진행되는 수많은 가설검정들은 중심극한정리에 기대어 편하게 진행할 수 있게 된다.\n\n예를 들어, 모분산(\\(\\sigma^2\\))에 관한 정보가 없는채로 모평균에 관한 검정을 진행할 때 사용하는 t-test를 생각해보자. 여기서는 t-분포를 따르는 t-통계량을 검정통계량(test statistics)으로 사용하게 되는데, 사실 t-통계량은 정규분포를 따르는 통계량을 적절히 조합하여 정의되는 통계량에 해당한다. 이에따라, 우리는 Sample size가 충분히 크기만 하면 별다른 정규성 검정없이 중심극한정리에 기대어 t-test를 수행할 수 있게 된다.\n\n참고로 중심극한정리는 “해당 그룹의 표본 크기(sample size)”에 영향을 받는 것이지, “샘플링 횟수”와는 무관하다.\n중심극한정리에 관한 자세한 고찰은 작년 초에 슬기로운통계생활 블로그에 기고했던 글로 갈음한다: ▶️ 글 보러가기\n\n통계적 가설검정의 원리와 p-value의 의미를 정확하게 파악하고 있어야 한다.\n\n통계적 가설검정은 귀무가설(null hypothersis, \\(H_0\\))이 참이라는 가정하에 진행된다. 설명의 편의상, 다음과 같이 모평균 \\(\\mu\\)에 관한 우단측 검정(대립가설 \\(H_1: \\mu > \\mu_0\\), e.g. G마켓 충성 고객들의 6개월 내 평균 구매 금액은 10만원을 상회한다)을 가정해보자.\n여기서, 모평균에 관한 검정이므로 검정통계량은 표본평균 \\(\\bar{X}\\)가 되고, 데이터로부터 관측된 표본평균 \\(\\bar{x}\\)를 관측된 검정통계량으로 이용하게 된다. 실험에 참가한 각 그룹의 Sample size를 충분히 크게(\\(\\ge30\\)) 확보했다고 하면, 중심극한정리에 의해 검정통계량의 분포는 정규분포를 근사한다고 가정할 수 있다.\n이에 따라 귀무가설이 참이라는 하에 검정통계량의 분포는 다음과 같이 정의된다:\n\\[\n\\bar{X} \\overset{under H_0}{\\widetilde{d}} N(\\mu_0, \\frac{\\sigma^2}{n})\n\\]\n\n본 예제에서 모분산에 관한 정보는 알려져있다고 가정하자.\n참고로 가설을 쓸 때 귀무가설에 =을 붙이나, 대립가설에 =을 붙이나 헷갈려 하는 사람들이 많다. 우리는 귀무가설이 참이라는 가정하에 검정통계량 분포를 구하므로, 당연히 귀무가설에 =를 붙이는 것이 맞다.\n\n검정통계량의 귀무가설 하 분포가 정의됐으므로, 이제 데이터로부터 관측된 검정통계량인 표본평균 \\(\\bar{x}\\)를 통해 이 결과가 통계적으로 충분히 유의하다고 할만한 결과인지 검정을 할 수 있다.\n관측된 검정통계량을 검정의 편의를 위해 표준화를 시키면 우리가 알고 있는 모평균 검정 시의 검정통계량 \\(Z\\)가 나오게 된다.\n\\[\nZ = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} \\overset{under H_0}{\\widetilde{d}} N(0, 1)\n\\]\n이때 관측된 검정통계량의 값을 극단적으로 4라고 가정해보자. 그럼, p-value는 검정통계량 값보다 더 극단값이 나올 확률로 정의되므로 다음과 같이 정의할 수 있다. 연속형 자료이므로 =을 붙이든 말든 상관없다.\n\\[\n\\textrm{Pr}\\left ( Z > 4 \\;| H_0 \\textrm{ is true} \\right ) < 0.01\n\\]\n그림으로는 다음과 같이 표현 될 것이다.\n\n\n\n출처: https://sciencebasedmedicine.org/tag/p-value/\n\n\np-value가 1% 미만에 해당하는 매우 작은 값이 나왔다. 실험에서 관측한 데이터로부터 두 그룹 간의 모평균에 차이가 있다는 귀무가설을 기각할만한 충분한 증거를 얻었다고 할 수 있다. p-value가 수학적으로 위 식처럼 정의되는 것은 알겠는데, 왜 우리는 그 값이 충분히 작아야 귀무가설을 기각하는 결정을 할 수 있을까?\n\n앞서 말했듯이 통계적 가설검정은 검정통계량의 분포를 귀무가설이 참이라는 가정하에 진행되며, 이에 따라 p-value란 만일 귀무가설이 참이라고 했을 때 관측된 검정통계량 보다 큰 값이 관측된 확률로 정의된다.\n여기서, 만약 관측된 검정통계량으로부터 계산한 p-value가 매우 작다면, 관측된 데이터가 귀무가설이 참이라고 가정한 분포로부터 나왔을(관측됐을, 샘플링 됐을) 확률은 매우 작음을 뜻한다.\n즉, 우리가 관측한 데이터를 바탕으로 귀무가설을 기각하는 결정을 하게 되도 그 주장이 잘못됐을 가능성은 매우 작다는 이야기와도 같다. 이에 따라 우리는 p-value가 충분히 작으면 귀무가설을 기각하는 결정을 할 수 있게 된다.\n통계적 가설검정의 원리에 관하여 과거 티스토리 블로그에 글을 작성했던 적이 있다. 혹시 설명이 부족했다면 참고하기 바란다: ▶️ 글 보러가기\n\n\n여러가지의 가설을 동시에(simultaneously) 검정하게 될 경우 p-value에 관한 조정이 필요로 된다.\n\n여기서 “동시에”의 포인트는 하나의 처리에 대해 여러가지 반응값에 대한 가설 검정을 수행할 때를 말한다. 이는 통계학에서 이야기하는 다중검정(multiple testing)의 상황인데, 단일 검정을 수행할 때와 같은 방식으로 검정을 진행하게 되면 심각한 문제가 발생한다.\n예를 들어, 2가지 추천 알고리즘(즉, 추천 알고리즘의 종류가 하나의 처리가 된다)에 대해 CTR, CVR 각각에 통계적으로 유의한 차이가 있는지 검정을 수행한다고 해보자. 이때 단일 가설 검정에서 하던 대로 두 가설 모두 유의수준(significance level) 5%로 놓고 모비율 검정을 수행하는 상황을 가정해보면:\n\n귀무가설이 참일 때 귀무가설을 기각시키지 않을 올바른 결정을 내릴 확률은 \\((1-0.05) = 95\\%\\)가 되며, 이에 따라 두 가설 모두 “동시에” 올바르게 판단할 확률은 \\((1-0.05)^2 = 90.25\\%\\)가 된다. 이를 뒤집어서 말하면, 귀무가설이 참인데 귀무가설을 기각시키는 잘못된 판단을 내릴 확률7은\\((1-0.9025) = 9.75\\%\\) 씩이나 된다.\n\n즉, 유의수준 5%로 여러 개의 가설검정을 수행하면 1종 오류 \\(\\alpha\\) 값이 매우 커지는 문제가 발생한다. 이에 따라, 다수의 검정을 동시에 수행할 경우 전체오류율 \\(\\alpha\\)를 유지한채로 가설 검정을 수행할 필요가 있다. 이러한 방법론을 통칭하여 FWER(family-wise error rate)을 조절하는 방법이라고 하며, 대표적으로는 본페로니 교정(bonferroni correction)을 예로 들 수 있다.\n단, 가설의 수가 매우 많아질 경우에 FWER를 조절하더라도 지나치게 보수적인 검정을 수행하게 되는 문제가 있어, FDR(false discovery rate)을 조절하는 방법론이 필요로 된다. 여기서는 p-value에 기반한 새로운 값인 q-value 라는 것이 등장하게 된다.\n\n\n\n통계학의 이런 부분들로부터 도움을 받았습니다:\n\nDesign of Experiment\nMathematical Statistics\n\nRandom variables\nSampling Distribution and Limiting Distribution\nConvergence in Distribution\nCentral Limit Theorem\nStatistical Hypothesis Test\nPower Function\nPower Analysis\nNonparametric Method\n\nMultiple Testing - FWER, FDR\n\n\n\n\n회귀적 모델링\n\n\\(Y\\)가 연속형 변수인 일반적인 선형회귀분석에서는 \\(Y\\) 자체의 분포가 아닌 \\(Y|X\\) ( \\(Y\\) given \\(X\\))의 분포가 정규분포를 따라야 한다.\n\n그래서, 보통 회귀선을 적합하고 남은 잔차(residuals)가 정규분포를 따르는지 잔차분석을 통해 체크한다. 이러한 이유에서 잔차분석은 “회귀분석의 꽃”이라고 불리기도 한다. 실제 문제에서 회귀계수에 관한 추정이 중요한 분석 목적 중 하나 일때는 꼭 잔차분석을 디테일하게 수행해야해야만 한다. 사실, 실제 문제에서 이러한 분석 목적을 갖고 수행하는 잔차분석은 매우 지루하고 어려운 작업이다.\n독립변수(\\(X\\))들이 정규성을 따르는지 확인하는 행위는 불필요하다. 풀고자 하는 문제에 선형회귀모형이 필요로 되고 모형 적합에 큰 문제가 없는 데이터라면, 바로 모형을 적합한 뒤에 이를 Baseline model로 잡고 잔차분석을 수행하며 모형을 개선해나가면 된다.\n\n“예측”이 목적이라면, 회귀모형에서 굳이 다중공선성(multicolinearlity)을 체킹할 필요는 없다.\n\n먼저 다중공선성이란, 독립변수들 간의 강한 상관으로 회귀계수 추정량의 표준오차가 크게 발산하여 회귀계수 추정량을 신뢰할 수 없는 문제를 말한다.\n\n참고로 다중공선성은 각 독립변수를 나머지 독립변수들의 선형 결합으로 회귀를 수행하여 결정계수 \\(R^2\\)를 통해 분산팽창요인(VIF)을 계산하여 체킹할 수 있다.\n\n통계학과 학부생들은 회귀계수의 추정량을 잘 추정해내기 위한 관점을 바탕으로 선형회귀분석을 배우기 때문에 다중공선성이라는 단어에 매우 민감하다. 회귀계수의 추정에 큰 문제를 일으키는 놈이라고 들었기 때문이다. 그래서, 이 문제를 예측 모델링에도 가져오게 된다.\n회귀계수 추정에 관여하는 \\(X\\)의 design matrix8에서 다중공선성의 존재가 그대로 모델에 반영이 되기 때문에, 다중공선성이 있는 상황에도 예측값 \\(\\hat{Y}\\)의 추정 자체에는 별다른 문제를 일으키지 않는다. 다만, 회귀계수 각각의 추정값에 대해서는 큰 문제를 일으킨다. 이러한 이유에서 Y와 X간의 relationship에 관한 평가, X가 Y에 미치는 영향 등에 관한 추론적 목적을 갖는 상황에서 보통 필요로 되는 회귀모형에서 다중공선성의 존재는 매우 치명적인 것이다.\n\n사실 실제 데이터를 바탕으로 좋은 회귀모형을 적합하는 일은 매우 어렵다. 잔차분석이 회귀분석의 꽃이라 불리우는 이유도 여기에 있다.\n\n그러나, 앞서 말했듯이 예측이 주 목적인 상황에 회귀모형을 가져다 쓰며 다중공선성에 매몰될 필요는 없다. 그리고, 개인적으로는 예측이 분석 목적인 상황에서는 딱히 회귀모형을 가져다 쓸 필요성을 느끼지 못한다. \\(n \\rightarrow p\\) , \\(n < p\\) 와 같은 고차원 자료에 대한 모델링이 필요한 경우를 제외하면 말이다.\n\n고차원 자료분석의 공부에는 The University of IOWA의 Patrick Breheny 교수님의 자료를 추천한다: ▶️ 자료 보러가기\n\n가설검정에서 발생하는 고차원문제, 고전적인 회귀분석의 문제점, 고차원 자료에 관한 예측을 수행할 때 나타나는 문제들을 해결하는 방법들에 대해 배울 수 있다. 앞서 이야기한 다중공선성이 존재하는 상황에도 예측값 \\(\\hat{Y}\\) 자체의 추정에는 별다른 문제를 일으키지 않는다는 부분도 해당 강의 자료 중 고전적인 회귀분석이 갖는 문제점을 통해 이해할 수 있었다. 아울러, 범주형 변수(Categorical variables)를 더미변수화 하여 모델에 반영하는 것 외에, 해당 변수가 갖는 본질적인 특성을 반영하여 모델링에 사용할 수 있는 방법에 대해서도 배울 수 있다.\n고차원 자료의 예측 모델링은 Penalty term을 개선해나가며 정의되는 다양한 형태의 Penalized regression을 통해 이루어진다. 이는 연속형 자료뿐만이 아닌 Binary outcome에 사용되는 logistic regression, Survival outcome에 사용되는 Cox regression으로도 자연스러운 확장이 가능하다.\nPenalty term이 고도화됨에 따라 고차원 자료의 예측에 사용되는 모형은 갈수록 경량화되고 예측 성능은 이전과 비슷한 수준 내지는 조금 더 뛰어난 퍼포먼스를 보이게 된다.\n\n\n(번외) Computer Science나 산업공학과 계열에서 지도학습 기법의 모델들에 관한 공부를 시작할 때 맨처음 접하게 되는 모델 또한 선형회귀모형이다. 그러나, 이 쪽에서는 선형회귀모형을 완벽히 예측 모델링의 관점에서 설명한다. 그래서, 다중공선성, 잔차분석에 관한 것들에 대한 것은 일체 언급되지 않는다. 여기서 선형회귀모형을 소개한 뒤에 다음으로 소개되는 것은 보통 Penalized regression(e.g. Lasso, Ridge)인데, 이쪽 관점에서 Penalized regression의 모티베이션은 regularization9에 해당한다. 추론(회귀계수의 점 추정, 구간 추정)을 중시하는 관점에서 회귀모형의 이론을 전개하는 통계학에서 Penalized regression의 모티베이션은 다중공선성에 해당한다.\n\n(번외) 확률분포에 기반한 통계적 모델과 머신러닝을 두부자르듯이 둘로 나눠보기란 매우 어렵다. 서로의 부족함을 채워주는 관계로 보는 것이 더 적절해 보인다.\n\n그 자체로 필요없는 행위이긴 하나, 종종 통계학을 전공하여 전통적인 회귀분석을 시작으로 확률분포를 가정한 모델들에 대해 공부를 하다가 Statistical Learning이라는 개념을 접하고 머신러닝 관점에서 자주 설명하곤 하는 모델들을 공부하기 시작하면 둘을 완벽히 나눌 순 없을까에 관한 생각을 흔히 하곤 한다.\n앞서 이야기 했듯이 회귀분석이라는 개념은 전통적인 통계학의 영역에 속하지만, 머신러닝의 관점에서도 설명이 가능하다. 이처럼 문제를 바라보는 관점에 따라 각 모델은 통계학 관점에서 설명 될수도, 머신러닝 관점에서 설명될 수도 있다. 굳이 따져보자면, 전통적인 통계학은 추론(inference)에 머신러닝은 예측(prediction)에 가치를 더 두고 있는 듯 하다.10\n궁극적으로 내가 하고 싶은 말은 개인적으로 통계학과 머신러닝의 차이는 그저 문제를 바라보는 관점의 차이라고 생각하고, 자꾸 둘을 나눠서 정의하고 공부하려는 습관은 오히려 공부의 확장성과 이해에 방해가 될 수 있다는 것이다.\n\n\n\n통계학의 이런 부분들로부터 도움을 받았습니다:\n\nRegression Analysis\n\nRegression Analysis by Example (명작)\n\nHigh-Dimensional Data Analysis\nStatistical Learning\n\nIntroduction to Statistical Learning (명작 I)\nThe Elements of Statistics Learning (명작 II. 난이도 높음)\n\n\n\n\n\n시간 순서로 관측된 자료의 모델링\n\n소인수 분해, 고윳값 분해를 통해 1보다 큰 자연수와 행렬이 갖는 고유한 특성을 파악할 수 있듯이, 시간 순서대로 관측된 시계열 자료 또한 추세-순환 요소(trend-cycle component), 계절 요소(seasonal component), 나머지 성분(remainder)으로 나누어 보면 해당 자료가 갖는 본질적인 특징을 파악할 수 있다.\n\n시계열 분해는 모델링 단계에서 큰 도움이 된다.\n\n추세-순환 요소, 계절 요소 외에 나머지 성분의 패턴이 랜덤하지 않고 일정한 패턴을 보인다면, 해당 자료의 모델링에는 남은 변동을 설명할 수 있을만한 다른 시계열 자료11가 필요로 됨을 뜻한다. 즉, 자기 자신의 과거 패턴만으로는 설명이 부족한 자료라 할 수 있다.\n시계열 자료의 관측 형태(e.g. Daily, Weekly, Monthly)에 따라 여러가지 윈도우의 계절성을 가질 수 있는데, 시계열 분해를 통해 해당 자료가 갖는 계절성에 관해 상세한 파악이 가능하다.\n\n\n시계열의 ACF, PACF까지 살펴보면 더 많은 도움이 된다.\n\nACF와 PACF는 기본적으로 시계열에 자기상관(Autocorrelation)이 있는지 체크해주는 통계량에 해당한다. 여기서, 자기상관은 시계열 자료가 갖는 잠재적인 서열적 상관성을 의미한다.\n예를 들어, 하루 전의 값이 다음 날의 값에 얼마나 영향을 미치는 지와 같이 말이다. ACF와 PACF를 통해 파악된 시계열의 성질에 따라 AR(Auto-regressive) term을 모델에 넣어주면 예측 퍼포먼스 개선에 도움이 될 수 있다.\n\n푸리에 급수(fourier series)는 긴 기간의 계절성을 갖는 자료의 계절 패턴에 관한 표현에 많은 도움이 된다.\n시간 영역에 관한 함수로 표현되는 시계열 자료를 주파수(frequency) 영역으로 바꿔서 나타내보면 시계열 자료가 갖는 또다른 본질적인 특성을 찾아낼 수도 있다.\n\n이와 관련한 이론으로 Fast Fourier Transfrom, Discrete Wavelet Transform 등이 있다.\n시계열 자료의 예측 모델링에서 일종의 Feature engineering 기법으로 종종 이용이 된다.\n\n시계열 자료의 예측은 기본적으로 단기 예측에서만 유효하다. 장기 예측을 위해서는 과거의 패턴이 미래에도 긴 시간 유지된다는 암묵적으로 강한 가정이 필요한데, 실제로 이 가정이 충족되기란 매우 어렵다.\n문제 상황에 따라 때로는 수십년간 쌓인 엄청난 양의 시계열 자료보다 더 적은 양의 최근 몇년 자료만을 모델 학습에 사용하는 것이 더 나은 퍼포먼스를 보일 수 있다.\nARCH, GARCH를 포함한 ARIMA 계열의 모델이 형태가 꽤 간단하여 만만해 보이나, Feature engineering에 큰 공을 들이지 않은 Neural Net 기반 모델보다 더 뛰어난 퍼포먼스를 보일때가 많다.\n시계열 자료에 관한 예측 모델링 이전에 본질적으로 예측 가능성이 존재하는 시계열 자료인지에 관한 고민이 선행되어야 한다.\n\n시계열의 예측 가능성을 판단하는 사항은 크게 4가지가 있다(Hyndman and Athanasopoulos 2021):\n\n시계열의 예측에 기여하는 요인들을 얼마나 잘 이해하고 있는가\n얼마나 많은 양의 데이터를 이용할 수 있는가\n미래는 과거와 얼마나 비슷한가\n예측이라는 행위 자체가 우리가 예측 하려는 것에 영향을 미치는가\n\n가령, 국내 주거용 전기 수요의 단기 예측은 꽤 정확하게 수행이 가능할 것이다. 위 4가지 사항을 만족시키는 자료이기 때문이다.\n\n전기 수요는 기온에 상당한 영향을 받으며, 휴일, 전반적인 경기에 약간의 영향을 받는다.\n수십년 간 쌓인 국내 전기 수요 자료와 기상 환경에 관한 자료를 이용할 수 있다.\n몇 주 후 수준의 단기 예측 에서는 전기 수요의 형태가 과거와 비슷하다는 가정이 가능하다.\n국내 전기 사용료는 수요에 민감하여 급박하게 변화하지 않는다. 즉, 전기 수요의 예측이 주거용 전기 사용 고객들의 행동에 영향을 미치진 않는다.\n\n반대로, 통화 환율은 예측이 불가능한 시계열 자료에 해당한다. 위 사항들 중 2번째 사항만 만족한다.\n\n환율에 영향을 미치는 요인들에 대한 이해에 한계가 있다. 예측할 수 없는 수많은 것들이 환율에 영향을 미칠 수 있다.\n미래 환율의 변화 형태가 과거와 비슷할 것이라 가정하는 것은 매우 어렵다. 경제 또는 정치에 중대한 위기가 발생할 경우 환율은 과거의 패턴과는 상당히 다르게 움직일 것이다.\n무엇보다 환율의 예측은 그 자체로 시장의 사람들에게 직접적 영향을 미친다. 예를 들어, 환율을 꽤 정확하게 예측할 수 있는 모형이 개발되었다고 가정했을 때, 환율이 오를 것이라 예측하면 시장은 즉시 그것에 반응할 것이다.12\n\n\n여러가지 시계열에 관한 모델링13에 앞서 허구적 회귀(spurious regression)에 관한 개념을 알아둘 필요가 있다.\n\n허구적 회귀란, 실제로는 서로 독립적인 아무 관련이 없는 비정상 시계열 변수 간에 선형회귀모형의 적합 결과가 통계적으로 유의하게 나타나는 경우를 말한다.\n서로 독립인 두 비정상(non-stationary) 시계열이 모두 강한 계절성과 추세를 갖는 경우 이들 간의 선형회귀 결과는 매우 유의하게 나타날 수 있다.\n이러한 결과는 아무런 의미를 갖지 못하고 이를 허구적이라 표현한다: ▶️ 허구적 상관에 관한 15가지 예제 확인하러 가기\n\n\n\n통계학의 이런 부분들로부터 도움을 받았습니다:\n\nTime Series Data Analysis\n\nTime Series Decomposition\nARIMA model (Including ARCH, GARCH)\nSpurious Regression\nSpectral Analysis (STFT, FFT, DWT)\nTime Series Forecasting\n\nFPP3\n\n\n\n\n\n\n알아두면 재밌는 것들\n\n통계학을 이해하는 데에 가장 중요한 첫 걸음은 확률변수(random variable)는 분포(distribution)을 갖는다는 점을 이해하는 것이다.\n\n통계량(statistics)은 확률변수의 선형결합으로 표현할 수 있고, 따라서 통계량 또한 분포를 갖는다.\n이 부분을 이해하면 통계학의 이론 전개를 위해 구분되어 표기되는 표본평균 \\(\\bar{X}\\)와 \\(\\bar{x}\\)가 어떻게 다르고, 왜 이렇게 표기가 되어야만 하는지 이해할 수 있을 것이다. 전자는 분포를 갖는 통계량을 의미하며, 후자는 표본으로부터 관측한 실현값으로 상수를 의미한다. 확률변수는 분포를 갖는다는 개념을 이해하고 있으면 이 개념을 받아들이기는 어렵지 않을 것이다. 이러한 사실 외에도 확률변수가 분포를 갖는다는 개념을 이해하면 통계학 전반에 대한 이해에 정말 많은 도움이 될 것이다.\n\n확률(Probability)과 가능도(Likelihood)는 다르다.\n\n확률은 모수(\\(\\theta\\), parameter)14가 주어졌을때 데이터(\\(X\\))가 관측될 가능성을 나타낸다.\n\\[\n{\\rm{Probability}} = Pr(X|\\theta)\n\\]\n가능도는 데이터(\\(X\\))가 주어졌을 때 특정 모수가 관측될 가능성으로, 모수에 관한 함수에 해당하며 기호 \\(L\\)로 나타낸다.\n\\[\n{\\rm{Likelihood}} = f(X|\\theta) = L(\\theta|X)\n\\]\n\n단, 여기서 주어진 데이터는 우리가 사전에 알고 있는 모수적 모형(e.g. 정규분포)으로부터 주어진 확률표본(random sample)15이여야 한다.\n이는 우리가 중심극한정리에 기대게 되는 또 하나의 이유가 된다.\n\n예제를 통해 확률과 가능도의 차이를 알아보자. 통계고등학교의 학생들의 키(X)는 평균이 \\(\\mu\\)이고 분산이 100인 정규분포16를 따른다고 하자. 여기서 확률과 가능도 각각은 다음과 같이 정의될 수 있다.\n\n확률 개념의 설명을 위해 통계고등학교 학생들의 키에 관한 모평균 \\(\\mu\\) 또한 170으로 알려져있다고 하자.\n\\[\n{\\rm{Probability}} = Pr(X >x|\\mu = 170, \\sigma^2 = 100)\n\\]\n\n즉, 위 식은 통계고등학교 학생들의 키의 평균이 170(모평균)이고 분산이 100(모분산)일 때 학생들의 키 값들이(데이터) 관측될 가능성을 나타낸 것에 해당하며, 이러한 예를 두고 우리는 “확률”이라고 표현한다.\n위 식에 따라 다음과 같이 통계고등학교 학생들의 키가 170보다 클 확률을 구할 수 있다:\n\\[\n{\\rm{Probability}} = Pr(X >170|\\mu = 170, \\sigma^2 = 100)\n\\]\n\n가능도 개념의 설명을 위해 통계고등학교의 전교생 중 30명을 확률표본으로 추출하여 관측한 상황이라고 해보자.\n\n앞서 우리에게 알려진 모집단의 정보는 모분산밖에 없는 상황인데, 통계고등학교 학생들의 평균키를 추정해야하는 상황이다.\n우리가 사전에 알고있는 모수적 모형(정규분포)를 따르는 확률표본 학생 30명의 키를 바탕으로 가능도함수를 이용해 우리가 관심있는 모수값에 관한 관측 가능성을 나타낼 수 있다.\n\\[\n{\\rm{Likelihood}} = L(\\mu|x_1, x_2, \\cdots, x_{30})\n\\]\n\n즉, 30명의 키를 관측했을 때(데이터가 주어졌을때) 우리가 관심있는 모수인 모평균(통계고등학교 학생들 키의 평균값)이 관측될 가능성을 나타낸 것에 해당하며, 우리는 이러한 예를 두고 “가능도”라고 표현한다.\n여기서 주어진 데이터가 관측될 확률을 최대로 해주는 모수를 추정할 수 있고, 우리는 이러한 추정 방법을 최대가능도추정(Maximum Likelihood Estimation)이라고 표현하며, 이렇게 추정된 추정량을 MLE라고 일컫는다.\n\n\n\n(번외) 베이지안은 앞서 살펴본 Likelihood와 사전확률(prior)의 곱에 비례하는 모수 \\(\\theta\\)의 사후분포(posterior)를 추정하는 영역이다. 이에 따라 베이지안의 모수 추정에는 훨씬 더 많은 계산량이 필요로 된다. 그러나, 상황에 따라 베이지안 추정이 Likelihood만으로 모수 추정을 수행하는 Frequentist 관점의 추정보다 계산량에 비례하여 충분히 더 나은 퍼포먼스와 효율을 보이지 않는 경우도 많다. 즉, Likelihood만을 이용한 추정보다 베이지안이 더 뛰어난 추정법이라 생각하여 무지성으로 모든 문제에 베이지안 추정을 활용하고자 하는 사고는 피하는 것이 좋다는 말이다. 도메인 지식 또는 사전 연구에 의한 Prior의 정의, 풀고자하는 문제의 Context에 따라 베이지안 관점의 추정 또한 적절히 맞춰 활용될 필요가 있다. 딥러닝을 바라보는 관점에 있어서 종종 등장하는 다음의 밈과 같은 맥락에 해당하는 이야기이다.\n\n\n좋은 추정량이 되기 위해서는 불편성(unbiasedness), 일치성(consistency), 유효성(efficieny)을 만족해야한다.\n\n추정량이라는 단어를 처음 드는 사람을 위해 잠깐 설명을 해보자면, 특정 통계량이 모수 \\(\\theta\\)의 추정에 사용 될 때 이를 추정량(estimator)라고 한다. 실무에서 흔히 추정하게 되는 두 모수 모평균 \\(\\mu\\), 모비율 \\(p\\) 의 추정에는 표본평균 \\(\\bar{X}\\), 표본비율 \\(\\hat{p}\\)을 추정량으로 사용한다. 예를 들어, 평균 매출과 같은 것들에 관한 추론은 모평균에 관한 추정, CTR, CVR과 같은 비율 지표들에 관한 추론은 모비율에 관한 추정을 나타낸다.\n일치성은 추정량으로써 반드시 가져야하는 성질로, 이 성질이 만족하지 않으면 통계적 성질을 갖기 힘들다. 즉, 우리가 모평균과 모비율의 추정에 사용하는 표본평균과 표본비율은 기본적으로 이 성질을 만족하는 추정량에 해당한다.\n불편성은 추정량의 기댓값 \\(E(\\hat{\\theta})\\)이 모수가 되어야함을 나타내며, 여기서 기댓값이란 분포의 중심 위치로 추정량17이 갖는 분포의 중심위치가 모수 \\(\\theta\\)와 동일함을 의미한다. 불편성을 만족하는 추정량이 항상 더 좋은 추정량은 아니며, 편의추정량(biased estimator)이라도 그 편의가 크지 않고 분산이 불편추정량에 비해 현저히 작다면 편의추정량이 해당 모수를 추정하는 더 좋은 추정량으로 볼 수도 있다.\n유효성은 절대적 정의가 아닌 상대적 정의로, 분산이 더 작은 추정량이 더 유효하다고 표현한다.\n\n우리가 \\(n\\)개의 확률표본을 가지고 있다고 할 때, 모수 \\(\\theta\\)에 관한 추정은 \\(n\\)개의 표본 대신 \\(\\sum X_i\\)이 가지고 있는 모수 \\(\\theta\\)의 정보만으로도 동일한 추론이 가능하다.\n\n충분통계량(sufficient statistics)에 관한 개념으로, 충분통계량은 \\(n\\)개의 확률표본이 가지고 있는 \\(\\theta\\)의 정보력과 동일한 정보력을 가진 통계량을 말한다. 정보력은 가지되 가능한 단순한 형태를 취하는 것이 바람직 할 것이다.\n별 것 아닌 사실처럼 보이지만, 내용을 곱씹어보면 모수에 관한 추정을 \\(n\\)개의 통계량까지 필요없이 통계량의 합의 형태만으로 동일한 추정을 할 수 있다는 놀라운 이야기다. 참고로 저 합을 확률표본의 수로 나누면 표본평균, 표본비율과 동일한 형태가 되는데, 이는 곧 표본평균과 표본비율이 갖는 정보량이 \\(n\\)개의 확률표본이 갖는 정보량과 같다는 놀라운 사실을 나타낸다. 아울러, 이 사실 또한 우리가 중심극한정리에 기대게 되는 또 하나의 이유가 된다. 이와 관련한 R.A. Fisher 경의 유명한 일화를 교수님으로부터 들었는데 잘 기억이 나질 않는다.😂\n\nMLE는 기본적으로 불변성(invariance property)18을 가지며, 표본만 많이 확보하면 앞서 소개한 추정량이 가질 수 있는 좋은 성질들을 모두 가질 수 있다.\n\n이는 다른 추정량은 갖지 못하는 MLE의 매우 특별한 성질이며, 동시에 ML 추정이 언제나 사랑받는 이유에 해당한다.\n최대가능도추정량의 극한분포를 살펴보면 Sample size \\(N\\)이 충분히 크면, MLE는 근사적으로 불편추정량이 되고 최소 분산을 갖게 됨을 확인할 수 있다. 즉, MLE는 \\(N\\)이 크기만 하면 UMVUE(Uniformly Minimum Variance Unbiased Estimator)가 된다는 말이다.\n\n\n\n통계학의 이런 부분들로부터 도움을 받았습니다:\n\nMathematical Statistics의 전반\n\n이처럼 데이터에 기초한 추정에서 우리가 당연하게 생각하고 사용하는 것들은 사실 수리통계학에서 엄밀하게 증명된 것들에 해당한다.\n즉, 수리통계학이라는 존재는 데이터에 기초한 가설의 설정과 그에 관한 추정과 검증을 매일같이 반복하는 우리에겐 너무나도 든든한 존재라는 것이다.\np.s. DA/DS라는 도메인의 모든 부분이 통계학으로서 설명된다는 취지로 하는 말은 결코 아니며, 알다시피 DA/DS에서 반복적으로 이루어지는 작업들 중 통계학 외에 선형대수, 정보 이론, 최적화 이론에 기초하고 있는 것들 또한 많다. 아울러, 현재 산업 전반에서 DA/DS가 차지하고 있는 인기와 위치에는 엔지니어링이라는 영역의 기여도 상당히 큰 부분을 차지하고 있다.\np.s. 혹시 통계학을 전공하고 있는 학부생임에도 수리통계학 공부가 하기 싫고 재미가 없다면, 이 부분에 흥미를 조금 돋궈줄만한 글을 과거 슬기로운 통계생활 블로그에 기고했던 적이 있다. 속는 셈치고 읽어보길 바란다: ▶️ 글 읽어보러 가기"
  },
  {
    "objectID": "posts/2023-05-01-what-statistics-has-taught-me/index.html#맺음말",
    "href": "posts/2023-05-01-what-statistics-has-taught-me/index.html#맺음말",
    "title": "통계학, 그게 왜 중요한데?",
    "section": "맺음말",
    "text": "맺음말\n통계학이라는 학문이 DA/DS에서 갖는 가치는 무엇이라고 생각하시나요? 저는 통계학이 Data Analytics, Data Science라는 영역을 꼰대스럽게 바라보게 해준다고 생각합니다. 좀 더 풀어서 말해보면, 우리는 매일같이 데이터를 기반으로 가설을 세우고 이를 검증하는 일을 하게 되는데, 통계학은 이때 내리게 되는 의사결정 하나하나를 이성적이고 냉정하게 내릴 수 있도록 도와주는 역할을 한다고 생각합니다. 데이터 전처리, 지표 설계 및 해석, 모델링, 추정량과 분석 결과에 관한 해석 등 가설 검증에서 하게되는 여러 모든 작업들에서 말이죠.\n그리고, 통계학 이야기와는 별개로 현업에서 일을 하면서 늘 이런 생각을 하곤 합니다. 실무에서 분석 배경 및 문제 상황에 관한 Context를 이해하고 주어진 데이터로 제 나름대로의 방법론을 설계하여 문제를 풀고 있으면, 과연 이게 최적의 방법인지에 대해서요. 나보다 한 수, 두 수 위에 사람도 과연 이렇게 문제를 풀 것인가에 대해서 말이죠. 이 부분에 대한 확신을 위해, 그리고 주어진 상황과 문제, 데이터에 맞게 최적의 결과를 뽑아내기 위해 우리는 끊임없이 공부를 하게 되는 것이 아닌가 생각해봅니다.\n오늘 쓴 글은 사실 제가 예전부터 써보고 싶은 주제의 글이었습니다. 그런데, 막상 본 주제로 글쓰기를 시작하니 너무 어렵고 부담이 되더군요. 그래서, 자연스럽게 방어적인 글쓰기를 하게 되어 자극적인 글 제목에 비해 글의 내용이 너무 싱겁지 않았을지 조금 걱정이 되기도 합니다.😂 여담이긴 하지만, PAP에 기고했던 지난 글은 초고부터 교정까지 5-6시간 정도면 됐는데, 이번 글은 Introduction에 관한 초고를 쓰고 교정하는 데에만 그정도 시간이 소요된 것 같습니다. “통계학이 중요한 이유”에 관해 이야기한 글이라기 보다는, 현업에 있으면서 소중하다고 느낀 통계학에서 배운 포인트들을 나열함으로써 “통계학을 배워두면 이렇게 쓸모가 있어요.” 정도를 이야기하는 글로 받아주셨으면 합니다.\n아직 제 수준에 건드려서는 안되는 주제로 글을 쓰고 있는게 아닌가 하는 생각이 들어, 주제를 바꿔볼까 생각했지만.. 이런 자극적인 주제로 학계, 산업계 다양한 곳에 계신 재야의 통계학 고수분들께 날카로운 피드백을 받는 것 또한 제게 많은 도움이 될 것 같아서 꾸역꾸역 글을 마무리했습니다. 여러분들이 생각하시는 통계학이라는 학문이 DA/DS에서 갖는 가치는 무엇이라고 생각하시나요?"
  },
  {
    "objectID": "posts/2023-05-15-monthly_memory-202304/index.html",
    "href": "posts/2023-05-15-monthly_memory-202304/index.html",
    "title": "월간 회고록: 4월",
    "section": "",
    "text": "The illustration by Mary Amato\n회고를 분기 별로 나눠서 하는게 나을지 고민 중이다. 굳이 회고할만한 굵직한 것들이 없는데 매달 회고를 할 필요가 있나 싶기도 해서. 그럼에도 불구하고, 이렇게 결국 월간 회고를 쓰고 있는 이유는 분기 마다 쓰면 너무 거창한 작업이 될까 싶기도 해서이다.😂 월간 회고의 목적은 오로지 지난 한 달을 가볍게 성찰하는 것에 오는 개인적인 재미와 만족감에 있기에."
  },
  {
    "objectID": "posts/2023-05-15-monthly_memory-202304/index.html#일",
    "href": "posts/2023-05-15-monthly_memory-202304/index.html#일",
    "title": "월간 회고록: 4월",
    "section": "일",
    "text": "일\n\nPython 관련 잡설\nR에 익숙하고 tidyverse를 좋아하는 사람에게 Python에서 pandas만을 활용해 Data wrangling을 수행하라는 것은 고문과 같다. pandas가 구리다는 것은 아니고, 나는 tidyverse의 pipe-friendly한 코드를 작성하는 것이 더 익숙하고 편하기 때문이다. 그래서, Python에서 tidyverse style을 구현한 라이브러리를 찾아볼 수 밖에 없었다. 내가 불편해하는 것은 누군가도 불편해했을 것이고, 이에따라 누군가가 구현을 해놓았을 것이기 때문에..😂 다행히 몇 가지 패키지를 발견 했고, 그 중 가장 완성도있고 최근까지 업데이트가 되고있는 라이브러리를 소개해본다.\n\ndatar: https://github.com/pwwang/datar\n\ndatar는 Python에서 여러 백엔드로 제공되는 data manipulation을 위한 API를 R의 tidyverse 패키지와 최대한 align되도록 재구상한 라이브러리에 해당한다. 사실 작년 4분기에 Python 기반 도구인 Streamlit으로 대시보드를 개발하며 siuba라는 라이브러리를 이용해 data wrangling을 수행했었는데, siuba의 경우 pipe-friendly한 코드를 짜기에는 부족한 기능들이 너무나 많았다. 이와 달리, datar은 tidyverse로 데이터를 만질때 종종 사용하는 R Base의 함수들까지 상당수 커버하고 있다. 당연히 R에서 tidyverse로 데이터를 가지고 노는 만큼은 편하지 않지만, 그래도 이정도면 아주 만족스러운 퀄리티다. 레퍼런스도 아주 쉽고 상세하게 제공하고 있다. 앞으로도 해당 패키지를 꾸준히 업데이트를 해나가는 데에 동기부여가 될 수 있도록, 이런 레포에는 꼭 스타를 눌러주자! tidyverse 문법을 선호하는 Python 사용자라면 한 번쯤 사용해보기 바란다.\n\n\n고객 관심사의 선제적 반영\n전에 말했듯 우리 팀은 요즘 모바일 홈 개인화에 집중을 하고 있는데, 이 부분에 있어서 다음과 같은 부분을 반영해보기 위해 시계열 자료분석을 수행했었다:\n\n당장 관심을 가질만한 상품을 띄워주는 것도 중요하지만, 추후 맞이할 특정 시즌에 구매 의사가 생길만한 상품글까지 고려하여 랭킹 시스템을 구성한다면 고객들의 쇼핑 만족도를 더욱 높여줄 수 있지 않을까?\n\n업무와 직접적으로 관련한 방법론들이기에 자세하게 A to Z까지 소개하긴 어렵지만, 해당 문제를 해결하기 위해 사용한 방법론들을 블로그에 소개하기도 했다. 관심있는 사람들은 참고해보기 바란다.\n▶️ 글 보러가기\n아무튼, 이 프로젝트를 4월 초에 잘 마무리하고 타 팀에 공유세션까지 잘 마칠 수 있었다. 그런데, 가만 생각해보니 나는 강한 가정을 내포한채로 이 문제를 해결한 것이었다:\n\n과거 1년의 추세는 올해에도 이어질 것이다.\n\n앞서 제시한 문제를 보면 알겠지만 결국 예측 문제다. 이 방법론을 좀 더 정교하게 개발하고 싶은 마음이 들었다. 사실, 현재는 1년치 시계열 자기자신의 추세, 계절성만으로 해당 문제를 해결하는 매우 간단하게 풀어내는 로직이다. 계산량을 고려해봐야겠지만 3년 내지 4년 규모의 시계열을 고려하여 yearly seasonality까지 캐치할 수 있도록 하고, 모형에 이전 시차의 기온, 습도, 강수량, 휴일 효과 등까지 반영하여 Monthly로 모형을 갱신하여 매달 예측을 수행해 나간다면, 매달 다가오는 시즌의 고객 관심사를 훨씬 더 정교하게 예측할 수 있을 것으로 생각된다.\n물론, 아직 이상적인 상상에 불과하긴하다.\n\n\n\n구상한 방법론을 적용해본 뒤, 실제로 맞닥뜨리는 흔한 상황\n\n\n짧은 시간에 끝날만한 수준의 모델링 작업은 아닌 것 같아서, 시도해볼만한 가치가 있는 일인지에 대해 팀 내에 이야기를 해봐야겠다.😀"
  },
  {
    "objectID": "posts/2023-05-15-monthly_memory-202304/index.html#개인",
    "href": "posts/2023-05-15-monthly_memory-202304/index.html#개인",
    "title": "월간 회고록: 4월",
    "section": "개인",
    "text": "개인\n\nPomodoro Out, Task In\n지난 연간 회고록에서 뽀모도로로 시간을 관리하고 있다고 했는데, 쓸수록 영.. 별로였다. 뽀모도로를 기반으로 업무와 공부를 하다보니 생기는 부작용은 다음과 같았다:\n\n집중력 하락: 기본적으로 25분 공부 5분 휴식인데, 간혹 회의가 길어져서 1뽀모도로(25분)가 넘어가면 이 뽀모도로로 공부한 시간을 정확히 측정하는 데에 강박이 생겨 일일이 휴식을 Skip하러 가곤 했다. 그리고, 진정한 집중은 “시간이 언제 이만큼 흘렀지?”를 느낄 때라고 할 수 있는데, 뽀모도로 타이머를 기반으로 움직이다보니 오히려 시계를 더 많이 보게 된다.\n한 주간 해결한 Task의 양과 질이 아닌 집중한 시간에 집착: 나는 월간 회고를 블로그에 올리고 있지만, 개인적으로 주간 회고도 하고 있다. 개인적으로 쌓아둔 주간 회고를 기반으로 월간 회고가 만들어진다고 보면 된다. 맥의 Focus To-Do 앱을 유료 결제하면 매주 뽀모도로를 얼마나 적립했는지 자세한 보고서를 확인할 수 있다. 이 부분이 오히려 내겐 독이 됐다. 돌아보니 올해 생각해 둔 목표의 달성을 위한 Task들을 이번 주에 얼마나 해결했냐를 평가하는 것이 아니라, 그저 집중한 시간이 얼마인지만 늘상 체크하고 있었다.\n\n물론, 늘 그렇듯 뽀모도로 라는 시간 관리법 자체가 구리다는 것이 아니라 그저 나에게는 조금 안맞는 방법이었다는 것이다.😀 사실 그 전부터 뽀모도로에 부작용을 느껴왔지만 선뜻 버리지 못한 근본적인 이유는 올해 설정해둔 목표를 관리하는 방식에 있었다. 나는 Notion에서 내 올해 목표, 오늘 할 일 등을 관리하고 있는데, 올해 목표를 설정만 해두고 해당 목표를 달성하기 위해 수행해야하는 Task들을 구체적으로 정의해놓진 않았다. 연간 목표를 세운게 처음이라 그런지 많이 서툴렀던 것 같다.\n이렇다보니 벌써 2023년의 2분기가 저물어가는데, 해당 목표에 내가 얼마만큼이나 다가갔는지에 대한 정량적인 평가가 불가능했다. 그리고, 이런 평가가 불가능하다보니 거대한 연간 목표의 달성을 위해 내가 열심히 달려가고 있다는 것을 느낄만한 성취감 또한 없었다. 이러한 고민을 안고있었기에 “시간” 중심으로라도 성취감을 느낄 수 있었던 뽀모도로를 버리긴 힘들었다. 그러다가, 이 영상을 마주했다.\n\n드림코딩이라는 채널에서 공유해주신 개발자가 사용하는 노션 노트인데, 올해 목표를 설정하고 해당 목표를 달성하기 위한 Task들을 정의하여 이를 중심으로 일정을 관리하고 싶었던 내 니즈에 완벽하게 부합하는 템플릿이였다. 또한, 위 영상을 보며 노션의 데이터베이스 활용법도 터득할 수 있었는데, 내가 여지껏 노션을 참 무식하게 써왔다는 것 또한 깨달을 수 있었다. 아무튼, 이 템플릿 덕분에 기존에 주먹구구 식으로 관리하던 To do list를 연초에 작성해뒀던 연간 목표와 align하고 각 목표 달성을 위한 Task들을 세부적으로 정의함으로써, Task 중심으로 일정을 관리할 수 있게됐다. 확실히 Task 중심의 관리가 목표에 점차 다가가고 있는 나를 가시적으로 볼 수 있있게 해주기에 훨씬 더 많은 성취감을 주는 듯 하다.\n\n\n\n\n\n해당 템플릿을 따라 목표를 작성하고 완료를 해나가면 위와 같이 Progress 바가 자동으로 채워지는데, 이게 가져다주는 성취감이 크다. 나같이 게으르고 파워 J인 사람들에게는 한 해를 열심히 살아나가기 위해 주기적인 성취감, 자극이 필요로 되기에 이런게 꼭 필요하다.😂\n\n\n지치지 않는 나만의 방법\n내가 자꾸 이런 식으로 일정 관리의 루틴화를 시도하는 이유는 사실 지치지 않기 위해서다. 평일에 업무를 마치고 퇴근한 뒤에 운동을 하고 공부를 하는 삶의 반복은 누군가에게 번아웃을 가져다줄 수 있다. 이 부분에 있어서 번아웃을 경험하지 않으려면, 퇴근 뒤에 하는 운동과 공부가 특별한 행위가 아닌 것으로 여겨져야 한다. 우리는 매일 밥을 먹고 양치를 하지만, 매일 밥을 챙겨먹고 양치를 하느라 지쳐서 번아웃이 왔다는 사람은 들어본 적 없을 것이다. 특별한 일을 일상의 일로 만들기 위해서는 루틴이 필요하고, 루틴이 몸에 익으면 우리는 이를 특별한 일이 아닌 일상의 일로 여길 수 있게 된다. 특별할 것이 없는 당연한 생활이라는 말이다. 우리는 이런 행위를 다른 말로는 “습관”이라 표현하기도 한다.\n나는 게으른 사람이긴 하지만, 다행히 이제는 퇴근 후에 공부하고 운동을 하는 삶이 더이상 특별한 일이라 느껴지지 않는다. 그저 내 꿈에 조금 더 빠르게 다가가기 위해 필요한 일상적인 행위에 불과하다. 아직 내가 모자란 점은 주말에 시간을 보낼 때이다. 토요일에 계획해둔 Task를 다 완료하면 마음이 그렇게 들뜬다. 이게 주말은 아직 똑바로 루틴화가 안됐다는 증거다. 뭔가 큰 일을 해낸 것 같고, 일요일은 그저 쉬고싶은 마음이 든다.😂 성취감과는 조금 다르다. 평일에 하는 공부도 일상이긴하나 계획해둔 것을 다했을 때 느껴지는 성취감은 매우 크다. 그러나, 주말에 설정해둔 계획을 완료한 것처럼 내가 막 엄청 기특하고 마음이 들뜨진 않는다. 사실, 아직 마음 한켠에 일주일에 하루 정도는 마음 놓고 쉬고 싶은 생각이 있어서 그런 듯하다. 그게 주말 이틀 중 하루인 것 같고. 뭐가 맞는 지는 잘 모르겠다. 평일, 주말 가리지 않고 평생 지속가능하도록, 무던하게 자기 발전에 시간을 할애할 줄 아는 그런 사람이 되고 싶다."
  },
  {
    "objectID": "posts/2023-05-15-monthly_memory-202304/index.html#맺음말",
    "href": "posts/2023-05-15-monthly_memory-202304/index.html#맺음말",
    "title": "월간 회고록: 4월",
    "section": "맺음말",
    "text": "맺음말\n4월에 가장 뜻깊었던 일은 흐지부지하게 시간 중심으로 목표와 일정을 관리해왔던 것을, 연간 목표와 align한 Task 해결을 중심으로 바꾼 것이라 생각한다. 1분기부터 이렇게 했다면 훨씬 더 많은 성취가 있었지 않았을까 하는 조금 아쉬움도 든다. 아직 2023년 절반도 지나지 않았다. 살면서 처음으로 세워 본 올해 목표들을 꼭 다 달성할 수 있는 한해가 되기를 바래본다.\n\n꾸준함이 모든 것을 이긴다"
  },
  {
    "objectID": "posts/2023-05-29-Introduction-to-causal-inference/index.html",
    "href": "posts/2023-05-29-Introduction-to-causal-inference/index.html",
    "title": "인과추론 입문하기",
    "section": "",
    "text": "출처: Korea Summer Workshop on Causal Inference 2022\n오늘은 인과추론의 기초적인 내용들에 대해 이야기해보려고 합니다. 누군가는 이런 질문을 던질 수도 있습니다:\nYes, No로는 답할 수 없겠네요. “항상” 인과추론이 필요로되는 것만은 아니기 때문이죠. 그럼에도 불구하고, 이러한 유형의 질문은 항상 꼭 던져봐야합니다. 내가 하고 있는 비즈니스에 Data science를 도입하기 전에 가장 먼저 던져봐야 하는 질문은 “Data science가 꼭 필요한가?”인 것 처럼요. 우리는 실무에서 언제 “인과추론”의 필요성을 마주하게 될까요? 그리고,\n얽히고 섥혀서 서로 내생적(endogenous)으로 영향을 주고 받는 현실 세계에서 우리는 어떻게 인과적인 효과를 추정해볼 수 있을까요? 이러한 물음들에 답이 될만한 이야기들을 시작해보겠습니다."
  },
  {
    "objectID": "posts/2023-05-29-Introduction-to-causal-inference/index.html#인과추론의-필요성",
    "href": "posts/2023-05-29-Introduction-to-causal-inference/index.html#인과추론의-필요성",
    "title": "인과추론 입문하기",
    "section": "인과추론의 필요성",
    "text": "인과추론의 필요성\n“상관관계(correlation)는 인과관계(causation)가 아니다.”, 통계학을 공부하며 늘상 듣게되는 이야기입니다. 즉, \\(X\\)와 \\(Y\\)가 상관이 있다는 말이 \\(X\\)가 \\(Y\\)의 원인임을 의미하는 것은 아닙니다. 그럼에도 불구하고, 통계학에서 상관관계를 중심으로 이론 전개를 하는 이유는 이 또한 충분한 중요성이 있어서겠죠. 늘 그렇듯 데이터 분석의 목적, Research question에 따라 우리는 적절한 방법론을 취해주면 됩니다.\n그렇다면, 상관관계만으로도 충분한 상황은 언제일까요? 바로 output(\\(Y\\))의 예측에 관심이 있을 때입니다. 상관관계는 예측에 매우 유용합니다. 심지어, 둘 간의 인과관계가 존재하지 않을때도요. 실제로 우리가 예측 모델링을 수행할 때 흔히들 correlation 정도는 살펴보지만, causation까지 파고들진 않습니다. 그렇다고해서, \\(Y\\)의 예측에 도움이 된다는 이유로 너무 쌩뚱맞은 변수를 모델에 반영하는 것은 좋은 예측 모델링이라 하긴 어렵겠죠. 반대로, 인과관계의 추정이 꼭 필요로 될 때가 있습니다. 앞서 이야기한 예측 문제를 풀 때가 아닌, output(\\(Y\\))의 개선, 향상을 위한 input(\\(X\\))의 intervention을 고안하는 것에 관심이 있을 때인데요. 예를 들자면 이런 것들이 있겠네요:\n\n신약을 도입해야하는가 말아야하는가\n마케팅 프로모션을 해야하는가 말아야하는가\n정부 정책을 시행해야하는가 말아야하는가\n\n사실 이러한 Output-oriented approach(예측 모델링), Input-oriented approach(인과추론)는 서로 완전히 배타적이진 않으며, 상호보완적으로 연결됩니다. 예컨데, 인과추론을 통해 얻게되는 원인변수를 prediction에 이용한다면 안정적으로 모델 성능을 향상시키고, 해석적으로도 훨씬 더 나은 모델을 가질 수 있게 되겠죠. 반대로, 어떤 변수가 output의 예측에 큰 도움이 되고 있다면 인과적 관계가 존재할 가능성이 높겠죠. 결국에 다시 돌아봐야할 것은 우리의 목적입니다. 우리가 어떤 context에서 이 문제를 풀기 시작했고, 구체적으로 풀어내야하는 것은 무엇인지에 대해서요."
  },
  {
    "objectID": "posts/2023-05-29-Introduction-to-causal-inference/index.html#인과추론의-어려움",
    "href": "posts/2023-05-29-Introduction-to-causal-inference/index.html#인과추론의-어려움",
    "title": "인과추론 입문하기",
    "section": "인과추론의 어려움",
    "text": "인과추론의 어려움\n\nEverthing is Endogenous\n\n인과추론의 가장 중요한 대전제입니다. 우리가 관심있는 원인변수 외에도 영향을 미치는 수많은 교란요인(confounding variables)들이 서로 내생적으로 얽혀있기 때문에 인과적 효과를 발라내기란 매우 어렵습니다. 이를 인과추론에서는 endogeneity problem이라고 표현하는데요. 대부분의 인과추론 방법론들은 endogeneity problem을 극복하기 위해 설계되었다고 합니다. 결국, 이 부분의 극복을 위해서는 데이터가 어떻게 생성되었는지, 즉 DGP(Data Generation Process)에 대해 알아야하는데요. 아시다시피 현실 세계에서 생성되는 데이터의 모든 DGP를 파악하기란 불가능합니다. DGP를 모르는데, endogeneity를 어떻게 해결할 것인가? 이것이 인과추론이 어려운 근본적 이유죠.\n인과추론에는 크게 2가지 접근법이 있습니다:\n\nPotential Outcome Framework: Havard의 Donald Rubin 교수가 제안한 접근법으로, 잠재적 결과(potential outcome)을 이용해 인과관계를 추론하고자 함\nStructural Causal Model: UCLA의 Judea Pearl 교수가 제안한 접근법으로, 변수들을 인과 그래프로 모델링하여 인과관계를 추론하고자 함\n\n가장 쉬운 해결책은 Potentail Outcome Framework에 기반하여 DGP를 알 수 있는 연구 디자인을 수행하는 것입니다. 인과추론의 Gold Standard라 알려져있는 RCT(Randomized Controlled Trial) 또한 이러한 연구 디자인의 예 중 하나라고 할 수 있겠습니다. 본 글의 나머지 영역에서는 Potential Outcome Framework에 대한 기초적인 내용들에 대해 이야기해보려고 합니다."
  },
  {
    "objectID": "posts/2023-05-29-Introduction-to-causal-inference/index.html#potential-outcome-framework-기초",
    "href": "posts/2023-05-29-Introduction-to-causal-inference/index.html#potential-outcome-framework-기초",
    "title": "인과추론 입문하기",
    "section": "Potential Outcome Framework 기초",
    "text": "Potential Outcome Framework 기초\nPotential Outcome Framework는 Design based approach의 핵심이자, 전통적으로 가장 많이 활용되는 Causal model이라 할 수 있습니다. Potential Outcome Framework에서 인과추론이 가능한 원리에 대해 한 번 알아보죠. Potential Outcome Framework에서는 인과관계에 대해 잠재적 결과를 바탕으로 접근합니다. 여기서 “잠재적(potential)”의 의미는 실제로 발생하지 않은 일을 의미합니다. 즉, 특정 처리의 인과적 효과를 처리를 받았을 때의 실제 결과와, 만약 처리를 받지 않았으면 존재했을 잠재적 결과 간의 차이로 정의하고 분석해보자는 것이죠. 여기서, 우리는 처리를 받지 않았으면 있었을 잠재적 결과를 Counterfactual(반사실)이라고 부릅니다.\n\n\n\n\n\n다만, Potential Outcome Framework를 이용하더라도 각 개인에 대한 처리 효과(ITE, individual treatment effect)는 구할 수 없습니다. 현실에서 Counterfactual 실제로 관찰하는 것은 불가능하기 때문입니다. 이를 우리는 Fundamental Problem of Causal Inference라 부릅니다. 하지만, 처리를 받은 그룹의 평균과 처리를 받지 않은 그룹(Control group, 이하 통제군)의 평균 차이를 통해 처리의 평균적인 인과 효과(Average treatment effect, 이하 ATE)는 구할 수 있습니다.\nATE를 추정하는 일, 즉 인과추론이 어려운 이유는 이상적인 존재인 Counterfactual과 통제군의 차이에서 기인할 것이고, 즉 이를 해결하기 위해서는 Counterfactual과 최대한 가까운 비교가능한(comparable) 통제군을 찾아서 비교하는 과정이 필요로 될 것입니다. 그리고, 이러한 비교가능한 통제군을 잘 구성하여 인과추론을 수행하려는 접근법이 바로 Potential Outcome Framework에 해당합니다.\n앞서 이야기한 것들을 처음 접하는 분들이라면 이해가 어려울 수 있습니다. 이럴때는 오히려 수식을 통한 정의가 직관적인 이해에 도움이 될 수 있습니다.😀 먼저 잠재적 결과를 표기하는 notation에 대해 알아보죠.\n\\[\nT_i = \\begin{cases} 1 & \\rm{If \\ \\ unit \\ \\ i \\ \\ received \\ \\ the \\ \\ treatment} \\\\0 & \\rm{If \\ \\ unit \\ \\ i \\ \\ did \\ \\ not \\ \\ receive \\ \\ the \\ \\ treatment} \\end{cases}\n\\]\n여기서 \\(T_i\\)는 실험대상(unit) \\(i\\)에 관한 처리 여부를 의미합니다. 참고로 종종 \\(T\\)를 \\(D\\)로 표현하는 경우도 있습니다. 그리고, 실험대상 \\(i\\)의 결과(outcome)는 \\(Y_i\\)로 표기할 것으로 합니다. 여기에 \\(T_i\\)를 아래첨자로 추가하면 잠재적 결과를 표현할 수 있습니다.\n\n\\(Y_{0i}\\): 실험대상 \\(i\\)가 처리를 받지않았을 때의 잠재적 결과\n\\(Y_{1i}\\): 실험대상 \\(i\\)가 처리를 받았을 때의 잠재적 결과\n\n참고로, 잠재적 결과는 종종 \\(Y_i(0)\\), \\(Y_i(1)\\)로 표현하기도 합니다. 앞서 언급한 내용이지만, 잠재적 결과가 실제로 관측이 된 경우를 Factual(사실), 관측하지 못한 경우를 Counterfactual(반사실)이라 부릅니다. 앞서 배운 표기를 이용하여 ITE를 표현해보면 다음과 같습니다:\n\\[\n{\\rm{ITE}} = Y_{1i} - Y_{0i}\n\\]\nCounterfactual의 관측은 현실세계에서 어려우므로 앞서 말했듯 Potential Outcome Framework를 이용하더라도 ITE의 추정은 불가능합니다. 그러나, ATE의 추정은 가능하다고 했습니다.\n\\[\n{\\rm{ATE}} = E[Y_1 - Y_0]\n\\]\n여기서 \\(E\\)는 통계학에서 흔히 접할 수 있는 기댓값(expectation) 개념1입니다. 여기서 앞서 이야기하지 않았던 ATE보다 조금 더 추정이 쉬운 값인 처치군에 대한 평균처치효과(Average treatment effect on the treated, 이하 ATT)에 대해서도 정의를 해두겠습니다.\n\\[\n{\\rm{ATT}} = E[Y_1-Y_0 | T=1]\n\\]\n때때로 우리는 주어진 데이터, 가능한 연구 디자인에 따라 ATE가 아닌 ATT를 추정하는 것으로 만족해야할 수 있습니다. ATT를 ATE로 확장하기 위해서는 처리군과 통제군이 서로 완전히 비슷해서 서로 역할을 바꿔도(exchangeable) 결과가 동일했을 거라는 조금 더 추가적인 강한 가정이 있어야 합니다.\n자, 그럼 기초적인 수식 정의는 끝이났습니다. 아무튼 현실세계에서는 ATE와 ATT의 추정도 어려울텐데, 비교가능한 통제군을 구성하는 것만으로도 어떻게 이것이 가능한 것일까요? 이 부분도 수식으로 한 번 적어보면 직관적인 이해에 큰 도움이 될 것입니다. 먼저, 우리가 현실세계에서 흔히 관측할 수 있는 효과는 Association(연관)입니다.\n\\[\n{\\rm{Association}} = E[Y|T=1] - E[Y|T=0]\n\\]\n여기서 Association은 두 변수 간의 매우 일반적인 관련성, 경향성을 나타내는 개념으로, 두 변수 간의 연결성을 보여주지만 원인과 결과를 관계를 보여주진 않습니다. 통계학에서 흔히 이야기하는 Correlation(상관)은 Association에 포함되는 더 구체적인 통계적 개념으로, 두 변수가 증가 또는 감소 추세를 보일 때를 의미한다고 생각하시면 될 것 같습니다. 아무튼, 이 Association 식을 조금 가지고 놀아보겠습니다.\n\\[\n\\begin{align}\n\\rm{Association} & = E[Y|T=1] - E[Y|T=0] \\\\ & = E[Y_1|T=1] - E[Y_0|T=0] \\\\ & = E[Y_1|T=1] - E[Y_0|T=0] + E[Y_0|T=1] - E[Y_0|T=1] \\\\ & = \\{ E[Y_1|T=1] - E[Y_0|T=1] \\} + \\{ E[Y_0|T=1] - E[Y_0|T=0] \\} \\\\ & = \\rm{ATT} + \\rm{BIAS}\n\\end{align}\n\\]\n마법이 일어났습니다. 즉, 우리는 Bias2를 통제해주면 현실에서도 ATT를 추정할 수 있게됩니다. 즉, 두 집단의 평균 차이가 곧 인과 효과(causal effect)가 된다는 말이죠. Bias란, Context에 따라 Selection Bias라고 표현되기도 하는데, 처치를 받지 않았을 때의 처치군과 통제군의 차이입니다. 즉, 처리 전에는 둘 간의 차이가 없어야 한다는 것이며, 앞서 수식 정의 전 단계에서 설명했듯 비교가능한 통제군을 구성해야한다는 의미가 바로 여기에 있는 것이죠. 즉, 비교가능한 통제군이란 다음의 등식이 성립하는 상황을 이야기합니다.\n\\[\nE[Y_0|T=1] = E[Y_0|T=0]\n\\]\n참고로 이러한 Bias가 발생하는 근본적인 원인은 앞서 인과추론의 어려움에 대해 설명할 때 잠깐 등장했던 교란변수의 존재때문입니다. Bias가 발생할 수 밖에 없는 상황을 2가지를 예시로 들어보겠습니다.\n\nBias가 발생하는 예: 학습에 태블릿을 사용하는 것이 학업 성취도에 증가에 영향을 미치는가?\n\n별다른 연구 디자인 없이, 단순하게 수업 간에 태블릿을 지급하는 학교의 학업 성취도와 태블릿을 지급하지 않는 학교의 학업 성취도를 비교할 경우 Bias가 발생하여 태블릿이 학업 성취도에 미치는 인과적 효과를 추정하기 어렵습니다. 여기서 Bias를 발생시킬 수 있는 교란변수의 예는 교사의 질, 학교 수업료, 위치 등이 될 수 있습니다. 태블릿을 무상으로 지급하는 학교의 경우 애초에 학비가 더 비싸고 부유한 집의 자녀들이 다닐 것이기 때문에 애초에 \\(E[Y_0|T=1]\\)과 \\(E[Y_0|T=0]\\)이 비교가능하지 않을 것입니다.\n\nSelection Bias가 발생하는 예: 반려동물을 키우는 것이 우울증 감소에 영향을 미치는가?\n\n반려동물을 키우는 것은 개인의 자발적 선택이므로 Selection Bias라고 표현을 했습니다.3 그런데, 자발적 선택이 우울증과 관련이 있을수도 있습니다. 가령, 우울감이 심할수록 반려동물을 키우는 경향이 클 수 있죠. 그러면, 반려동물을 키우는 처리군과 키우지 않는 통제군은 반려동물을 키운다는 사실(처치)을 제외하고도 그룹 간 특성(우울증의 정도)이 다를 수 있습니다. 이 경우도 마찬가지로 애초에 \\(E[Y_0|T=1]\\)과 \\(E[Y_0|T=0]\\)이 비교가능하지 않은 경우이므로, 반려동물이 우울증에 영향을 미치는 인과적 효과를 추정하기란 어려울 것입니다.\n\n\n위 두 예시가 Bias의 이해에 도움이 되었기를 바랍니다. 아직 우리는 Association으로 ATT를 추정할 수 있는 바만 확인했으므로, 다시 수식으로 돌아가보죠. ATE로는 어떻게 확장이 가능할까요?Bias는 0이라고 가정하고 Association 식을 다시 써보겠습니다:\n\\[\n\\begin{align}\n{\\rm{Association}} & = E[Y|T=1]-E[Y|T=0] \\\\ & = ATT \\\\ & = E[Y_1-Y_0|T=1]\n\\end{align}\n\\]\n앞서 살짝 언급했듯 ATE로의 확장은 exchangeability가 성립할 때 가능하다고 했습니다. 즉, ATE로의 확장을 위해서는 처치 전 교환가능(exchangeble):\n\\[\nE[Y_0|T=1] = E[Y_0|T=0]\n\\]\n뿐만이 아닌, 처치 후 교환가능까지 성립을 해야합니다. 이 개념은 수식으로 다음과 같이 표현할 수 있습니다:\n\\[\nE[Y_1 - Y_0 | T=1] = E[Y_1 - Y_0 | T=0]\n\\]\n위 식이 성립하면 다시 한 번 마법이 일어납니다.\n\\[\n\\begin{align}\n{\\rm{Association}} & = E[Y|T=1]-E[Y|T=0] \\\\ & = ATT \\\\ & = E[Y_1-Y_0|T=1] \\\\& = E[Y_1-Y_0 | T=0] \\\\ & = E[Y_1-Y_0] \\\\ & = {\\rm{ATE}}\n\\end{align}\n\\]\n앞서 수식을 통해 확인해본 바들을 정리해보겠습니다:\n\nPotential Outcome Framework를 통해 Bias를 통제하여 비교가능한 통제군을 구성하면 Association으로 Causation ATT를 추정할 수 있다.\n여기서 더 나아가, Causation을 전체 샘플에 확장하기 위해 ATE를 구하고자 한다면, 처리군과 통제군이 서로 완전히 비슷해서 서로 역할을 바꿔도 결과가 동일했을거라는 더 강한 추가적인 가정이 있어야 한다.\n\n마지막으로 드리고 싶은 말씀은 실험 디자인, 관측 가능한 데이터, Context에 따라 ATT의 추정까지만 가능한(또는 필요한) 경우도 있으며, 인과추론 자체가 불가능한 상황이 존재할 수도 있다는 것입니다."
  },
  {
    "objectID": "posts/2023-05-29-Introduction-to-causal-inference/index.html#맺음말",
    "href": "posts/2023-05-29-Introduction-to-causal-inference/index.html#맺음말",
    "title": "인과추론 입문하기",
    "section": "맺음말",
    "text": "맺음말\n오늘은 인과추론의 필요성과 어려움, Potential Outcome을 통해 Causation을 추정하는 원리에 대해 이야기해보았습니다. 지난 3월에는 Potential Outcome Framework 관점의 인과추론 방법론 중 하나인 준실험설계(Quasi experimental design)에 대해서 이야기해보기도 했는데요. 이 글을 통해 인과추론을 처음 접하신 분들이라면, 어쩌면 조금 더 인과추론의 흥미를 돋궈줄 수도 있을지도 모르겠다는 생각이 들어 3월에 작성했던 글을 공유드려봅니다.\n▶️ 글 보러가기\n인과추론은 원인과 결과를 분석하는 것이긴 하나, 더 본질적인 것은 인과추론은 Data science를 구성하는 또 하나의 분석 도구라는 것입니다. 주어진 Context에서 본인이 관심있는 부분 또는 해결해야하는 문제에 있어서 필요로 되는 부분이 Input에 관한 Intervention을 고안하는 일인지, Output에 관한 예측에 관한 일인지에 따라 인과추론의 필요 유무가 결정될 것입니다. 어떤 일을 시작하기 전에 항상 스스로에게 질문을 던져봅시다. 이 문제를 해결하기 위해 “OOO이 필요한가?” 라고 말이죠."
  },
  {
    "objectID": "posts/2023-05-15-monthly-memory-202304/index.html",
    "href": "posts/2023-05-15-monthly-memory-202304/index.html",
    "title": "월간 회고록: 4월",
    "section": "",
    "text": "The illustration by Mary Amato\n회고를 분기 별로 나눠서 하는게 나을지 고민 중이다. 굳이 회고할만한 굵직한 것들이 없는데 매달 회고를 할 필요가 있나 싶기도 해서. 그럼에도 불구하고, 이렇게 결국 월간 회고를 쓰고 있는 이유는 분기 마다 쓰면 너무 거창한 작업이 될까 싶기도 해서이다.😂 월간 회고의 목적은 오로지 지난 한 달을 가볍게 성찰하는 것에 오는 개인적인 재미와 만족감에 있기에."
  },
  {
    "objectID": "posts/2023-05-15-monthly-memory-202304/index.html#일",
    "href": "posts/2023-05-15-monthly-memory-202304/index.html#일",
    "title": "월간 회고록: 4월",
    "section": "일",
    "text": "일\n\nPython 관련 잡설\nR에 익숙하고 tidyverse를 좋아하는 사람에게 Python에서 pandas만을 활용해 Data wrangling을 수행하라는 것은 고문과 같다. pandas가 구리다는 것은 아니고, 나는 tidyverse의 pipe-friendly한 코드를 작성하는 것이 더 익숙하고 편하기 때문이다. 그래서, Python에서 tidyverse style을 구현한 라이브러리를 찾아볼 수 밖에 없었다. 내가 불편해하는 것은 누군가도 불편해했을 것이고, 이에따라 누군가가 구현을 해놓았을 것이기 때문에..😂 다행히 몇 가지 패키지를 발견 했고, 그 중 가장 완성도있고 최근까지 업데이트가 되고있는 라이브러리를 소개해본다.\n\ndatar: https://github.com/pwwang/datar\n\ndatar는 Python에서 여러 백엔드로 제공되는 data manipulation을 위한 API를 R의 tidyverse 패키지와 최대한 align되도록 재구상한 라이브러리에 해당한다. 사실 작년 4분기에 Python 기반 도구인 Streamlit으로 대시보드를 개발하며 siuba라는 라이브러리를 이용해 data wrangling을 수행했었는데, siuba의 경우 pipe-friendly한 코드를 짜기에는 부족한 기능들이 너무나 많았다. 이와 달리, datar은 tidyverse로 데이터를 만질때 종종 사용하는 R Base의 함수들까지 상당수 커버하고 있다. 당연히 R에서 tidyverse로 데이터를 가지고 노는 만큼은 편하지 않지만, 그래도 이정도면 아주 만족스러운 퀄리티다. 레퍼런스도 아주 쉽고 상세하게 제공하고 있다. 앞으로도 해당 패키지를 꾸준히 업데이트를 해나가는 데에 동기부여가 될 수 있도록, 이런 레포에는 꼭 스타를 눌러주자! tidyverse 문법을 선호하는 Python 사용자라면 한 번쯤 사용해보기 바란다.\n\n\n고객 관심사의 선제적 반영\n전에 말했듯 우리 팀은 요즘 모바일 홈 개인화에 집중을 하고 있는데, 이 부분에 있어서 다음과 같은 부분을 반영해보기 위해 시계열 자료분석을 수행했었다:\n\n당장 관심을 가질만한 상품을 띄워주는 것도 중요하지만, 추후 맞이할 특정 시즌에 구매 의사가 생길만한 상품글까지 고려하여 랭킹 시스템을 구성한다면 고객들의 쇼핑 만족도를 더욱 높여줄 수 있지 않을까?\n\n업무와 직접적으로 관련한 방법론들이기에 자세하게 A to Z까지 소개하긴 어렵지만, 해당 문제를 해결하기 위해 사용한 방법론들을 블로그에 소개하기도 했다. 관심있는 사람들은 참고해보기 바란다.\n▶️ 글 보러가기\n아무튼, 이 프로젝트를 4월 초에 잘 마무리하고 타 팀에 공유세션까지 잘 마칠 수 있었다. 그런데, 가만 생각해보니 나는 강한 가정을 내포한채로 이 문제를 해결한 것이었다:\n\n과거 1년의 추세는 올해에도 이어질 것이다.\n\n앞서 제시한 문제를 보면 알겠지만 결국 예측 문제다. 이 방법론을 좀 더 정교하게 개발하고 싶은 마음이 들었다. 사실, 현재는 1년치 시계열 자기자신의 추세, 계절성만으로 해당 문제를 해결하는 매우 간단하게 풀어내는 로직이다. 계산량을 고려해봐야겠지만 3년 내지 4년 규모의 시계열을 고려하여 yearly seasonality까지 캐치할 수 있도록 하고, 모형에 이전 시차의 기온, 습도, 강수량, 휴일 효과 등까지 반영하여 Monthly로 모형을 갱신하여 매달 예측을 수행해 나간다면, 매달 다가오는 시즌의 고객 관심사를 훨씬 더 정교하게 예측할 수 있을 것으로 생각된다.\n물론, 아직 이상적인 상상에 불과하긴하다.\n\n\n\n구상한 방법론을 적용해본 뒤, 실제로 맞닥뜨리는 흔한 상황\n\n\n짧은 시간에 끝날만한 수준의 모델링 작업은 아닌 것 같아서, 시도해볼만한 가치가 있는 일인지에 대해 팀 내에 이야기를 해봐야겠다.😀"
  },
  {
    "objectID": "posts/2023-05-15-monthly-memory-202304/index.html#개인",
    "href": "posts/2023-05-15-monthly-memory-202304/index.html#개인",
    "title": "월간 회고록: 4월",
    "section": "개인",
    "text": "개인\n\nPomodoro Out, Task In\n지난 연간 회고록에서 뽀모도로로 시간을 관리하고 있다고 했는데, 쓸수록 영.. 별로였다. 뽀모도로를 기반으로 업무와 공부를 하다보니 생기는 부작용은 다음과 같았다:\n\n집중력 하락: 기본적으로 25분 공부 5분 휴식인데, 간혹 회의가 길어져서 1뽀모도로(25분)가 넘어가면 이 뽀모도로로 공부한 시간을 정확히 측정하는 데에 강박이 생겨 일일이 휴식을 Skip하러 가곤 했다. 그리고, 진정한 집중은 “시간이 언제 이만큼 흘렀지?”를 느낄 때라고 할 수 있는데, 뽀모도로 타이머를 기반으로 움직이다보니 오히려 시계를 더 많이 보게 된다.\n한 주간 해결한 Task의 양과 질이 아닌 집중한 시간에 집착: 나는 월간 회고를 블로그에 올리고 있지만, 개인적으로 주간 회고도 하고 있다. 개인적으로 쌓아둔 주간 회고를 기반으로 월간 회고가 만들어진다고 보면 된다. 맥의 Focus To-Do 앱을 유료 결제하면 매주 뽀모도로를 얼마나 적립했는지 자세한 보고서를 확인할 수 있다. 이 부분이 오히려 내겐 독이 됐다. 돌아보니 올해 생각해 둔 목표의 달성을 위한 Task들을 이번 주에 얼마나 해결했냐를 평가하는 것이 아니라, 그저 집중한 시간이 얼마인지만 늘상 체크하고 있었다.\n\n물론, 늘 그렇듯 뽀모도로 라는 시간 관리법 자체가 구리다는 것이 아니라 그저 나에게는 조금 안맞는 방법이었다는 것이다.😀 사실 그 전부터 뽀모도로에 부작용을 느껴왔지만 선뜻 버리지 못한 근본적인 이유는 올해 설정해둔 목표를 관리하는 방식에 있었다. 나는 Notion에서 내 올해 목표, 오늘 할 일 등을 관리하고 있는데, 올해 목표를 설정만 해두고 해당 목표를 달성하기 위해 수행해야하는 Task들을 구체적으로 정의해놓진 않았다. 연간 목표를 세운게 처음이라 그런지 많이 서툴렀던 것 같다.\n이렇다보니 벌써 2023년의 2분기가 저물어가는데, 해당 목표에 내가 얼마만큼이나 다가갔는지에 대한 정량적인 평가가 불가능했다. 그리고, 이런 평가가 불가능하다보니 거대한 연간 목표의 달성을 위해 내가 열심히 달려가고 있다는 것을 느낄만한 성취감 또한 없었다. 이러한 고민을 안고있었기에 “시간” 중심으로라도 성취감을 느낄 수 있었던 뽀모도로를 버리긴 힘들었다. 그러다가, 이 영상을 마주했다.\n\n드림코딩이라는 채널에서 공유해주신 개발자가 사용하는 노션 노트인데, 올해 목표를 설정하고 해당 목표를 달성하기 위한 Task들을 정의하여 이를 중심으로 일정을 관리하고 싶었던 내 니즈에 완벽하게 부합하는 템플릿이였다. 또한, 위 영상을 보며 노션의 데이터베이스 활용법도 터득할 수 있었는데, 내가 여지껏 노션을 참 무식하게 써왔다는 것 또한 깨달을 수 있었다. 아무튼, 이 템플릿 덕분에 기존에 주먹구구 식으로 관리하던 To do list를 연초에 작성해뒀던 연간 목표와 align하고 각 목표 달성을 위한 Task들을 세부적으로 정의함으로써, Task 중심으로 일정을 관리할 수 있게됐다. 확실히 Task 중심의 관리가 목표에 점차 다가가고 있는 나를 가시적으로 볼 수 있있게 해주기에 훨씬 더 많은 성취감을 주는 듯 하다.\n\n\n\n\n\n해당 템플릿을 따라 목표를 작성하고 완료를 해나가면 위와 같이 Progress 바가 자동으로 채워지는데, 이게 가져다주는 성취감이 크다. 나같이 게으르고 파워 J인 사람들에게는 한 해를 열심히 살아나가기 위해 주기적인 성취감, 자극이 필요로 되기에 이런게 꼭 필요하다.😂\n\n\n지치지 않는 나만의 방법\n내가 자꾸 이런 식으로 일정 관리의 루틴화를 시도하는 이유는 사실 지치지 않기 위해서다. 평일에 업무를 마치고 퇴근한 뒤에 운동을 하고 공부를 하는 삶의 반복은 누군가에게 번아웃을 가져다줄 수 있다. 이 부분에 있어서 번아웃을 경험하지 않으려면, 퇴근 뒤에 하는 운동과 공부가 특별한 행위가 아닌 것으로 여겨져야 한다. 우리는 매일 밥을 먹고 양치를 하지만, 매일 밥을 챙겨먹고 양치를 하느라 지쳐서 번아웃이 왔다는 사람은 들어본 적 없을 것이다. 특별한 일을 일상의 일로 만들기 위해서는 루틴이 필요하고, 루틴이 몸에 익으면 우리는 이를 특별한 일이 아닌 일상의 일로 여길 수 있게 된다. 특별할 것이 없는 당연한 생활이라는 말이다. 우리는 이런 행위를 다른 말로는 “습관”이라 표현하기도 한다.\n나는 게으른 사람이긴 하지만, 다행히 이제는 퇴근 후에 공부하고 운동을 하는 삶이 더이상 특별한 일이라 느껴지지 않는다. 그저 내 꿈에 조금 더 빠르게 다가가기 위해 필요한 일상적인 행위에 불과하다. 아직 내가 모자란 점은 주말에 시간을 보낼 때이다. 토요일에 계획해둔 Task를 다 완료하면 마음이 그렇게 들뜬다. 이게 주말은 아직 똑바로 루틴화가 안됐다는 증거다. 뭔가 큰 일을 해낸 것 같고, 일요일은 그저 쉬고싶은 마음이 든다.😂 성취감과는 조금 다르다. 평일에 하는 공부도 일상이긴하나 계획해둔 것을 다했을 때 느껴지는 성취감은 매우 크다. 그러나, 주말에 설정해둔 계획을 완료한 것처럼 내가 막 엄청 기특하고 마음이 들뜨진 않는다. 사실, 아직 마음 한켠에 일주일에 하루 정도는 마음 놓고 쉬고 싶은 생각이 있어서 그런 듯하다. 그게 주말 이틀 중 하루인 것 같고. 뭐가 맞는 지는 잘 모르겠다. 평일, 주말 가리지 않고 평생 지속가능하도록, 무던하게 자기 발전에 시간을 할애할 줄 아는 그런 사람이 되고 싶다."
  },
  {
    "objectID": "posts/2023-05-15-monthly-memory-202304/index.html#맺음말",
    "href": "posts/2023-05-15-monthly-memory-202304/index.html#맺음말",
    "title": "월간 회고록: 4월",
    "section": "맺음말",
    "text": "맺음말\n4월에 가장 뜻깊었던 일은 흐지부지하게 시간 중심으로 목표와 일정을 관리해왔던 것을, 연간 목표와 align한 Task 해결을 중심으로 바꾼 것이라 생각한다. 1분기부터 이렇게 했다면 훨씬 더 많은 성취가 있었지 않았을까 하는 조금 아쉬움도 든다. 아직 2023년 절반도 지나지 않았다. 살면서 처음으로 세워 본 올해 목표들을 꼭 다 달성할 수 있는 한해가 되기를 바래본다.\n\n꾸준함이 모든 것을 이긴다"
  }
]