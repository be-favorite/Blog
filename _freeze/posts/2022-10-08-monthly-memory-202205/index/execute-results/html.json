{
  "hash": "fc526e5a30683191c0919a1f9e4f6224",
  "result": {
    "markdown": "---\ntitle: \"월간 회고록: 2022년 5월\"\ndescription: |\n  실무에서 1년반 동안 연구과제를 수행하며 겪은 크고 작은 난관들\ndate: \"2022-10-08\"\ndraft: FALSE\npreview: preview.jpg\ncitation:\n  url: https://taemobang.com/posts/2022-10-08-monthly-memory-202205/\ncategories: [Memory]\nbibliography: references.bib\n---\n\n\n\n\n[![Photo by Fredy Jacob on Unsplash](preview.jpg){width=\"800\"}](https://unsplash.com/photos/t0SlmanfFcg?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink)\n\n이직 준비하랴, 논문 작업하랴 미루고 미뤘던 월간 회고록들을 이제야 다시 적습니다. 5월 회고록을 작성하고 있는 이 시점은 어느덧 10월이군요. 부끄럽네요. 다행히 글감은 다 정해놨습니다. 하하. 지난 5월은 당시 재직 중인 연구센터에서 맡고 있는 학술연구과제 \\<빅데이터를 이용한 미세먼지 건강영향평가\\>에서 다변량 메타분석을 수행하는데에 가장 많은 시간을 할애했습니다. 1년반을 재직했던 직장에서 가장 많은 시간을 할애한 연구과제이기도 합니다. 분석해야할 질환들의 양이 정말 방대했거든요.🤯 대학원을 졸업하고 실무에 나와 처음으로 맡은 프로젝트인데, 데이터 수집, 데이터 마트 구축 부터 분석, 보고서 작성까지.. 정말 많은 양의 일을 혼자 처리했어야 했죠. 중간중간에 여러 난관에 봉착했고, 이 난관들을 하나하나 해결하면서 쾌감을 느끼기도 했습니다. 이번 월간 회고에서는 제가 겪은 크고 작은 난관들에 대해 이야기해보려고 합니다.\n\n## SAS를 다시 손에 잡다\n\n본 연구과제는 건강보험공단 빅데이터 맞춤형 연구DB를 기반으로 진행되는데요. 자료의 특성 상 데이터 프라이버시때문에 건강보험공단에서 운영 중인 분석센터에 직접 가야지만 데이터를 열람하고 처리할 수 있습니다. 연구DB 신청 당시 분석 툴에 SAS + R을 선택할 수 있길래 당연히 데이터를 확인하고 처리하는 것도 R로 가능할 것이라 생각 했는데, 구축된 데이터는 SAS 서버 상에 놓여져있더군요. 보통 수십기가가 넘는 데이터를 처리해야하기 때문에, 분석센터의 데스크탑 로컬에 구축된 R로 원 자료를 핸들링하는 것은 어려웠습니다. 그래서, SAS로 원 자료를 랭글링[^1]해야만 하는 상황이였습니다.\n\n[^1]: **데이터 랭글링**(Data Wrangling) 혹은 데이터 먼징(Data Munging)이라고 불리는 이것은 원자료(raw data)를 보다 쉽게 접근하고 분석할 수 있도록 데이터를 정리하고 통합하는 과정. [🔗Source](https://ko.wikipedia.org/wiki/데이터_랭글링)\n\n좀 당황스러웠습니다. 하하.. SAS는 학부생 때 2년정도 배우고 손을 뗀 상태였기 때문이죠. 제가 원하는 데이터 마트를 원활하게 구축하기 위해서는 SAS에서 SQL과 Macro[^2]의 적절한 활용이 필요했습니다. 다행히, 학부생때 공부했던 기억이 남아있었죠. 그러나, 약 3년의 시간이 흘렀기에 복습이 필요했고 위키독스의 [**SAS로 하는 기초 데이터 전처리, 핸들링(Data handling)**](https://wikidocs.net/book/2678) 를 활용해 기본기를 다시 다진 후에 본격적인 랭글링 작업에 들어갔습니다. 분석센터로 출장을 가기 전 데이터 구조를 떠올리며 미리 코드를 작성하고, 현장에서 발생하는 오류가 있다면 디버깅만을 해주는 식으로 효율적으로 작업을 진행해 나갔습니다.\n\n[^2]: 일반적인 프로그래밍 언어의 함수(function)와 같은 기능을 한다고 보시면 됩니다\n\n이러한 작업을 반복하다보니 SAS SQL, Macro가 손에 익었고 제가 필요로 하는 데이터 마트를 수월하게 구축하고 추출해낼 수 있었습니다.\n\n## 풀고자 했던 문제\n\n본 과제 이름은 \\<빅데이터를 이용한 미세먼지 건강영향평가\\>이긴 하나, 6종의 대기오염원(${\\rm{PM}}_{10}$, ${\\rm{PM}}_{2.5}$, ${\\rm{NO}}_{2}$, ${\\rm{SO}}_{2}$, ${\\rm{O}}_{3}$, ${\\rm{CO}}$)에 관해 모두 평가를 실시했어야 했습니다. 말이 거창해서 건강영향평가이지만, 기본적인 로직은 간단합니다. 질병의 일 발생/입원 건수를 $Y$로 잡고, 일 대기오염원 농도를 $X$로 잡아서 시계열 회귀 모델링을 하는 것이죠. 이를 통해 대기오염원들이 특정 질병의 발생/입원에 미치는 건강영향을 평가하는 것입니다. 여기서, 대기오염원이 미치는 효과는 비선형(non-linear)의 형태를 띤다는 점과 지연 효과(lag effect)[^3]를 갖는다는 점까지 모형에서 고려해주어야 했죠. Distributed lag non-linear model[@gasparrini2010] (이하, DLNM)은 이 둘을 동시에(simultaneously) 모델링 하게끔 해줍니다. 크게 봤을 때는 시계열 회귀모형의 일종이죠. 환경적 요인들로 Outcome을 모델링하는 생태학적 연구분야에서는 DLNM이 최신 시계열 회귀모형이라고 보시면 될 것 같습니다. 본 분야의 모델링 쪽 연구는 [Gasparrini](http://www.ag-myresearch.com) 교수가 주도하고 있구요.\n\n[^3]: 예를 들자면, 5일 전에 마신 고농도의 초미세먼지가 질병 발생에 영향을 미치는 것\n\n대학원에서 가장 많이 시간을 할애했던 연구 분야는 시계열 자료분석이었고, 이때 시계열을 회귀적으로 모델링하는 접근과 Distributed lag model에 대해 공부했던 경험이 있어 DLNM으로의 확장은 크게 어렵지 않았습니다.\n\n## 모형 최적화\n\nDLNM은 $X$ 의 비선형 효과와 지연 효과까지 모델링하기 때문에, 튜닝해야하는 초모수(Hyperparameter)가 참 많은데요. 문제는 앞서 말씀드렸다시피 봐야할 질환들이 굉장히 많았다는 것이죠. 신경과 질환 6개, 정신과 질환 3개, 호흡기 질환 8개, 안과 질환 21개 총 38개 질환을 봐야했습니다. 여기서 문제는 2가지가 더 있었어요.\n\n-   6종의 대기오염원 각각에 대해 모델링\n\n    -   여러개 대기오염원을 고려하는 경우, 다중공선성 이슈로 인해 회귀계수 추정량의 표준오차가 굉장히 커지는 문제가 있었습니다. 이 문제를 겪고 다시 조사했던 선행연구들을 들여다보니 모두 대기오염원 1개씩만 모형에 포함시켜 모델링했더군요.\n\n-   우리나라 6개 대도시 지역 각각에 대해 모델링\n\n    -   지역마다 질환 발생 패턴은 비슷하나, 대기오염원의 농도가 지리적 환경에 따라 다르기 때문에 좀 더 정교한 모델링을 위해서는 각 지역마다 모델링이 필요했습니다.\n\n그럼 제가 최적화해야 했던 모델은 총 몇개였을까요? $38\\times6\\times6=1,368$개의 모형을 최적화해야 했습니다. 정녕 이것이 혼자 할 수 있는 양이였을까요?.. 참고로, 튜닝해야할 초모수가 하나가 아니라 3가지정도 됐었습니다. 이 사실을 깨닫고 한동안 꽤나 막막했습니다. 선행 연구 사례들을 살펴봐도, 논문 하나에 실리는 대부분의 연구는 일반적으로 특정 하나의 지역 및 질환에 국한하여 수행된 대기오염원 건강영향평가였죠.\n\n아시다시피, $Y$와 $X$간의 relationship에 관한 모델링이 주 목적인 회귀 모델링에서는 그 과정에서 분석자의 세심한 검토(e.g. 잔차분석)가 필요합니다. 그러나, 1,368개의 모형을 최적화해야하는 제게, 모형 하나하나를 세심하게 살펴서 모델링하기란 불가능했습니다. 그래서, 모형에 들어갈 공변량[^4]에 대한 variable selection과 모형 최적화를 위해 몇 가지 초모수들을 튜닝하는, 추후에 논문을 평가할 리뷰어들이 납득이 가능한 하나의 획일화된 알고리즘을 만들 필요성을 느꼈습니다. 그 속에는 계산양이 적지않기 때문에 병렬처리 또한 구현을 해야 했고요. 선행연구들을 살펴봤으나 대개 이쪽 연구는 환경역학 쪽 학술지에 게재되서인지, 모델링 과정에 있어서 어떤 식으로 초모수 튜닝을 수행했고, 변수 선택은 어떻게 했고에 대한 것들이 매우 러프하게 기술되어 있었습니다. 그래서, 제가 알고리즘을 직접 짜봐야했죠.\n\n[^4]: covariates. 몇 가지 기상요인들을 공변량으로 고려.\n\n선형회귀분석에서 변수 선택을 할 때 쓰는 일반적인 알고리즘 중 하나인 best subset selection과 초모수 탐색 기법 중 가장 기본이라 할 수 있는 grid search를 결합하여 알고리즘을 만들어냈습니다. 이 알고리즘을 R로 구현하고 병렬처리까지 도입하여 1,368개의 모형에 대한 최적화를 해낼 수 있었습니다.\n\n## 지역별 분석 결과를 하나의 분석 결과로\n\n분석을 완료하고 분석 결과를 요약해서 각 과 교수님들께 송부했으나, 문제가 하나 있었습니다. 6개 대도시 별 결과가 조금 상이한 질환들이 있었는데, 이것들을 어떤 합리적인 근거로 설명을 하느냐 였죠. 그래서, 나왔던 한 가지 해결책이 6개 지역별 분석 결과를 하나의 분석 결과로 요약하는 것이었습니다. 이를 위해서는, Gasparrini 교수가 제안한 다변량 메타분석(mutivariate meta-analysis, [@gasparrini2012]) 방법론을 이용해야했습니다. 몇 가지 상황에 사용할 수 있는 여러 다변량 메타분석 방법론을 제시하고 있으니, 관심있으신 분들은 한번 쯤 들여다보셔도 좋을 것 같습니다. 최적화 해둔 모형들에 대해 다변량 메타분석을 수행하며, Reference로 참고할만한 R 코드들이 없어서 Github를 뒤지는데에 꽤 시간을 썼습니다. 그렇게 찾아낸 괜찮은 Reference를 바탕으로 다변량 메타분석을 R 상에서 수행하는 로직을 이해하고, 확장하여 적용할 수 있었습니다. 다변량 메타분석으로 지역별 분석 결과를 요약하는 것을 끝으로, 더이상의 추가적인 모델링은 없었습니다. 분석 결과를 요약하는 시각화를 꾸준하게 개선해나갔고, 동시에 본격적인 논문화 작업을 시작했습니다.\n\n## 맺음말\n\n이번 5월 회고록에서는 제 첫 직장이였던 곳에서 연구과제를 수행하며 겪었던 난관들에 대해서 이야기해봤는데요. 회고를 하며 그때 겪었던 감정들이 다시 새록새록 떠올랐습니다. 막막해보이는 것들도 충분한 시간을 할애하고 노력하면 결국 해결할 수 있는 것이라는 큰 교훈도 다시 한 번 얻을 수 있었고요. 회고라고 했지만, 어찌보면 나 이만큼 힘들었다고 푸념을 적어 놓은 것 같기도하네요.😂 실무에 계신 분들이든, 학업에 계신 분들이든 다 각자가 겪고 있는 난관이 있을텐데요. 쉽게 포기하는 마음보다는, \"이 문제를 어떻게 해결할 수 있을까?\"에 충분한 시간을 할애해보시기를 권해봅니다. 오늘도 이러한 난관을 헤쳐나가기 위해 고민하고, 노력하고 계신 분들께 응원의 말씀을 올리며 글을 마칩니다.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}