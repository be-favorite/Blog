---
title: "월간 회고록: 6-7월"
description: |
    워크샵, Airflow, 2023'KISS Summer School
date: "2023-08-12"
draft: FALSE
preview: preview.jpg
citation:
    url: https://taemobang.com/posts/2023-08-12-monthly-memory-202306-07/
categories: [Memory]
---

[![The illustration by Mary Amato](preview.png){fig-align="center"}](https://notioly.com)

어느덧 3분기가 저물어간다. 벌써 8월 중순이다. 지난 6월, 7월의 회고를 몰아서 작성해본다. 정신 똑바로 차리고 살자.

## 업무

### 워크샵

어쩌다보니 6, 7월에 3주에 걸쳐 매주 하나씩 워크샵을 진행했다. 서로 의견을 자유롭게 교환하며 진득하게 이야기를 나눌 것이 있을 때는 역시 오프라인 워크샵만한게 없다. 워크샵을 자연스럽게 여는 말은 언제나 팀장님의 역할이었는데, 이게 참 어려워 보인다. 이런 거 볼때마다 난 항상 상상해본다. 내가 저 자리에 가면 저렇게 할 수 있을까? "워크샵"은 어찌보면 정답이 없는 여러 주제들을 두고 자유롭게 의견을 교환하는 토론의 장과 같다고도 생각하는데, 편안하고 수평적인 팀 분위기에도 워크샵이라는 이벤트를 시작할 때 감도는 은근한 긴장감을 말로써 풀어주는 역할을 한다는게.. 참 어려워보이고 대단해보인다.

그래서인지, 늘 오전은 워밍업(?) 느낌으로 가볍게 시작하여 조금 이른 점심을 먹곤 한다. 이것마저 팀장님이 의도한거라면.. 존경을 보낸다.😂 업무와는 거리가 있는 이런저런 각자 얘기도 나누면서 말이다. 오후엔 끝장 토론을 진행하며 오전에 열어둔 여러 주제들을 하나씩 격파해 나간다. 끊임없이 머리를 쓰고 상대방의 말을 듣고, 내가 할 말도 정리를 해야하기에 몰입해서 하다보면 3-4시간이 훌쩍 지나간다. 끝나고나면 체력적으로 인지, 정신적으로 인지 모르겠지만 어느정도 지쳐있다. 워크샵에 열심히 참가했다는 증거인 듯 해서 이런 느낌이 나쁘진 않다. 찬반양론이 갈릴 수 있는 주제들에 대해 서로의 이견을 좁혀나가며 하나의 결론을 얻어냈다는 사실도 내가 몰두한 지난 몇시간을 뿌듯한 기억으로 떠올리게 해준다.

뭐든 첫 경험이 참 중요한 것 같다. 워크샵 일정이 잡혔을 때는 다른 감정들 보단 "기대"라는 감정이 더 앞선다. 물론, 내가 워크샵을 주도해서 진행해야 되는 상황이라면 "부담", "긴장"이라는 감정이 앞설 수도 있겠지만.😂 아무튼, 이런 분위기의 워크샵을 사회초년생으로서 경험할 수 있다는게 참 감사하고, 앞으로의 내 커리어에도 많은 도움이 될 것 같다.

### Airflow

팀 내 동료(시니어 DA)분께 전반적인 Airflow 사용법에 대해 배웠다. 부끄러운 이야기일 수 있는데, 업무 간에 Airflow를 활용한 경험이 있긴하나, Airflow를 정식으로(?) 배운적도 없거니와 사내 Git의 legacy code 격으로 짜여진 형태를 보고 끼워 맞추기를 한 것에 불과한 수준이라 한번 쯤 Official한 Airflow 사용법을 배워둘 필요성을 느끼고 있었다. 이런 생각을 하는 와중에 선뜻 python operator를 활용한 Airflow 사용법을 알려주신다고 하니.. 너무 감사했다. 한시간 반정도 본인의 맥북하나로 뚝딱뚝딱 설명해주시는데 쉽게쉽게 설명해주시는 덕에, 짧은 시간에 많은 것들을 습득할 수 있었다.

사내 Git에서 쓰이고 있는 bash operator를 활용한 Airflow 코드가 아주 복잡한 형태로 짜여져있었던 이유는, jinja template을 커스텀해서 사용한 탓?..이라고 대략적인 이유를 들었는데 official한 python operator 사용법을 듣고 나니 Airflow가 꽤 쉽게 느껴졌다. bash operator까지 활용하면 자동화할 수 있는 워크플로도 꽤 많이 떠오르고 말이다. 배운걸 까먹지 전에 마침 마트화 필요성이 있는 테이블이 있었는데, 다음날 바로 배운 것들을 업무에 적용해보았고 큰 어려움없이 일 배치 작업을 구현할 수 있었다.

이 날 이후 필요시 Airflow를 편하게 활용하고 있는데, 업무 생산성에 많은 도움이 된다. 지난 몇년간 Airflow라는 키워드를 봐오며, 이게 뭔데? 라는 생각만하고 Try를 안해본 지난 날의 나를 반성한다.. 제대로 안해볼꺼면 시작도 안한다는 태도의 게으른 완벽주의자 성향을 올해 들어서야 뜯어고치고 있다. 다음의 두 문장을 가슴에 품고 살도록 하자.

-   Just do it

-   Done is better than perfect.

## 개인

### 2023' KISS Summer School

![출처: https://kiss.statground.net/intro/](fig1.png){width="600"}

통계마당에서 주관한 2023 KISS Summer School에 참가했다. 사내의 외부교육 지원제도를 이용하여 두 세션 모두 개인적인 지출없이 들을 수 있었다(사랑해요 G마켓💘). 토, 일 이틀 간에 걸쳐 각 세션을 진행해주시는 교수님 두 분 모두 개인적으로 한번 쯤 뵙고 싶었던 분들이었다. 페이스북에서 두 교수님이 툭툭 나눠주시는 말씀을 감명깊게 읽었던 적이 많았기 때문에.😂 왜인지는 모르겠지만, 오랜만에 수식을 끄적이며 강의를 들으니 기분이 좋았다. 오랜만에 듣는 통계학과 교수님들 수업이라, 잠깐 대학원생때로 돌아간 듯한 느낌이라 그런 걸지도 모르겠다. 세션 자체도 좋았지만 쉬는시간에 온라인에서만 뵙던 교수님들과 이야기를 나눌 수 있음에도 참 감사했다.

직장을 다니고 있는 와중에도 주말에 시간을 내어 설레는 맘을 품고 이런 세션을 참가하고 있는 내가 참 신기하다. 학부생, 대학원생 때는 이런 감정이 전혀 없었는데, 역시 지나고보니 참 소중한 시간들이었다. 조금이라도 더 주도적으로 수업에 참가할껄\~ 이라는 무의미한 후회를 해본다. 이러한 세션을 마련해주신 통계마당 유재성 님께도 감사의 마음을 전한다.😀 마지막으로 간략한 수강 후기를 적어보려고 한다.

-   Theory and Methods for missing data analysis - 김재광 교수님![출처: https://kiss.statground.net/intro/](fig2.png){width="600"}

    -   Missing data theory는 통계학과를 전공하며 수강해볼 기회가 없었던 터라 많은 기대가 됐다. 다변량 수업을 들으며 EM algorithm에 대해 배웠는데, EM algorithm이 Frequentist 관점의 missing data theory에서 핵심 이론이라는 사실이 참 흥미로웠다. 그리고, Missing의 여러 케이스 중 MCAR(Missing Completely at Random)인 경우 Missing을 해결하지 않아도 모수 추정에 아무런 문제가 없음을 수식으로 확인할 수 있는 부분도 재밌었다.

    -   강의 마지막 시간에 Potential Outcome Framework 관점의 인과추론을 Missing data problem 관점에서 풀어내주셨다. Missing data framework와 Potential Outcome Framework는 구조적으로 일치하기 때문에, missing data 관련 method를 causal inference에 적용해볼 수 있다고 한다.

    -   그 외 많은 테크닉들을 알려주셨는데 내 수준에는 교수님의 강의를 따라가기 버거웠다.😭 교수님 세션 때 수강을 하러오신 타 대학 교수님들도 꽤 있어보였는데, 끄덕이며 들으시는 모습을 보는데 참 부러웠다.😂

    -   사실 Imputation 관점에서 sample size가 충분히 크면 missing이 있는 data point는 날려버리면 된다. 모수 추정 효율에 큰 영향이 없다는 전제하에 말이다. 나는 sample 확보 비용이 그리 크지 않은 이커머스 업계에 있기 때문에 다행스러운 부분이지만, 농업, 의학 등과 같이 sample 확보가 어렵고 확보된 sample 하나하나가 매우 비싼 도메인에서는 missing data를 다루는 방법론에 관한 이해가 필히 되있어야 할 것으로 보인다. 농업, 의학 실험 간에 자료에 missing이 발생하는 경우는 흔히 접할 수 있기 때문이다.

    -   Imputation 관점을 떠나서 한 가지 기억해둘만한 부분을 정리해보면. 특정 자료의 분포를 추정하고자 할 때, 도메인 지식 상으로는 특정 변수가 해당 자료의 분포에 큰 영향을 미침을 알고 있으나 해당 변수는 우리가 관측이 불가능한 상황이라고 해보자. 즉, 데이터에 해당 변수는 측정이 되어있지 않은 상태인 것이다. 이럴 때에는 지시변수로 latent variable을 도입하여 EM algorithm을 통해 분포 추정을 하면 효율적인 추정이 가능하다고 한다. 특정 집단의 키를 추정하고자 하는데 남, 녀의 키가 다르다는 것은 알고 있으나 Data에 성별 변수가 없는 상황을 예로 들수 있다. 이때 성별을 나타내는 지시변수를 latent variable로 놓고 EM algorithm을 통해 해당 집단의 키의 분포를 추정하면, 데이터를 묘사하는 보다 올바른 확률 추정이 가능할 것이다.

-   딥러닝의 통계학적 이해 - 장원 교수님![출처: https://kiss.statground.net/intro/](fig3.png){width="600"}

    -   통계학자가 설명해주는 신경망 모형 그자체로도 너무 특별한 강의였다. Image classification 문제를 풀기 위해 과거 기술 존재했던 기술적 한계를 기반으로 Neural Net의 Motivation을 정확하게 설명해주시며 강의를 시작하셨다. 개인적으로 어떤 기술, 알고리즘에 대해 배울 때 가장 먼저 선행되어야 하는 것은 해당 기술이 등장하게 된 Background, Context를 이해하는 것이라고 보는데, 딱 이런 스타일로 Neural Net의 motivation 부터 CNN, RNN, Transformer의 등장, Neural Net의 모수 추정 한계가 발생했던 이유, 그로 인해 나온 해결 방법들, Regularization 문제까지 이해하기 쉽게 설명을 해주셨다. 모든 딥러닝 강의들이 이런 플롯(?)이었다면, 종종 보이는 아무 문제, 데이터, Context에서 딥러닝을 무지성으로 갖다 쓰려는 행태는 사전에 방지할 수 있었을거라고 본다.

    -   또한, Perceptron의 모형 구조, likelihood에 기반한 loss function을 수식으로 정의한 뒤에 동일한 Framework를 유지하며 Deep Feed-Forword Networks, 일종의 Feature-extraction layer가 얹어진 형태인 CNN, RNN, Transformer layer의 모형 구조를 쭉 정의해주시는 것도 너무 좋았다.

    -   수식 없이 Neural-net layer 형태에 관한 그림에 의존해 해당 모형을 설명하는 것들 듣곤 할 때마다, "그래서, 이 모델이 이 문제에 왜 잘 working하는 건데?"에 대한 의문이 되려 생기곤 했는데 이 부분이 많이 해소되었다. 그리고, 추천 모델, 자연어 모델을 개발하는 팀에서 데이터 분석을 하고 있다보니 Transformer 얘기를 늘상 듣곤했는데, 해당 기술이 등장한 맥락과 왜 이 기술이 해당 문제들을 푸는데에 잘 working할 수 밖에 없는지 정도에 대해서는 이해할 수 있었다. 아무튼 해당 세션 또한 참 뜻깊고 재밌었다.
